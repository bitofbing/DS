{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from string import ascii_uppercase\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 300,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"savefig.format\": \"pdf\",\n",
    "    \"lines.linewidth\": 2,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"axes.titlesize\": 18,\n",
    "    'axes.labelweight': 'bold',\n",
    "    \"xtick.labelsize\": 14,\n",
    "    \"ytick.labelsize\": 14,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"grid.alpha\": 0.5,\n",
    "    \"font.family\": \"DejaVu Sans\",  # 更稳定跨平台字体\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "    \"text.usetex\": False,\n",
    "    \"text.antialiased\": True\n",
    "})"
   ],
   "id": "b10cd896fe239f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1、使用PPT算法规范化",
   "id": "aa89f94331879342"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def normalize_evidence(evidence_list, all_hypotheses = {'A', 'B', 'C'}):\n",
    "    \"\"\"\n",
    "    归一化证据，使所有证据具有相同的事件键 {'A', 'B', 'C'}\n",
    "    - 输入: evidence_list (列表), 包含多个证据体，每个证据体是一个字典，键为元组，值为概率。\n",
    "    - 输出: 归一化后的证据列表，每个证据体具有相同的事件键 {'A', 'B', 'C'}。\n",
    "    \"\"\"\n",
    "      # 确保所有证据具有相同的键\n",
    "    normalized = []\n",
    "\n",
    "    for ev in evidence_list:\n",
    "        new_ev = {h: 0 for h in all_hypotheses}  # 初始化为 0\n",
    "        for key, prob in ev.items():\n",
    "            for h in key:  # 元组键可能包含多个事件\n",
    "                new_ev[h] += prob / len(key)  # 平均分配概率\n",
    "        normalized.append(new_ev)\n",
    "\n",
    "    return normalized"
   ],
   "id": "b80eda9218fe1f5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2、距离公式",
   "id": "4c3c24d140d33bd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.1 bhattacharyya距离公式",
   "id": "f0e6abed223cdfbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def bhattacharyya_distance(p, q):\n",
    "    \"\"\"\n",
    "    计算 Bhattacharyya 距离\n",
    "    - 输入: p, q (列表或数组), 两个概率分布。\n",
    "    - 输出: Bhattacharyya 距离，用于衡量两个概率分布之间的相似性。\n",
    "    \"\"\"\n",
    "    p, q = np.array(p), np.array(q)\n",
    "    return -np.log(np.sum(np.sqrt(p * q)) + 1e-10)  # 避免 log(0)"
   ],
   "id": "a051fc2244db5c18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2 计算两个焦元之间的 Jaccard 相似度",
   "id": "4212f9561bd48d03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def jaccard_similarity(A, B):\n",
    "    \"\"\"\n",
    "    计算两个焦元之间的 Jaccard 相似度\n",
    "    \"\"\"\n",
    "    if not A and not B:  # 处理空集情况\n",
    "        return 1.0\n",
    "    intersection = len(A.intersection(B))\n",
    "    union = len(A.union(B))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def build_jaccard_matrix(focal_elements):\n",
    "    \"\"\"\n",
    "    构建 Jaccard 相似度矩阵 D\n",
    "    \"\"\"\n",
    "    n = len(focal_elements)\n",
    "    D = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):  # 只计算上三角，减少重复计算\n",
    "            sim = jaccard_similarity(focal_elements[i], focal_elements[j])\n",
    "            D[i, j] = D[j, i] = sim  # 矩阵对称\n",
    "    return D\n",
    "\n",
    "def jousselme_distance(m1, m2):\n",
    "    \"\"\"\n",
    "    计算 Jousselme 距离\n",
    "    \"\"\"\n",
    "    focal_elements = list(set(m1.keys()).union(set(m2.keys())))  # 取并集\n",
    "    focal_elements = [frozenset(f) for f in focal_elements]\n",
    "\n",
    "    D = build_jaccard_matrix(focal_elements)  # 计算 Jaccard 矩阵\n",
    "\n",
    "    m1_vec = np.array([m1.get(f, 0) for f in focal_elements])\n",
    "    m2_vec = np.array([m2.get(f, 0) for f in focal_elements])\n",
    "\n",
    "    diff = m1_vec - m2_vec\n",
    "    distance = np.sqrt(0.5 * np.dot(diff.T, np.dot(D, diff)))\n",
    "    return distance\n",
    "\n",
    "def jousselme_similarity(evidence, method=\"inverse_quadratic\", sigma=1.0):\n",
    "    \"\"\"\n",
    "    计算 Jousselme 相似度\n",
    "    method 可选：\n",
    "    - \"inverse\"（默认）：1 / (1 + d)\n",
    "    - \"inverse_quadratic\"：1 / (1 + d²)（增强区分度）\n",
    "    - \"logarithmic\"：1 / (1 + log(1 + d))\n",
    "    - \"gaussian\"：exp(-d² / sigma²)（模拟高斯核）\n",
    "    \"\"\"\n",
    "    evidence_frozenset = [{frozenset(k): v for k, v in ev.items()} for ev in evidence]\n",
    "    distance = jousselme_distance(evidence_frozenset[0], evidence_frozenset[1])\n",
    "\n",
    "    if method == \"inverse\":\n",
    "        return 1 / (1 + distance)\n",
    "    elif method == \"inverse_quadratic\":\n",
    "        return 1 / (1 + distance**2)\n",
    "    elif method == \"logarithmic\":\n",
    "        return 1 / (1 + np.log1p(distance))\n",
    "    elif method == \"gaussian\":\n",
    "        return np.exp(-distance**2 / (2 * sigma**2))\n",
    "    elif method == \"exponential\":\n",
    "        return max(0, (1 - distance))\n",
    "    elif method == \"quadratic\":\n",
    "        return np.exp(-1.5*distance)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Choose from: 'inverse', 'inverse_quadratic', 'logarithmic', 'gaussian'.\")"
   ],
   "id": "62e92436e7761386"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.3 余旋相似度",
   "id": "55d0e6acf37d29c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cosine_similarity(p, q):\n",
    "    \"\"\"\n",
    "    计算余弦相似度\n",
    "    - 输入: p, q (字典), 两个证据体的焦元及其概率。\n",
    "    - 输出: 余弦相似度。\n",
    "    \"\"\"\n",
    "    keys = set(p.keys()).union(set(q.keys()))\n",
    "    vec_p = np.array([p.get(key, 0) for key in keys])\n",
    "    vec_q = np.array([q.get(key, 0) for key in keys])\n",
    "    return np.dot(vec_p, vec_q) / (np.linalg.norm(vec_p) * np.linalg.norm(vec_q) + 1e-8)"
   ],
   "id": "95809012a640aa69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.4 KL散度距离",
   "id": "3673c78e0d3621c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def kl_divergence(p, q):\n",
    "    \"\"\"\n",
    "    计算 KL 散度\n",
    "    - 输入: p, q (字典), 两个证据体的焦元及其概率。\n",
    "    - 输出: KL 散度。\n",
    "    \"\"\"\n",
    "    keys = set(p.keys()).union(set(q.keys()))\n",
    "    p = np.array([p.get(key, 0) for key in keys])\n",
    "    q = np.array([q.get(key, 0) for key in keys])\n",
    "    return np.sum(p * np.log(p / q + 1e-8))"
   ],
   "id": "7b29c2bb6d395447"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.5 JS散度距离",
   "id": "844ced98c7f481b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def js_divergence(p, q):\n",
    "    \"\"\"\n",
    "    计算 JS 散度\n",
    "    - 输入: p, q (字典), 两个证据体的焦元及其概率。\n",
    "    - 输出: JS 散度。\n",
    "    \"\"\"\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (kl_divergence(p, m) + kl_divergence(q, m))"
   ],
   "id": "27555945355b79a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.6 OWA距离公式",
   "id": "8f34d645345b6ce9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_singleton_relation(i, j):\n",
    "    \"\"\"\n",
    "    通用单例元素关系计算\n",
    "    支持: 字母(A-Z)、数字、或混合输入\n",
    "    关系公式: 1 - exp(-|a - b|)\n",
    "    \"\"\"\n",
    "\n",
    "    # 字母到数字的自动映射 (A->1, B->2, ..., Z->26)\n",
    "    def to_numeric(x):\n",
    "        if isinstance(x, str) and x.isalpha() and x in ascii_uppercase:\n",
    "            return ascii_uppercase.index(x) + 1\n",
    "        return int(x)  # 如果是数字字符串或数字\n",
    "\n",
    "    a = to_numeric(i)\n",
    "    b = to_numeric(j)\n",
    "    return 0 if a == b else 1 - np.exp(-abs(a - b))\n",
    "\n",
    "def compute_singleton_set(A, B, gamma):\n",
    "    \"\"\"计算单例到集合的关系\"\"\"\n",
    "    r_values = []\n",
    "    for b in B:\n",
    "        r_values.append(compute_singleton_relation(A, b))\n",
    "    return owa_aggregation(r_values, gamma, force_binary=False)\n",
    "\n",
    "\n",
    "def build_relation_matrix(focal_elements, gamma=0.8):\n",
    "    \"\"\"构建关系矩阵（适配字母型证据体）\"\"\"\n",
    "    n = len(focal_elements)\n",
    "    R_o = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):  # 仅计算上三角\n",
    "            A, B = focal_elements[i], focal_elements[j]\n",
    "\n",
    "            # Case 1: 单例-单例关系\n",
    "            if len(A) == 1 and len(B) == 1:\n",
    "                a = next(iter(A))  # 获取集合中的唯一元素\n",
    "                b = next(iter(B))\n",
    "                R_o[i, j] = compute_singleton_relation(a, b)\n",
    "\n",
    "            # Case 2: 单例-集合关系\n",
    "            elif len(A) == 1 and len(B) > 1:\n",
    "                a = next(iter(A))\n",
    "                R_o[i, j] = compute_singleton_set(a, B, gamma)\n",
    "\n",
    "            # Case 3: 集合-集合关系\n",
    "            elif len(A) > 1 and len(B) > 1:\n",
    "                r_values = []\n",
    "                for a in A:\n",
    "                    r_values.append(compute_singleton_set(a, B, gamma))\n",
    "                R_o[i, j] = owa_aggregation(r_values, gamma, force_binary=False)\n",
    "\n",
    "    # 对称填充下三角\n",
    "    R_o = np.triu(R_o) + np.triu(R_o, 1).T\n",
    "    return R_o\n",
    "\n",
    "\n",
    "def owa_aggregation(values, gamma, force_binary=False):\n",
    "    \"\"\"OWA聚合函数（保持不变）\"\"\"\n",
    "    if len(values) == 1:\n",
    "        return values[0]\n",
    "\n",
    "    if force_binary:  # 二元情况直接计算\n",
    "        sorted_v = sorted(values, reverse=True)\n",
    "        return gamma * sorted_v[0] + (1 - gamma) * sorted_v[1]\n",
    "\n",
    "    sorted_values = np.array(sorted(values, reverse=True))\n",
    "    n = len(sorted_values)\n",
    "\n",
    "    def entropy(w):\n",
    "        return -np.sum(w * np.log(w + 1e-10))\n",
    "\n",
    "    constraints = [\n",
    "        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},\n",
    "        {'type': 'eq', 'fun': lambda w: np.sum([(n - k - 1) / (n - 1) * w[k] for k in range(n)]) - gamma}\n",
    "    ]\n",
    "    bounds = [(0.001, 1) for _ in range(n)]\n",
    "    res = minimize(entropy, x0=np.ones(n) / n, bounds=bounds,\n",
    "                   constraints=constraints, options={'ftol': 1e-10})\n",
    "\n",
    "    weights = res.x / np.sum(res.x)\n",
    "    return np.dot(sorted_values, weights)\n",
    "\n",
    "\n",
    "def d_OWA(m1, m2, gamma=0.8):\n",
    "    \"\"\"计算两个证据体之间的距离\"\"\"\n",
    "    # 获取所有焦元（保持顺序一致性）\n",
    "    focal_elements = sorted(list(set(m1.keys()).union(set(m2.keys()))), key=lambda x: (len(x), x))\n",
    "\n",
    "    # 构建关系矩阵\n",
    "    R_o = build_relation_matrix(focal_elements, gamma)\n",
    "\n",
    "    # 转换为向量（保持相同顺序）\n",
    "    m1_vec = np.array([m1.get(f, 0) for f in focal_elements])\n",
    "    m2_vec = np.array([m2.get(f, 0) for f in focal_elements])\n",
    "\n",
    "    # 计算距离\n",
    "    diff = m1_vec - m2_vec\n",
    "    M = np.eye(len(R_o)) - R_o\n",
    "    distance = np.sqrt(0.5 * diff.T @ M @ diff)\n",
    "    return distance\n",
    "\n",
    "\n",
    "def owa_similarity_matrix(evidence, gamma=0.8):\n",
    "    \"\"\"计算证据体列表的相似度矩阵\"\"\"\n",
    "    # 转换证据体格式\n",
    "    evidence_frozenset = [{frozenset(k): v for k, v in ev.items()} for ev in evidence]\n",
    "\n",
    "    n = len(evidence_frozenset)\n",
    "    similarity_matrix = np.eye(n)  # 对角线为1\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distance = d_OWA(evidence_frozenset[i], evidence_frozenset[j], gamma)\n",
    "            similarity = 1 - distance\n",
    "            similarity_matrix[i][j] = similarity\n",
    "            similarity_matrix[j][i] = similarity\n",
    "\n",
    "    return similarity_matrix"
   ],
   "id": "f9e363d0f58ec7e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3、相似度矩阵",
   "id": "32d47250735510b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.1 基于bhattacharyyade的相似度矩阵",
   "id": "9d1484463acfb457"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_similarity_matrix(evidence, sigma=1.0):\n",
    "    \"\"\"\n",
    "    计算相似度矩阵 S\n",
    "    - 输入: evidence (列表), 归一化后的证据列表。\n",
    "    - 输出: 相似度矩阵 S，表示证据之间的相似性。\n",
    "    \"\"\"\n",
    "    n = len(evidence)\n",
    "    S = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            p = list(evidence[i].values())\n",
    "            q = list(evidence[j].values())\n",
    "\n",
    "            distance = bhattacharyya_distance(p, q)\n",
    "            similarity = np.exp(distance)  # 转化为相似度\n",
    "            # similarity = np.exp(-(distance ** 2) / (2 * sigma ** 2))  # 高斯核函数\n",
    "\n",
    "            S[i, j] = similarity\n",
    "            S[j, i] = similarity  # 矩阵对称\n",
    "\n",
    "    return S"
   ],
   "id": "9805c7683690c05b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.2 基于Jaccard的相似度矩阵",
   "id": "b903fd41481352d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_similarity_matrix_jousselme(evidence, method):\n",
    "    \"\"\"\n",
    "    计算相似度矩阵 S\n",
    "    - 输入: evidence (列表), 归一化后的证据列表。\n",
    "    - 输出: 相似度矩阵 S，表示证据之间的相似性。\n",
    "    \"\"\"\n",
    "    n = len(evidence)\n",
    "    S = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            list_dict = [evidence[i], evidence[j]]\n",
    "            distance = jousselme_similarity(list_dict,method)\n",
    "\n",
    "            S[i, j] = distance\n",
    "            S[j, i] = distance  # 矩阵对称\n",
    "\n",
    "    return S"
   ],
   "id": "bf20758f8ea7aacf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.3 余旋相似度矩阵",
   "id": "67eee245b00c07f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_similarity_matrix_cos(evidence):\n",
    "    n = len(evidence)\n",
    "    S = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            similarity = cosine_similarity(evidence[i], evidence[j])\n",
    "            S[i, j] = similarity\n",
    "            S[j, i] = similarity  # 矩阵对称\n",
    "    return S"
   ],
   "id": "f40ae428c26692cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.4 JS散度相似度矩阵",
   "id": "24a171abdb3eff04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_similarity_matrix_JS(evidence):\n",
    "    n = len(evidence)\n",
    "    S = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            divergence = js_divergence(evidence[i], evidence[j])\n",
    "            similarity = np.exp(-divergence)  # 转化为相似度\n",
    "            S[i, j] = similarity\n",
    "            S[j, i] = similarity  # 矩阵对称\n",
    "    return S"
   ],
   "id": "478a015037c395cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4、非线性聚合",
   "id": "6935894705a19f57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def nonlinear_aggregation(S, alpha=1.0):\n",
    "    \"\"\"\n",
    "    限制 log-sum-exp 放大效应，避免某些证据占据主导地位\n",
    "    \"\"\"\n",
    "    return np.log(1 + np.sum(np.exp(alpha * S), axis=1)) / alpha"
   ],
   "id": "20f65b6a654cbf3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5、自适应高斯变换",
   "id": "30b6c1ce3933c9b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def adaptive_gaussian_transform(x):\n",
    "    \"\"\"\n",
    "    自适应高斯变换：根据数据的均值和标准差动态调整 mu 和 sigma\n",
    "    \"\"\"\n",
    "    mu = np.mean(x)\n",
    "    sigma = np.std(x)\n",
    "    return np.exp(-0.5 * ((x - mu) / (sigma + 1e-8))**2)  # 防止除零"
   ],
   "id": "e7035310b490faa7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6、通过相似度矩阵计算依赖度权重",
   "id": "182eccb1711c1ae5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6.1 结合自适应高斯变换的权重计算",
   "id": "6646074cd010c04c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def new_compute_weight_from_similarity_gaussian(S, lambda_entropy=0.1, beta=3.5, alpha=1.0):\n",
    "    S_agg = nonlinear_aggregation(S, alpha)  # 非线性聚合\n",
    "    S_agg_gaussian = adaptive_gaussian_transform(S_agg)  # 自适应高斯变换\n",
    "    def objective(w):\n",
    "        return -np.dot(w, S_agg_gaussian) + lambda_entropy * np.sum(w * np.log(w + 1e-8))\n",
    "\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    bounds = [(0, 1)] * len(S_agg_gaussian)\n",
    "\n",
    "    w0 = S_agg_gaussian / np.sum(S_agg_gaussian)\n",
    "    result = minimize(objective, w0, bounds=bounds, constraints=constraints)\n",
    "\n",
    "    return result.x"
   ],
   "id": "6b36db14dbda0732"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6.2 计算归一化的可信度权重 Crd",
   "id": "1683302edd671e3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_credibility_from_similarity(S):\n",
    "    \"\"\"\n",
    "    计算归一化的可信度权重 Crd\n",
    "    - 输入: S (相似度矩阵)\n",
    "    - 输出: Crd (可信度权重向量)\n",
    "    \"\"\"\n",
    "    # 计算支持度 Sup(m_i)\n",
    "    Sup = np.sum(S, axis=1) - np.diag(S)  # 去掉自身对自身的支持度\n",
    "\n",
    "    # 计算可信度 Crd 并归一化\n",
    "    Crd = Sup / np.sum(Sup) if np.sum(Sup) != 0 else np.ones_like(Sup) / len(Sup)  # 避免除零\n",
    "\n",
    "    return Crd"
   ],
   "id": "88569b6fcee4fe90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6.3 一个原始的权重计算",
   "id": "6870bbda7e210f9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_weight_from_similarity(S):\n",
    "    \"\"\"\n",
    "    计算权重 R：\n",
    "    - R = 归一化的 S_avg\n",
    "    - 输入: S (矩阵), 相似度矩阵。\n",
    "    - 输出: 权重 R，表示每个证据的可靠性。\n",
    "    \"\"\"\n",
    "    S_avg = np.mean(S, axis=1)  # 每行的均值\n",
    "    S_avg = S_avg ** 5  # 增强高相似度证据源的权重\n",
    "    R = S_avg / np.sum(S_avg)  # 归一化\n",
    "    return R"
   ],
   "id": "1d91445240099fc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6.4 非线性聚合权重计算",
   "id": "90b2bff37d17d085"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def new_compute_weight_from_similarity_nonlinear(S, lambda_entropy=0.1, beta=3.5, alpha=1.0):\n",
    "    S_agg = nonlinear_aggregation(S, alpha)  # 非线性聚合\n",
    "    S_agg_sigmoid = 1 / (1 + np.exp(-beta * (S_agg - np.mean(S_agg))))\n",
    "    def objective(w):\n",
    "        return -np.dot(w, S_agg_sigmoid) + lambda_entropy * np.sum(w * np.log(w + 1e-8))\n",
    "\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    bounds = [(0, 1)] * len(S_agg_sigmoid)\n",
    "\n",
    "    w0 = S_agg_sigmoid / np.sum(S_agg_sigmoid)\n",
    "    # result = minimize(objective, w0, bounds=bounds, constraints=constraints)\n",
    "    # 优化\n",
    "    result = minimize(\n",
    "        objective,\n",
    "        w0,\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "        method='SLSQP'  # 使用 SLSQP 算法\n",
    "    )\n",
    "    return result.x"
   ],
   "id": "621f4f2a99023002"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 使用 Tsallis 熵正则化",
   "id": "b3947b0f35a1bc97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_weight_tsallis(S, q=1.5, lambda_tsallis=0.1):\n",
    "    \"\"\"\n",
    "    使用 Tsallis 熵正则化，使高相似度权重更大，同时避免过度集中。\n",
    "    \"\"\"\n",
    "    S_avg = np.mean(S, axis=1)\n",
    "\n",
    "    def objective(w):\n",
    "        tsallis_entropy = lambda_tsallis * (1 - np.sum(w ** q)) / (q - 1)\n",
    "        return -np.dot(w, S_avg) + tsallis_entropy\n",
    "\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    bounds = [(0, 1)] * len(S_avg)\n",
    "\n",
    "    w0 = np.ones(len(S_avg)) / len(S_avg)\n",
    "    result = minimize(objective, w0, bounds=bounds, constraints=constraints)\n",
    "\n",
    "    return result.x"
   ],
   "id": "a95a514731137bc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7、软似然估计函数",
   "id": "531cbd95da65650f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def soft_likelihood_function(omega, evidence, R_values, alpha):\n",
    "    \"\"\"\n",
    "    软似然函数 (Soft Likelihood Function, SLF)\n",
    "    - 输入: omega (字符串), 当前假设；evidence (列表), 归一化后的证据列表；R (数组), 权重；alpha (浮点数), 乐观系数。\n",
    "    - 输出: 软似然值 L，表示当前假设的软似然估计。\n",
    "    \"\"\"\n",
    "    N = len(evidence)\n",
    "    prob_values = [ev.get(omega, 0) for ev in evidence]\n",
    "    # R_values = R / np.sum(R)  # 归一化可靠性度\n",
    "\n",
    "    # 排序索引\n",
    "    sorted_indices = np.argsort([prob_values[i] * R_values[i] for i in range(N)])[::-1]\n",
    "\n",
    "    # 计算权重向量\n",
    "    w = []\n",
    "    for i in range(N):\n",
    "        if i == 0:\n",
    "            w.append(R_values[sorted_indices[i]] ** ((1 - alpha) / alpha))\n",
    "        else:\n",
    "            sum_k = np.sum([R_values[sorted_indices[k]] for k in range(i + 1)])\n",
    "            sum_k_prev = np.sum([R_values[sorted_indices[k]] for k in range(i)])\n",
    "            w.append(sum_k ** ((1 - alpha) / alpha) - sum_k_prev ** ((1 - alpha) / alpha))\n",
    "\n",
    "    # 计算软似然值\n",
    "    L = 0\n",
    "    for i in range(N):\n",
    "        prod = 1\n",
    "        for k in range(i + 1):\n",
    "            prod *= prob_values[sorted_indices[k]]\n",
    "        L += w[i] * prod\n",
    "\n",
    "    return L"
   ],
   "id": "898b9d859e4e2cec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 8、归一化融合",
   "id": "72dcfd76f28ce912"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def fuse_evidence(evidence, R, hypotheses = {'A', 'B', 'C'}, alpha=0.1):\n",
    "    \"\"\"\n",
    "    计算融合概率：\n",
    "    - 使用软似然函数进行最终的概率估计\n",
    "    - 输入: evidence (列表), 归一化后的证据列表；R (数组), 权重；alpha (浮点数), 乐观系数。\n",
    "    - 输出: 融合后的概率分布。\n",
    "    \"\"\"\n",
    "    fused = {}\n",
    "\n",
    "    for omega in hypotheses:\n",
    "        L = soft_likelihood_function(omega, evidence, R, alpha)\n",
    "        fused[omega] = L\n",
    "\n",
    "    # print(\"fused_result_before_nor:\", fused)\n",
    "    # 归一化\n",
    "    total_L = sum(fused.values()) + 1e-10\n",
    "    fused = {k: v / total_L for k, v in fused.items()}\n",
    "\n",
    "    return fused"
   ],
   "id": "f8d86be8425750ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 经典融合算法\n",
   "id": "41e4f6ec1854e09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def dempster_combine(m1, m2):\n",
    "    \"\"\"\n",
    "    Dempster 组合规则 (DCR)\n",
    "    - 输入: m1 和 m2 是两个证据体的字典表示。\n",
    "    - 输出: 融合后的证据体 (字典)。\n",
    "    \"\"\"\n",
    "    hypotheses = set(m1.keys()).union(set(m2.keys()))\n",
    "    m_combined = {h: 0.0 for h in hypotheses}\n",
    "\n",
    "    # 计算冲突因子 K\n",
    "    K = 0.0\n",
    "    for h1 in m1:\n",
    "        for h2 in m2:\n",
    "            if h1 != h2:  # 如果 h1 和 h2 不相同（即冲突）\n",
    "                K += m1[h1] * m2[h2]\n",
    "\n",
    "    # 计算组合后的 BPA\n",
    "    for h1 in m1:\n",
    "        for h2 in m2:\n",
    "            if h1 == h2:  # 如果 h1 和 h2 相同（即一致）\n",
    "                m_combined[h1] += m1[h1] * m2[h2] / (1 - K)\n",
    "\n",
    "    return m_combined\n",
    "\n",
    "def fuse_evidence_multiple_times(adjusted_evidence, n):\n",
    "    \"\"\"\n",
    "    多次融合证据 (Step 3-2)\n",
    "    - 输入: adjusted_evidence (字典), 调整后的证据体。\n",
    "    - 输入: n (整数), 融合次数。\n",
    "    - 输出: 最终融合结果 (字典)。\n",
    "    \"\"\"\n",
    "    result = adjusted_evidence\n",
    "    for _ in range(n - 1):\n",
    "        result = dempster_combine(result, adjusted_evidence)\n",
    "    return result"
   ],
   "id": "1eb7856f0a986344"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 一个统一的执行入口",
   "id": "88562a5bad8552bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main_function(evidence,all_hypotheses = {'A', 'B', 'C'}):\n",
    "    # 处理证据体格式\n",
    "    normalized_evidence = normalize_evidence(evidence, all_hypotheses)\n",
    "\n",
    "    # 计算相似度矩阵\n",
    "    S = compute_similarity_matrix_jousselme(normalized_evidence, \"exponential\")\n",
    "    # S = owa_similarity_matrix(evidence)\n",
    "\n",
    "    # 计算权重 R\n",
    "    # print(S)\n",
    "    R =new_compute_weight_from_similarity_nonlinear(S)\n",
    "    R_cre = compute_credibility_from_similarity(S)\n",
    "\n",
    "    # 乘积融合\n",
    "    # product = R * R_cre\n",
    "    # R_fused = product / np.sum(product)\n",
    "    # R_final = R_fused ** 2 / np.sum(R_fused ** 2)\n",
    "    # # 指数加权参数\n",
    "    # beta, gamma = 2.0, 2.0  # 可调整放大效果\n",
    "    #\n",
    "    # # 计算指数加权融合\n",
    "    # W_final = (R ** beta) * (R_cre ** gamma)\n",
    "    # alpha = 0.8  # 相似度权重占比80%\n",
    "    # w_fused = alpha * R + (1 - alpha) * R_cre\n",
    "    # w_fused /= np.sum(w_fused)  # 归一化\n",
    "\n",
    "    # 相似度引导的乘积融合\n",
    "    lambda_ = 0.2  # 置信度微调系数\n",
    "    w_fused = R * (1 + lambda_ * R_cre)\n",
    "    w_fused /= np.sum(w_fused)  # 归一化\n",
    "\n",
    "    # 计算融合概率\n",
    "    alpha = 0.1  # 乐观系数\n",
    "    fused_probabilities = fuse_evidence(normalized_evidence, w_fused, all_hypotheses, alpha)\n",
    "    # fused_probabilities_cre = fuse_evidence(normalized_evidence, R_cre, alpha)\n",
    "\n",
    "    # print(\"Optimal R:\", R)\n",
    "    # print(\"Optimal R_fused:\", w_fused)\n",
    "    # print(\"Fused probabilities (after SLF):\", fused_probabilities)\n",
    "    # print(\"Optimal R_cre:\", R_cre)\n",
    "    # print(\"Fused probabilities (after SLF_cre):\", fused_probabilities_cre)\n",
    "    # Step 3-2: 多次融合证据\n",
    "    # fused_result_after_slf_cre =dempster_combine(fused_probabilities,fused_probabilities_cre)\n",
    "    # print(\"fused_result_after_slf_cre:\", fused_result_after_slf_cre)\n",
    "    return fused_probabilities"
   ],
   "id": "9236197717fc5fbb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 9、结果比较好的组合",
   "id": "39cdb1499ba91ccd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9.1 bhattacharyya距离公式+6.1 结合自适应高斯变换的权重计算",
   "id": "5d67b3f14fa6c336"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| 结果项 | 结果                                                                                                                         |\n",
    "|-----|----------------------------------------------------------------------------------------------------------------------------|\n",
    "| Optimal R  | [2.80777604e-01 1.72480929e-04 2.80777604e-01 2.80777604e-01 1.57494707e-01]                                               |\n",
    "| Fused probabilities (after SLF)   | {'B': 0.007348130686321714, 'C': 0.009587746280615624, 'A': 0.9830641224921434}                                            |"
   ],
   "id": "5865b7b156b25bde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9.2 bhattacharyya距离公式 + 非线性聚合权重计算",
   "id": "c8cac76336d45b0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| 结果项 | 结果                                                                                                                         |\n",
    "|-----|----------------------------------------------------------------------------------------------------------------------------|\n",
    "| Optimal R  | [0.23428291 0.04293472 0.23428291 0.23428293 0.25421653]                                               |\n",
    "| Fused probabilities (after SLF)   | {'A': 0.9764979236385228, 'B': 0.007834025132210548, 'C': 0.015668050264421095}                                           |"
   ],
   "id": "59d31a78175b517"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9.3 jousselme距离计算 + 非线性聚合权重计算",
   "id": "bf747c770f96f464"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| 结果项 | 结果                                                                                                                         |\n",
    "|-----|----------------------------------------------------------------------------------------------------------------------------|\n",
    "| Optimal R  | [0.1644752  0.01659062 0.2818091  0.28180911 0.25531597]                                             |\n",
    "| Fused probabilities (after SLF)   | {'A': 0.9902539405429464, 'B': 0.0015288616048609559, 'C': 0.008217197364307286}                                           |"
   ],
   "id": "3c7b241aef80b793"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9.4 jousselme距离计算 + 原始的幂指数",
   "id": "e7f631af760df4d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| 结果项 | 结果                                                                                                                         |\n",
    "|-----|----------------------------------------------------------------------------------------------------------------------------|\n",
    "| Optimal R  | [0.19700691 0.00984098 0.27078616 0.27078616 0.25157979]                                             |\n",
    "| Fused probabilities (after SLF)   | {'A': 0.9895654088843753, 'B': 0.0016163896707215495, 'C': 0.008818200962981408}                                           |"
   ],
   "id": "31469965560d8908"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 10、测试",
   "id": "979bf4c08d083827"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10.1 一般数据测试",
   "id": "f1517707f42c2fdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 你的证据体\n",
    "# evidence = [\n",
    "#     {('A',): 0.7, ('B',): 0.15, ('C',): 0.15},\n",
    "#     {('B',): 0.5, ('C',): 0.5},\n",
    "#     {('A',): 0.7, ('B',): 0.15, ('C',): 0.15},\n",
    "#     {('A',): 0.7, ('B',): 0.15, ('C',): 0.15},\n",
    "#     {('A', 'C'): 0.8, ('B',): 0.2}\n",
    "# ]\n",
    "\n",
    "# evidence = [\n",
    "#     {('A',): 0.6, ('B',): 0.3, ('A','B'): 0.1},  # 证据体 1\n",
    "#     {('A',): 0.5, ('B',): 0.4, ('A','B'): 0.1}  # 证据体 2\n",
    "# ]\n",
    "\n",
    "# evidence = [\n",
    "#     {('A',): 0.5, ('B',): 0.2, ('C',): 0.3},\n",
    "#     {('B',): 0.9, ('C',): 0.1},\n",
    "#     {('A',): 0.55, ('B',): 0.1, ('A','C'): 0.35},\n",
    "#     {('A',): 0.55, ('B',): 0.1, ('A','C'): 0.35},\n",
    "#     {('A',): 0.6, ('B',): 0.1, ('A', 'C'): 0.3}\n",
    "# ]\n",
    "\n",
    "# evidence = [\n",
    "#     {('A',): 0.0437, ('B',): 0.3346, ('C',): 0.2916, ('A', 'B'): 0.0437, ('A', 'C'): 0.0239, ('B', 'C'): 0.2385, ('A', 'B', 'C'): 0.0239},  # 证据体 1 (Sepal Length)\n",
    "#     {('A',): 0.0865, ('B',): 0.2879, ('C',): 0.1839, ('A', 'B'): 0.0863, ('A', 'C'): 0.0865, ('B', 'C'): 0.1825, ('A', 'B', 'C'): 0.0863},  # 证据体 2 (Sepal Width)\n",
    "#     {('A',): 1.4e-09, ('B',): 0.6570, ('C',): 0.1726, ('A', 'B'): 1.3e-09, ('A', 'C'): 1.4e-11, ('B', 'C'): 0.1704, ('A', 'B', 'C'): 1.4e-11},  # 证据体 3 (Petal Length)\n",
    "#     {('A',): 8.20e-06, ('B',): 0.6616, ('C',): 0.1692, ('A', 'B'): 8.20e-06, ('A', 'C'): 3.80e-06, ('B', 'C'): 0.1692, ('A', 'B', 'C'): 3.80e-06}   # 证据体 4 (Petal Width)\n",
    "# ]\n",
    "\n",
    "# 应用一的数据\n",
    "# evidence = [\n",
    "#     {('A',): 0.40, ('B',): 0.28, ('C',): 0.30, ('A', 'C'): 0.02},  # S₁\n",
    "#     {('A',): 0.01, ('B',): 0.90, ('C',): 0.08, ('A', 'C'): 0.01},  # S₂\n",
    "#     {('A',): 0.63, ('B',): 0.06, ('C',): 0.01, ('A', 'C'): 0.30},  # S₃\n",
    "#     {('A',): 0.60, ('B',): 0.09, ('C',): 0.01, ('A', 'C'): 0.30},  # S₄\n",
    "#     {('A',): 0.60, ('B',): 0.09, ('C',): 0.01, ('A', 'C'): 0.30}   # S₅\n",
    "# ]\n",
    "#\n",
    "# # 应用二的数据\n",
    "evidence = [\n",
    "    {('A',): 0.7, ('B',): 0.1, ('A','B','C'): 0.2},  # 证据体 m1\n",
    "    {('A',): 0.7, ('A','B','C'): 0.3},               # 证据体 m2\n",
    "    {('A',): 0.65, ('B',): 0.15, ('A','B','C'): 0.20},  # 证据体 m3\n",
    "    {('A',): 0.75, ('C',): 0.05, ('A','B','C'): 0.20},  # 证据体 m4\n",
    "    {('B',): 0.20, ('C',): 0.80}                      # 证据体 m5\n",
    "]\n",
    "all_hypotheses = {'A', 'B', 'C'}\n",
    "main_function(evidence, all_hypotheses)"
   ],
   "id": "54ac7a1efed3d768"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## xiao1",
   "id": "f75775d156caddf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "\n",
    "\n",
    "def belief_jensen_shannon_divergence(m1, m2):\n",
    "    \"\"\"\n",
    "    计算两个基本置信分配(BBA)之间的信念詹森-香农散度(BJS)\n",
    "    对应Definition 3.1和公式(13)-(14)\n",
    "\n",
    "    参数:\n",
    "        m1, m2: 两个BBA字典，键为命题元组，值为置信度\n",
    "\n",
    "    返回:\n",
    "        BJS散度值\n",
    "    \"\"\"\n",
    "    # 获取所有命题的并集\n",
    "    propositions = set(m1.keys()).union(set(m2.keys()))\n",
    "\n",
    "    # 计算中间量m_avg = (m1 + m2)/2\n",
    "    m_avg = {}\n",
    "    for prop in propositions:\n",
    "        m_avg[prop] = (m1.get(prop, 0) + m2.get(prop, 0)) / 2\n",
    "\n",
    "    # 计算S(m1, m_avg)和S(m2, m_avg)\n",
    "    S1, S2 = 0, 0\n",
    "    for prop in propositions:\n",
    "        p1 = m1.get(prop, 0)\n",
    "        p2 = m2.get(prop, 0)\n",
    "        p_avg = m_avg.get(prop, 0)\n",
    "\n",
    "        if p1 > 0 and p_avg > 0:\n",
    "            S1 += p1 * log(p1 / p_avg)\n",
    "        if p2 > 0 and p_avg > 0:\n",
    "            S2 += p2 * log(p2 / p_avg)\n",
    "\n",
    "    # 计算BJS散度\n",
    "    BJS = 0.5 * (S1 + S2)\n",
    "    return BJS\n",
    "\n",
    "\n",
    "def calculate_credibility_degree(evidences):\n",
    "    \"\"\"\n",
    "    计算证据的可信度(4.1节)\n",
    "\n",
    "    参数:\n",
    "        evidences: 证据列表，每个证据是一个BBA字典\n",
    "\n",
    "    返回:\n",
    "        credibility_degrees: 每个证据的可信度列表\n",
    "    \"\"\"\n",
    "    k = len(evidences)\n",
    "    if k == 0:\n",
    "        return []\n",
    "\n",
    "    # Step 1-1: 构建BJS散度矩阵\n",
    "    DMM = np.zeros((k, k))\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            if i != j:\n",
    "                DMM[i][j] = belief_jensen_shannon_divergence(evidences[i], evidences[j])\n",
    "\n",
    "    # Step 1-2: 计算每个证据的平均距离\n",
    "    avg_distances = []\n",
    "    for i in range(k):\n",
    "        sum_dist = sum(DMM[i])  # 对角线为0，所以直接求和\n",
    "        avg_dist = sum_dist / (k - 1)\n",
    "        avg_distances.append(avg_dist)\n",
    "\n",
    "    # Step 1-3: 计算支持度\n",
    "    supports = []\n",
    "    for dist in avg_distances:\n",
    "        if dist == 0:\n",
    "            supports.append(float('inf'))  # 理论上不应该发生\n",
    "        else:\n",
    "            supports.append(1 / dist)\n",
    "\n",
    "    # Step 1-4: 计算可信度\n",
    "    sum_support = sum(supports)\n",
    "    credibility_degrees = [sup / sum_support for sup in supports]\n",
    "\n",
    "    return credibility_degrees\n",
    "\n",
    "\n",
    "def measure_information_volume(bba):\n",
    "    \"\"\"\n",
    "    计算单个证据的信息量(公式19)\n",
    "    对应4.2节Step 2-2\n",
    "\n",
    "    参数:\n",
    "        bba: 基本置信分配字典，键为命题元组\n",
    "\n",
    "    返回:\n",
    "        信息量值\n",
    "    \"\"\"\n",
    "    entropy = 0\n",
    "    for prop, mass in bba.items():\n",
    "        if mass > 0:\n",
    "            cardinality = len(prop)  # 元组的长度即为命题的基数\n",
    "            denominator = (2 ** cardinality) - 1\n",
    "            entropy += mass * log(mass / denominator) if denominator != 0 else 0\n",
    "    return np.exp(-entropy)\n",
    "\n",
    "\n",
    "def normalize_information_volumes(evidences):\n",
    "    \"\"\"\n",
    "    计算并归一化所有证据的信息量(4.2节)\n",
    "\n",
    "    参数:\n",
    "        evidences: 证据列表\n",
    "\n",
    "    返回:\n",
    "        normalized_IVs: 归一化后的信息量列表\n",
    "    \"\"\"\n",
    "    # Step 2-1和Step 2-2: 计算每个证据的信息量\n",
    "    IVs = [measure_information_volume(ev) for ev in evidences]\n",
    "\n",
    "    # Step 2-3: 归一化信息量\n",
    "    sum_IV = sum(IVs)\n",
    "    normalized_IVs = [iv / sum_IV for iv in IVs]\n",
    "\n",
    "    return normalized_IVs\n",
    "\n",
    "\n",
    "def generate_weighted_average_evidence(evidences):\n",
    "    \"\"\"\n",
    "    生成并融合加权平均证据(4.3节)\n",
    "\n",
    "    参数:\n",
    "        evidences: 证据列表\n",
    "\n",
    "    返回:\n",
    "        WAE: 加权平均证据(BBA字典)\n",
    "    \"\"\"\n",
    "    k = len(evidences)\n",
    "    if k == 0:\n",
    "        return {}\n",
    "\n",
    "    # Step 1: 计算可信度\n",
    "    credibility_degrees = calculate_credibility_degree(evidences)\n",
    "\n",
    "    # Step 2: 计算归一化信息量\n",
    "    normalized_IVs = normalize_information_volumes(evidences)\n",
    "\n",
    "    # Step 3-1: 调整可信度\n",
    "    ACrd = [credibility_degrees[i] * normalized_IVs[i] for i in range(k)]\n",
    "\n",
    "    # Step 3-2: 归一化调整后的可信度\n",
    "    sum_ACrd = sum(ACrd)\n",
    "    final_weights = [acrd / sum_ACrd for acrd in ACrd]\n",
    "\n",
    "    # Step 3-3: 计算加权平均证据\n",
    "    WAE = {}\n",
    "    # 获取所有可能的命题\n",
    "    all_propositions = set()\n",
    "    for ev in evidences:\n",
    "        all_propositions.update(ev.keys())\n",
    "\n",
    "    for prop in all_propositions:\n",
    "        weighted_mass = 0\n",
    "        for i in range(k):\n",
    "            weighted_mass += final_weights[i] * evidences[i].get(prop, 0)\n",
    "        WAE[prop] = weighted_mass\n",
    "\n",
    "    return WAE\n",
    "\n",
    "\n",
    "def dempster_combination(m1, m2):\n",
    "    \"\"\"\n",
    "    Dempster合成规则(对应Definition 2.4和公式7-8)\n",
    "\n",
    "    参数:\n",
    "        m1, m2: 两个BBA字典，键为命题元组\n",
    "\n",
    "    返回:\n",
    "        融合后的BBA字典\n",
    "    \"\"\"\n",
    "    # 获取所有命题\n",
    "    propositions = set(m1.keys()).union(set(m2.keys()))\n",
    "\n",
    "    # 计算冲突系数K\n",
    "    K = 0\n",
    "    for b in m1:\n",
    "        for c in m2:\n",
    "            if not set(b).intersection(set(c)):  # 元组交集为空\n",
    "                K += m1[b] * m2[c]\n",
    "\n",
    "    # 合成规则\n",
    "    m = {}\n",
    "    for a in propositions:\n",
    "        mass = 0\n",
    "        for b in m1:\n",
    "            for c in m2:\n",
    "                if set(b).intersection(set(c)) == set(a):  # 元组交集等于a\n",
    "                    mass += m1[b] * m2[c]\n",
    "        if a:  # 非空集\n",
    "            m[a] = mass / (1 - K) if (1 - K) != 0 else 0\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "def xiao1_fuse_evidences(evidences):\n",
    "    \"\"\"\n",
    "    完整证据融合流程(对应4.3节Step 3-4)\n",
    "\n",
    "    参数:\n",
    "        evidences: 证据列表，每个证据是键为命题元组的BBA字典\n",
    "\n",
    "    返回:\n",
    "        融合后的最终BBA\n",
    "    \"\"\"\n",
    "    if not evidences:\n",
    "        return {}\n",
    "\n",
    "    # 获取加权平均证据\n",
    "    WAE = generate_weighted_average_evidence(evidences)\n",
    "\n",
    "    # 多次应用Dempster规则融合\n",
    "    result = WAE\n",
    "    for _ in range(len(evidences) - 1):\n",
    "        result = dempster_combination(result, WAE)\n",
    "\n",
    "    return result"
   ],
   "id": "9f3b3e5d6d80f0a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## xiao2",
   "id": "7763d963d724bee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def jousselme_distance(m1, m2):\n",
    "    \"\"\"\n",
    "    计算两个BBA之间的Jousselme距离\n",
    "    \"\"\"\n",
    "    focal_elements = set(m1.keys()).union(set(m2.keys()))\n",
    "    distance = 0\n",
    "    for A in focal_elements:\n",
    "        for B in focal_elements:\n",
    "            intersection = len(set(A).intersection(set(B))) / len(set(A).union(set(B))) if len(\n",
    "                set(A).union(set(B))) != 0 else 0\n",
    "            distance += (m1.get(A, 0) - m2.get(A, 0)) * (m1.get(B, 0) - m2.get(B, 0)) * intersection\n",
    "    return np.sqrt(0.5 * distance)\n",
    "\n",
    "\n",
    "def measure_evidential_credibility(evidence):\n",
    "    \"\"\"\n",
    "    测量证据可信度 (4.1节)\n",
    "    \"\"\"\n",
    "    k = len(evidence)\n",
    "\n",
    "    # Step 1-1: 构建相似性矩阵\n",
    "    SMM = np.zeros((k, k))\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            if i == j:\n",
    "                SMM[i, j] = 1  # 与自身的相似度为1\n",
    "            else:\n",
    "                d_ij = jousselme_distance(evidence[i], evidence[j])\n",
    "                SMM[i, j] = 1 - d_ij  # Eq.(10)\n",
    "\n",
    "    # Step 1-2: 计算支持度\n",
    "    SD = np.zeros(k)\n",
    "    for i in range(k):\n",
    "        SD[i] = np.sum(SMM[i, :]) - 1  # 减去与自身的相似度 (Eq.12)\n",
    "\n",
    "    # Step 1-3: 计算局部可信度\n",
    "    CD = SD / np.sum(SD)  # Eq.(13)\n",
    "\n",
    "    # Step 1-4: 计算全局可信度\n",
    "    CDg = np.mean(CD)  # Eq.(14)\n",
    "\n",
    "    return CD, CDg\n",
    "\n",
    "\n",
    "def adjust_evidential_credibility(CD, CDg, omega=2.25, xi=0.88, eta=0.88):\n",
    "    \"\"\"\n",
    "    调整证据可信度 (4.2节)\n",
    "    \"\"\"\n",
    "    k = len(CD)\n",
    "\n",
    "    # Step 2-1: 调整可信度\n",
    "    ACD = np.zeros(k)\n",
    "    for i in range(k):\n",
    "        diff = CD[i] - CDg\n",
    "        if diff >= 0:\n",
    "            ACD[i] = diff ** xi  # Eq.(17) 第一种情况\n",
    "        else:\n",
    "            ACD[i] = -omega * ((-diff) ** eta)  # Eq.(17) 第二种情况\n",
    "\n",
    "    # Step 2-2: 归一化\n",
    "    min_ACD = np.min(ACD)\n",
    "    max_ACD = np.max(ACD)\n",
    "    ACD_bar = (ACD - min_ACD) / (max_ACD - min_ACD)  # Eq.(18)\n",
    "\n",
    "    # Step 2-3: 指数函数处理\n",
    "    exp_ACD = np.exp(ACD_bar)\n",
    "    ACD_tilde = exp_ACD / np.sum(exp_ACD)  # Eq.(19)\n",
    "\n",
    "    return ACD_tilde\n",
    "\n",
    "\n",
    "def generate_weighted_mean_evidence(evidence, weights):\n",
    "    \"\"\"\n",
    "    生成加权平均证据 (4.3节 Step 3-1)\n",
    "    \"\"\"\n",
    "    # 获取所有证据的焦元并集\n",
    "    focal_elements = set()\n",
    "    for m in evidence:\n",
    "        focal_elements.update(m.keys())\n",
    "\n",
    "    # 创建加权平均证据\n",
    "    WME = {}\n",
    "    for fe in focal_elements:\n",
    "        WME[fe] = 0.0\n",
    "        for i in range(len(evidence)):\n",
    "            WME[fe] += weights[i] * evidence[i].get(fe, 0.0)\n",
    "\n",
    "    return WME\n",
    "\n",
    "\n",
    "def dempster_combine(m1, m2):\n",
    "    \"\"\"\n",
    "    DS组合规则 (DCR) (Definition 2.3)\n",
    "    \"\"\"\n",
    "    # 获取所有焦元\n",
    "    focal_elements = set(m1.keys()).union(set(m2.keys()))\n",
    "\n",
    "    # 计算冲突系数K (Eq.6)\n",
    "    K = 0\n",
    "    for A in m1:\n",
    "        for B in m2:\n",
    "            if len(set(A).intersection(set(B))) == 0:\n",
    "                K += m1[A] * m2[B]\n",
    "\n",
    "    # 组合证据\n",
    "    m_combined = {}\n",
    "    for C in focal_elements:\n",
    "        if C == ():  # 空集\n",
    "            m_combined[C] = 0.0\n",
    "        else:\n",
    "            sum_val = 0.0\n",
    "            for A in m1:\n",
    "                for B in m2:\n",
    "                    if set(A).intersection(set(B)) == set(C):\n",
    "                        sum_val += m1[A] * m2[B]\n",
    "            m_combined[C] = sum_val / (1 - K) if (1 - K) != 0 else 0\n",
    "\n",
    "    return m_combined\n",
    "\n",
    "\n",
    "def xiao2_evidence_fusion(evidence):\n",
    "    \"\"\"\n",
    "    完整的证据融合算法\n",
    "    \"\"\"\n",
    "    # 1. 测量证据可信度\n",
    "    CD, CDg = measure_evidential_credibility(evidence)\n",
    "\n",
    "    # 2. 调整证据可信度\n",
    "    weights = adjust_evidential_credibility(CD, CDg)\n",
    "\n",
    "    # 3. 生成加权平均证据\n",
    "    WME = generate_weighted_mean_evidence(evidence, weights)\n",
    "\n",
    "    # 4. 组合证据 (k-1次)\n",
    "    combined = WME\n",
    "    for _ in range(len(evidence) - 1):\n",
    "        combined = dempster_combine(combined, WME)\n",
    "    return combined"
   ],
   "id": "cd74363a135f4e78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10.1.1 准确率测试",
   "id": "6550463586db4402"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "from sklearn.datasets import load_iris\n",
    "from itertools import chain, combinations\n",
    "\n",
    "\n",
    "class IrisBPAAnalyzer:\n",
    "    def __init__(self, train_samples=20, test_samples=10):\n",
    "        self.iris = load_iris()\n",
    "        self.train_samples = train_samples\n",
    "        self.test_samples = test_samples\n",
    "        self.class_models = {}\n",
    "        self.test_models = {}\n",
    "        self.fod = ['S', 'E', 'V']  # Setosa, Versicolor, Virginica\n",
    "        self.feature_names = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']\n",
    "\n",
    "    def _prepare_data(self, target_class='E'):\n",
    "        \"\"\"准备训练集和测试集\"\"\"\n",
    "        data = self.iris.data\n",
    "        target_idx = self.fod.index(target_class) if target_class in self.fod else 1  # 默认为E\n",
    "\n",
    "        # 为每个类别创建训练模型\n",
    "        for i, class_name in enumerate(self.iris.target_names):\n",
    "            class_data = data[self.iris.target == i]\n",
    "            train_indices = np.random.choice(len(class_data), self.train_samples, replace=False)\n",
    "            self.class_models[self.fod[i]] = {\n",
    "                'mean': np.mean(class_data[train_indices], axis=0),\n",
    "                'std': np.std(class_data[train_indices], axis=0, ddof=1),\n",
    "                'data': class_data[train_indices]\n",
    "            }\n",
    "\n",
    "        # 为指定类别创建测试模型（每个属性单独建模）\n",
    "        target_data = data[self.iris.target == target_idx]\n",
    "        remaining_indices = [i for i in range(len(target_data))\n",
    "                             if i not in self.class_models[target_class]['data']]\n",
    "        test_indices = np.random.choice(remaining_indices, self.test_samples, replace=False)\n",
    "\n",
    "        for i in range(4):  # 四个属性\n",
    "            self.test_models[self.feature_names[i]] = {\n",
    "                'mean': np.mean(target_data[test_indices, i]),\n",
    "                'std': np.std(target_data[test_indices, i], ddof=1),\n",
    "                'data': target_data[test_indices, i]\n",
    "            }\n",
    "\n",
    "    def _gaussian_mf(self, x, mean, std):\n",
    "        \"\"\"高斯隶属度函数\"\"\"\n",
    "        return np.exp(-0.5 * ((x - mean) / std) ** 2)\n",
    "\n",
    "    def _calculate_integral_interval(self, *params):\n",
    "        \"\"\"计算积分区间\"\"\"\n",
    "        bounds = []\n",
    "        for mean, std in params:\n",
    "            bounds.extend([mean + 3 * std, mean - 3 * std])\n",
    "        return min(bounds), max(bounds)\n",
    "\n",
    "    def _compute_intersection_area(self, mf_funcs, params):\n",
    "        \"\"\"计算曲线交集面积\"\"\"\n",
    "\n",
    "        def min_mf(x):\n",
    "            return min(mf(x) for mf in mf_funcs)\n",
    "\n",
    "        a, b = self._calculate_integral_interval(*params)\n",
    "        area, _ = quad(min_mf, a, b, limit=100)\n",
    "        return area\n",
    "\n",
    "    def _powerset(self, elements):\n",
    "        \"\"\"生成幂集（不包括空集）\"\"\"\n",
    "        return list(chain.from_iterable(combinations(elements, r)\n",
    "                                        for r in range(1, len(elements) + 1)))\n",
    "\n",
    "    def calculate_bpa_for_feature(self, feature_idx):\n",
    "        \"\"\"\n",
    "        计算单个特征的BPA\n",
    "        :param feature_idx: 使用的特征索引（0-3）\n",
    "        :return: 格式化后的BPA字典\n",
    "        \"\"\"\n",
    "        feature_name = self.feature_names[feature_idx]\n",
    "        test_mean = self.test_models[feature_name]['mean']\n",
    "        test_std = self.test_models[feature_name]['std']\n",
    "        test_area = np.sqrt(2 * np.pi) * test_std\n",
    "\n",
    "        # 计算各命题支持度\n",
    "        sup_dict = {}\n",
    "        elements = self.fod.copy()\n",
    "        propositions = self._powerset(elements)\n",
    "\n",
    "        # 计算所有命题的支持度\n",
    "        for prop in propositions:\n",
    "            params = [(self.class_models[c]['mean'][feature_idx],\n",
    "                       self.class_models[c]['std'][feature_idx]) for c in prop]\n",
    "            params.append((test_mean, test_std))\n",
    "\n",
    "            test_mf = lambda x: self._gaussian_mf(x, test_mean, test_std)\n",
    "            fault_mfs = [lambda x, m=self.class_models[c]['mean'][feature_idx],\n",
    "                                s=self.class_models[c]['std'][feature_idx]:\n",
    "                         self._gaussian_mf(x, m, s) for c in prop]\n",
    "\n",
    "            area = self._compute_intersection_area([test_mf, *fault_mfs], params)\n",
    "            sup_dict[tuple(sorted(prop))] = area / test_area  # 使用排序后的元组作为键\n",
    "\n",
    "        # 归一化\n",
    "        total_sup = sum(sup_dict.values())\n",
    "        bpa = {k: v / total_sup for k, v in sup_dict.items()}\n",
    "\n",
    "        return bpa\n",
    "\n",
    "    def generate_evidence(self, target_class='E', num_tests=1):\n",
    "        \"\"\"\n",
    "        生成证据体数组\n",
    "        :param target_class: 目标类别 ('S', 'E', 'V')\n",
    "        :param num_tests: 要生成的证据体数量\n",
    "        :return: 证据体数组，每个元素是一个包含4个BPA的列表\n",
    "        \"\"\"\n",
    "        evidence_array = []\n",
    "        for _ in range(num_tests):\n",
    "            self._prepare_data(target_class)\n",
    "            evidence = []\n",
    "            for i in range(4):  # 四个特征\n",
    "                bpa = self.calculate_bpa_for_feature(i)\n",
    "                evidence.append(bpa)\n",
    "            evidence_array.append(evidence)\n",
    "        return evidence_array"
   ],
   "id": "1392ed80a9aee615"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_accuracy():\n",
    "    \"\"\"评估融合准确率（测试版本，只使用50条证据体）\"\"\"\n",
    "    analyzer = IrisBPAAnalyzer(train_samples=20, test_samples=10)\n",
    "\n",
    "    # 定义所有假设（对应S, E, V）\n",
    "    all_hypotheses = ['S', 'E', 'V']\n",
    "    test_size = 10000  # 测试用，只生成50条\n",
    "\n",
    "    # 为每个类别生成证据体\n",
    "    print(\"正在为S类生成证据体...\")\n",
    "    evidence_S = analyzer.generate_evidence('S', test_size)\n",
    "    print(\"正在为E类生成证据体...\")\n",
    "    evidence_E = analyzer.generate_evidence('E', test_size)\n",
    "    print(\"正在为V类生成证据体...\")\n",
    "    evidence_V = analyzer.generate_evidence('V', test_size)\n",
    "\n",
    "    def test_evidence(evidence_list, true_label, test_function, func_name=\"\", total=test_size):\n",
    "        \"\"\"通用测试函数\"\"\"\n",
    "        correct = 0\n",
    "        for i, evidence in enumerate(evidence_list, 1):\n",
    "            # 显示进度（每10条显示一次）\n",
    "            if i % 2000 == 0 or i == total:\n",
    "                print(f\"\\r{func_name} 测试{true_label}类: 进度 {i}/{total} ({i/total*100:.1f}%)\", end=\"\")\n",
    "\n",
    "            # 根据函数类型调用不同的参数形式\n",
    "            if test_function.__name__ == \"main_function\":\n",
    "                result = test_function(evidence, all_hypotheses)\n",
    "            else:\n",
    "                result = test_function(evidence)\n",
    "\n",
    "             # 修改后的代码：处理元组key的情况\n",
    "            max_key = max(result, key=result.get)\n",
    "            predicted = max_key[0] if isinstance(max_key, tuple) else max_key\n",
    "            if predicted == true_label:\n",
    "                correct += 1\n",
    "        print()  # 换行\n",
    "        return correct\n",
    "\n",
    "    print(\"\\n开始测试main_function...\")\n",
    "    # 测试main_function（传入all_hypotheses）\n",
    "    correct_S = test_evidence(evidence_S, 'S', main_function, \"main_function\")\n",
    "    correct_E = test_evidence(evidence_E, 'E', main_function, \"main_function\")\n",
    "    correct_V = test_evidence(evidence_V, 'V', main_function, \"main_function\")\n",
    "\n",
    "    print(\"\\n开始测试xiao1...\")\n",
    "    # 测试xiao1\n",
    "    xiao1_correct_S = test_evidence(evidence_S, 'S', xiao1_fuse_evidences, \"xiao1\")\n",
    "    xiao1_correct_E = test_evidence(evidence_E, 'E', xiao1_fuse_evidences, \"xiao1\")\n",
    "    xiao1_correct_V = test_evidence(evidence_V, 'V', xiao1_fuse_evidences, \"xiao1\")\n",
    "\n",
    "    print(\"\\n开始测试xiao2...\")\n",
    "    # 测试xiao2\n",
    "    xiao2_correct_S = test_evidence(evidence_S, 'S', xiao2_evidence_fusion, \"xiao2\")\n",
    "    xiao2_correct_E = test_evidence(evidence_E, 'E', xiao2_evidence_fusion, \"xiao2\")\n",
    "    xiao2_correct_V = test_evidence(evidence_V, 'V', xiao2_evidence_fusion, \"xiao2\")\n",
    "\n",
    "    # 计算准确率\n",
    "    def calculate_accuracy(correct, total=test_size):\n",
    "        return correct / total\n",
    "\n",
    "    # main_function准确率\n",
    "    accuracy_S = calculate_accuracy(correct_S)\n",
    "    accuracy_E = calculate_accuracy(correct_E)\n",
    "    accuracy_V = calculate_accuracy(correct_V)\n",
    "    total_accuracy = (correct_S + correct_E + correct_V) / (3 * test_size)\n",
    "\n",
    "    # xiao1准确率\n",
    "    xiao1_accuracy_S = calculate_accuracy(xiao1_correct_S)\n",
    "    xiao1_accuracy_E = calculate_accuracy(xiao1_correct_E)\n",
    "    xiao1_accuracy_V = calculate_accuracy(xiao1_correct_V)\n",
    "    xiao1_total_accuracy = (xiao1_correct_S + xiao1_correct_E + xiao1_correct_V) / (3 * test_size)\n",
    "\n",
    "    # xiao2准确率\n",
    "    xiao2_accuracy_S = calculate_accuracy(xiao2_correct_S)\n",
    "    xiao2_accuracy_E = calculate_accuracy(xiao2_correct_E)\n",
    "    xiao2_accuracy_V = calculate_accuracy(xiao2_correct_V)\n",
    "    xiao2_total_accuracy = (xiao2_correct_S + xiao2_correct_E + xiao2_correct_V) / (3 * test_size)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"\\n=== 融合准确率测试结果（测试样本数：10000/类） ===\")\n",
    "    print(\"\\n--- main_function ---\")\n",
    "    print(f\"S类准确率: {accuracy_S:.4f} ({correct_S}/{test_size})\")\n",
    "    print(f\"E类准确率: {accuracy_E:.4f} ({correct_E}/{test_size})\")\n",
    "    print(f\"V类准确率: {accuracy_V:.4f} ({correct_V}/{test_size})\")\n",
    "    print(f\"总体准确率: {total_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\n--- xiao1 ---\")\n",
    "    print(f\"S类准确率: {xiao1_accuracy_S:.4f} ({xiao1_correct_S}/{test_size})\")\n",
    "    print(f\"E类准确率: {xiao1_accuracy_E:.4f} ({xiao1_correct_E}/{test_size})\")\n",
    "    print(f\"V类准确率: {xiao1_accuracy_V:.4f} ({xiao1_correct_V}/{test_size})\")\n",
    "    print(f\"总体准确率: {xiao1_total_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\n--- xiao2 ---\")\n",
    "    print(f\"S类准确率: {xiao2_accuracy_S:.4f} ({xiao2_correct_S}/{test_size})\")\n",
    "    print(f\"E类准确率: {xiao2_accuracy_E:.4f} ({xiao2_correct_E}/{test_size})\")\n",
    "    print(f\"V类准确率: {xiao2_accuracy_V:.4f} ({xiao2_correct_V}/{test_size})\")\n",
    "    print(f\"总体准确率: {xiao2_total_accuracy:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'main_function': {\n",
    "            'S_accuracy': accuracy_S,\n",
    "            'E_accuracy': accuracy_E,\n",
    "            'V_accuracy': accuracy_V,\n",
    "            'total_accuracy': total_accuracy\n",
    "        },\n",
    "        'xiao1': {\n",
    "            'S_accuracy': xiao1_accuracy_S,\n",
    "            'E_accuracy': xiao1_accuracy_E,\n",
    "            'V_accuracy': xiao1_accuracy_V,\n",
    "            'total_accuracy': xiao1_total_accuracy\n",
    "        },\n",
    "        'xiao2': {\n",
    "            'S_accuracy': xiao2_accuracy_S,\n",
    "            'E_accuracy': xiao2_accuracy_E,\n",
    "            'V_accuracy': xiao2_accuracy_V,\n",
    "            'total_accuracy': xiao2_total_accuracy\n",
    "        }\n",
    "    }"
   ],
   "id": "d8cd274732b8c07a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 准确率测试入口",
   "id": "b49c3ed99cb9d97e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "evaluate_accuracy()",
   "id": "15eb8b78e0f679b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10.2 强干扰模拟测试",
   "id": "fee170e9f3a051e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 实验设置\n",
    "# x_values = np.linspace(0, 0.95, 11)\n",
    "x_values = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "results = []\n",
    "\n",
    "for x in x_values:\n",
    "    # evidence = [\n",
    "    #     {('A',): 0.6, ('B',): 0.3, ('C',): 0.1},\n",
    "    #     {('A',): 0.5 - x, ('B',): 0.4, ('C',): 0.1 + x},\n",
    "    #     {('A',): 0.7, ('B',): 0.2, ('C',): 0.1}\n",
    "    # ]\n",
    "    evidence = [\n",
    "        {('A',): 0.9, ('B',): 0.05, ('C',): 0.05},\n",
    "        {('A',): 0.95 - x, ('B',): 0.05, ('C',): x},\n",
    "        {('A',): 0.9, ('B',): 0.05, ('C',): 0.05}\n",
    "    ]\n",
    "\n",
    "\n",
    "    normalized_evidence = normalize_evidence(evidence)\n",
    "    S = compute_similarity_matrix(normalized_evidence)\n",
    "    R = new_compute_weight_from_similarity_gaussian(S)\n",
    "    fused_probabilities = fuse_evidence(normalized_evidence, R, alpha=0.1)\n",
    "\n",
    "    results.append((x, fused_probabilities))\n",
    "\n",
    "# 输出结果\n",
    "for x, fused in results:\n",
    "    print(f\"Interference x = {x:.2f}: Fused probabilities = {fused}\")"
   ],
   "id": "6c847231be1e1391"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 多种相似度计算测试",
   "id": "30fe6b6724e365bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 你的证据体\n",
    "# evidence = [\n",
    "#     {('A',): 0.7, ('B',): 0.15, ('C',): 0.15},\n",
    "#     {('B',): 0.5, ('C',): 0.5},\n",
    "#     {('A',): 0.7, ('B',): 0.15, ('C',): 0.15},\n",
    "#     {('A',): 0.7, ('B',): 0.15, ('C',): 0.15},\n",
    "#     {('A', 'C'): 0.8, ('B',): 0.2}\n",
    "# ]\n",
    "\n",
    "# evidence = [\n",
    "#     {('A',): 0.5, ('B',): 0.2, ('C',): 0.3},\n",
    "#     {('B',): 0.9, ('C',): 0.1},\n",
    "#     {('A',): 0.55, ('B',): 0.1, ('A','C'): 0.35},\n",
    "#     {('A',): 0.55, ('B',): 0.1, ('A','C'): 0.35},\n",
    "#     {('A',): 0.6, ('B',): 0.1, ('A', 'C'): 0.3}\n",
    "# ]\n",
    "\n",
    "evidence = [\n",
    "    {('A',): 0.0437, ('B',): 0.3346, ('C',): 0.2916, ('A', 'B'): 0.0437, ('A', 'C'): 0.0239, ('B', 'C'): 0.2385, ('A', 'B', 'C'): 0.0239},  # 证据体 1 (Sepal Length)\n",
    "    {('A',): 0.0865, ('B',): 0.2879, ('C',): 0.1839, ('A', 'B'): 0.0863, ('A', 'C'): 0.0865, ('B', 'C'): 0.1825, ('A', 'B', 'C'): 0.0863},  # 证据体 2 (Sepal Width)\n",
    "    {('A',): 1.4e-09, ('B',): 0.6570, ('C',): 0.1726, ('A', 'B'): 1.3e-09, ('A', 'C'): 1.4e-11, ('B', 'C'): 0.1704, ('A', 'B', 'C'): 1.4e-11},  # 证据体 3 (Petal Length)\n",
    "    {('A',): 8.20e-06, ('B',): 0.6616, ('C',): 0.1692, ('A', 'B'): 8.20e-06, ('A', 'C'): 3.80e-06, ('B', 'C'): 0.1692, ('A', 'B', 'C'): 3.80e-06}   # 证据体 4 (Petal Width)\n",
    "]\n",
    "# 处理证据体格式\n",
    "normalized_evidence = normalize_evidence(evidence)\n",
    "\n",
    "methods = [\"inverse\", \"inverse_quadratic\", \"logarithmic\", \"gaussian\",\"quadratic\",\"exponential\"]\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    # 计算相似度矩阵\n",
    "    S = compute_similarity_matrix_jousselme(normalized_evidence,method)\n",
    "\n",
    "    # 计算权重 R\n",
    "    R =new_compute_weight_from_similarity_nonlinear(S)\n",
    "\n",
    "    # 计算融合概率\n",
    "    alpha = 0.1  # 乐观系数\n",
    "    fused_probabilities = fuse_evidence(normalized_evidence, R, alpha)\n",
    "    print(\"Optimal R:\", R)\n",
    "    print(\"Fused probabilities (after SLF):\", fused_probabilities)\n"
   ],
   "id": "f2b2793bbfb3503"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 实验部分",
   "id": "abd7affec940afdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 我将要分为4个实验，说明我的算法的优越性",
   "id": "8a50af12e1116dca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 实验 1：随机扰动噪声",
   "id": "894c2c7b34182644"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 目标： 检查融合算法对小范围扰动的稳定性",
   "id": "e7b6d97548d9770a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 预期效果：期望融合结果不会因为小幅度扰动产生剧烈变化。",
   "id": "155038bb84b0cdd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_experiments = 20  # 进行 20 次实验\n",
    "results = []\n",
    "for i in range(n_experiments):\n",
    "    noise = np.random.uniform(-0.02, 0.02)  # 随机噪声在 [-0.02, 0.02] 之间\n",
    "    evidence = [\n",
    "        {('A',): 0.9 + noise, ('B',): 0.05 - noise / 2, ('C',): 0.05 - noise / 2},\n",
    "        {('A',): 0.95 - noise, ('B',): 0.05, ('C',): noise},\n",
    "        {('A',): 0.9 - noise, ('B',): 0.05 + noise / 2, ('C',): 0.05 + noise / 2}\n",
    "    ]\n",
    "    fused_probabilities = main_function(evidence)\n",
    "    results.append((noise, fused_probabilities))\n",
    "\n",
    "# 输出结果\n",
    "for x, fused in results:\n",
    "    print(f\"Interference x = {x:.2f}: Fused probabilities = {fused}\")"
   ],
   "id": "abc5aa7e39b72e10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 实验2：对抗性噪声",
   "id": "93646d85260bec3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 目标： 添加刻意的强对抗性干扰，观察算法是否仍然能稳定融合\n",
    "\n"
   ],
   "id": "7f3472809f8b24b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 预期效果：算法应能识别出对抗性噪声，并避免被强烈误导。\n",
   "id": "5c3e620b3acfbd17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "\n",
    "results = []\n",
    "resultsAll = []\n",
    "noiseResult = []\n",
    "for i in range(30):  # 30 次实验\n",
    "    noise = np.random.uniform(0, 0.95)  # 施加对抗性扰动\n",
    "    evidence = [\n",
    "        {('A',): 0.9, ('B',): 0.05, ('C',): 0.05},\n",
    "        {('A',): 0.95 - noise, ('B',): 0.05, ('C',): noise},\n",
    "        # {('A',): 0.9, ('B',): 0.05, ('C',): 0.05},\n",
    "        {('A',): 0.6, ('B',): 0.2, ('C',): 0.2}  # 人为制造干扰\n",
    "    ]\n",
    "    fused_probabilities = main_function(evidence)\n",
    "    noiseResult.append(noise)\n",
    "    results.append(fused_probabilities)\n",
    "    resultsAll.append((noise, fused_probabilities))\n",
    "\n",
    "# # 输出结果\n",
    "for x, fused in resultsAll:\n",
    "    print(f\"Interference x = {x:.2f}: Fused probabilities = {fused}\")\n",
    "# 选取第一个probabilities作为基准分布\n",
    "p_ref = np.array(list(results[0].values()))\n",
    "\n",
    "# 计算KL散度\n",
    "kl_divergence = []\n",
    "for prob in results:\n",
    "    p = np.array(list(prob.values()))\n",
    "    kl_divergence.append(entropy(p, p_ref))\n",
    "\n",
    "# 绘制折线图\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(x=range(len(kl_divergence)), y=kl_divergence, marker='o', linewidth=2.5)\n",
    "plt.xlabel(\"Experiment Index\")\n",
    "plt.ylabel(\"KL Divergence\")\n",
    "# plt.title(\"KL Divergence of Fused Probabilities\")\n",
    "plt.show()"
   ],
   "id": "f21deacfe250e892"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 实验3：分布不均衡",
   "id": "c95dbe6c4d912bb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 目标： 给部分证据极端权重，测试融合策略的应对能力",
   "id": "4427e0655be1bd80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 预期效果：看算法是否会因极端值改变决策，或者能稳定融合主要证据。\n",
   "id": "c489ac91725a4573"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "evidence = [\n",
    "    {('A',): 0.9, ('B',): 0.05, ('C',): 0.05},\n",
    "    {('A',): 0.01, ('B',): 0.01, ('C',): 0.98},  # 极端倾向于 C\n",
    "    {('A',): 0.9, ('B',): 0.05, ('C',): 0.05}\n",
    "]\n",
    "fused_probabilities = main_function(evidence)"
   ],
   "id": "e6311d4976c75879"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 实验4：极端情况",
   "id": "46bbb073db2487f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 目标： 直接将某些证据值拉到极端，测试算法的收敛性",
   "id": "9d4c75b53bdddbdc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 预期效果：如果融合算法有鲁棒性，应不会让 C 直接取代所有决策。",
   "id": "f1277ccc551ba7bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "evidence = [\n",
    "    {('A',): 1.0, ('B',): 0.0, ('C',): 0.0},  # A 100% 确定\n",
    "    {('A',): 0.0, ('B',): 0.0, ('C',): 1.0},  # C 100% 确定\n",
    "    {('A',): 0.5, ('B',): 0.25, ('C',): 0.25}  # 平衡分布\n",
    "]\n",
    "main_function(evidence)"
   ],
   "id": "b37069ae421de10a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 11 图片生成\n",
   "id": "ab016ff05033950f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 实验一图片",
   "id": "607f3ca6b6c271dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 实验一 第二版",
   "id": "81df2f0caea65bd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# 准备数据\n",
    "error_data = {\n",
    "    'Interference x': [0.00, -0.01, 0.02, 0.02, -0.00, -0.01, -0.02, 0.00, 0.01, 0.02,\n",
    "                      0.00, -0.01, -0.01, -0.01, 0.01, -0.01, 0.01, 0.02, 0.00, -0.01],\n",
    "    'X1': [0.9999999164727388, 0.9999999392531336, 0.999999915656743, 0.9999999179921214, 0.9999999183218997,\n",
    "           0.999999928665607, 0.9999999412241183, 0.999999916426141, 0.9999999175927867, 0.9999999154647136,\n",
    "           0.9999999181373291, 0.9999999251703227, 0.9999999255262123, 0.9999999261824465, 0.9999999176532518,\n",
    "           0.9999999260374246, 0.999999917860726, 0.999999915995379, 0.999999918245941, 0.9999999280085583],\n",
    "    'X2': [7.08721616044662e-08, 5.6503262910144105e-08, 6.331263082929622e-08, 6.245209362995414e-08, 7.073493608512042e-08,\n",
    "           6.625519246889052e-08, 5.594896514990959e-08, 7.105347585166414e-08, 6.642172791763447e-08, 6.338106336242535e-08,\n",
    "           6.751724723539344e-08, 6.744805240638906e-08, 6.734373741901626e-08, 6.714139228802903e-08, 6.656559281255935e-08,\n",
    "           6.7187236216174e-08, 6.70121787326018e-08, 6.31911613516391e-08, 6.769259213926965e-08, 6.650866287582448e-08],\n",
    "    'X3': [1.2395484237242879e-08, 3.987592269574234e-09, 2.0766311511957138e-08, 1.9292291264549646e-08, 1.0684245777738602e-08,\n",
    "           4.822735602279193e-09, 2.5720594288916194e-09, 1.2260851368388372e-08, 1.572386317604222e-08, 2.0889839723089364e-08,\n",
    "           1.4084778532141106e-08, 7.12408856042652e-09, 6.8726220404778426e-09, 6.4189329182583274e-09, 1.5519654060300863e-08,\n",
    "           6.518066656057564e-09, 1.4865982014607078e-08, 2.0549265909690343e-08, 1.3800992696543222e-08, 5.2261107714817545e-09]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(error_data)\n",
    "\n",
    "# 设置基准分布Q（第一条数据）\n",
    "q = np.array([\n",
    "    df.loc[0, 'X1'],\n",
    "    df.loc[0, 'X2'],\n",
    "    df.loc[0, 'X3']\n",
    "])\n",
    "epsilon = 1e-20\n",
    "q = np.clip(q, epsilon, 1)\n",
    "q = q / q.sum()  # 归一化\n",
    "\n",
    "# 计算各数据点与基准分布的KL散度\n",
    "kl_results = []\n",
    "for idx, row in df.iterrows():\n",
    "    p = np.array([row['X1'], row['X2'], row['X3']])\n",
    "    p = np.clip(p, epsilon, 1)\n",
    "    p = p / p.sum()\n",
    "\n",
    "    # 计算KL(P||Q)\n",
    "    kl_divergence = entropy(p, q)\n",
    "    kl_results.append(kl_divergence)\n",
    "\n",
    "df['KL_Divergence'] = kl_results\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df.index, df['KL_Divergence'],\n",
    "         marker='o', linestyle='-', color='royalblue', linewidth=2,\n",
    "         label=f'KL(P||Q), Q=first data (x={df.loc[0, \"Interference x\"]:.2f})')\n",
    "\n",
    "# 标注关键点\n",
    "for i, row in df.iterrows():\n",
    "    if i == 0:  # 特殊标注基准点\n",
    "        plt.scatter(i, row['KL_Divergence'], color='black', s=150, zorder=5,\n",
    "                   label='Reference Point (KL=0)')\n",
    "    elif i in [df['KL_Divergence'].idxmax(), df['KL_Divergence'].idxmin()]:\n",
    "        color = 'red' if i == df['KL_Divergence'].idxmax() else 'green'\n",
    "        plt.scatter(i, row['KL_Divergence'], color=color, s=150, zorder=5,\n",
    "                   label=f'{\"Max\" if color==\"red\" else \"Min\"} KL: {row[\"KL_Divergence\"]:.1e}\\n(x={row[\"Interference x\"]:.2f})')\n",
    "    else:\n",
    "        plt.annotate(f\"x={row['Interference x']:.2f}\",\n",
    "                     xy=(i, row['KL_Divergence']),\n",
    "                     xytext=(5, 5), textcoords='offset points',\n",
    "                     fontsize=12, alpha=0.7)\n",
    "\n",
    "# 图表美化\n",
    "plt.xlabel('Data Index')\n",
    "plt.ylabel('KL Divergence (bits)')\n",
    "# plt.title('KL Divergence from First Data Point Distribution', fontsize=14)\n",
    "plt.xticks(df.index)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.legend(loc='upper right')  # 修改这一行\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"experiment1_new.pdf\", bbox_inches='tight', transparent=True)\n",
    "plt.show()\n",
    "\n",
    "# 输出基准分布和统计信息\n",
    "print(\"\\n基准分布Q（第一条数据）：\")\n",
    "print(f\"X1: {q[0]:.6f}, X2: {q[1]:.2e}, X3: {q[2]:.2e}\")\n",
    "print(\"\\nKL散度统计摘要：\")\n",
    "print(df[['Interference x', 'KL_Divergence']].describe())"
   ],
   "id": "363a8327d820dd17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 实验二图片2",
   "id": "dc9b8a66a7296f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==================== 数据准备 ====================\n",
    "data = {\n",
    "    \"Noise ratio (x)\": [0.10, 0.43, 0.57, 0.49, 0.83, 0.95, 0.01, 0.69, 0.81, 0.32,\n",
    "                        0.77, 0.87, 0.64, 0.66, 0.41, 0.55, 0.55, 0.15, 0.50, 0.54,\n",
    "                        0.59, 0.42, 0.27, 0.56, 0.50, 0.55, 0.30, 0.92, 0.29, 0.80],\n",
    "    \"Z1\": [0.995941, 0.966113, 0.951420, 0.961108, 0.915939, 0.903485, 0.998457,\n",
    "           0.935667, 0.918970, 0.977110, 0.924717, 0.910651, 0.942828, 0.940711,\n",
    "           0.968090, 0.954321, 0.954712, 0.993076, 0.960004, 0.956199, 0.949295,\n",
    "           0.967424, 0.982889, 0.952590, 0.959651, 0.953881, 0.979587, 0.905313,\n",
    "           0.980090, 0.919834],\n",
    "    \"Z2\": [0.001349, 0.003700, 0.005579, 0.003930, 0.015726, 0.021153, 0.001153,\n",
    "           0.009389, 0.014618, 0.003136, 0.012647, 0.017824, 0.007543, 0.008063,\n",
    "           0.003609, 0.005022, 0.004949, 0.001691, 0.003977, 0.004721, 0.006025,\n",
    "           0.003639, 0.002720, 0.005350, 0.004025, 0.005105, 0.002966, 0.020168,\n",
    "           0.002929, 0.014310],\n",
    "    \"Z3\": [0.002709, 0.030188, 0.043001, 0.034962, 0.068335, 0.075362, 0.000390,\n",
    "           0.054945, 0.066412, 0.019754, 0.062636, 0.071524, 0.049629, 0.051226,\n",
    "           0.028301, 0.040656, 0.040339, 0.005233, 0.036019, 0.039080, 0.044680,\n",
    "           0.028937, 0.014391, 0.042060, 0.036324, 0.041014, 0.017447, 0.074520,\n",
    "           0.016981, 0.065856]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==================== 可视化：箱线图 ====================\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    data=df.drop(columns=[\"Noise ratio (x)\"]),\n",
    "    palette={\"Z1\": \"blue\", \"Z2\": \"red\", \"Z3\": \"green\"}\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Target class\")\n",
    "plt.ylabel(\"Predicted probability\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"boxplot_probability_distribution.pdf\", bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()"
   ],
   "id": "945dfb0d0ce1a854"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==================== 数据准备 ====================\n",
    "# (x = 噪声比例, y = Z1类别的预测概率)\n",
    "data = [\n",
    "    (0.10, 0.9959413150116919), (0.43, 0.9661124976699548), (0.57, 0.9514195558549194),\n",
    "    (0.49, 0.9611077872545515), (0.83, 0.915938988391142), (0.95, 0.9034845903635726),\n",
    "    (0.01, 0.9984568515413714), (0.69, 0.9356666105918825), (0.81, 0.9189699842916913),\n",
    "    (0.32, 0.9771097738614918), (0.77, 0.9247174877976133), (0.87, 0.9106514118988247),\n",
    "    (0.64, 0.9428282446389429), (0.66, 0.9407111050831808), (0.41, 0.9680895444868959),\n",
    "    (0.55, 0.9543212368746771), (0.55, 0.954712358554443), (0.15, 0.9930764425762342),\n",
    "    (0.50, 0.9600037125608362), (0.54, 0.9561988507625073), (0.59, 0.9492948272587546),\n",
    "    (0.42, 0.967423804082141), (0.27, 0.9828888213427444), (0.56, 0.9525896043144539),\n",
    "    (0.50, 0.9596508815238165), (0.55, 0.9538807256334634), (0.30, 0.9795869184971074),\n",
    "    (0.92, 0.9053126225640622), (0.29, 0.9800897929890493), (0.80, 0.9198338103625441)\n",
    "]\n",
    "\n",
    "x_values = [item[0] for item in data]  # 噪声比例\n",
    "a_probs = [item[1] for item in data]   # Z1 概率\n",
    "\n",
    "# ==================== 可视化 ====================\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 绘制散点和趋势线\n",
    "plt.scatter(x_values, a_probs, color='blue', alpha=0.6, s=80, label='Observed probability of Z1')\n",
    "plt.plot(np.unique(x_values),\n",
    "         np.poly1d(np.polyfit(x_values, a_probs, 2))(np.unique(x_values)),\n",
    "         color='red', linestyle='--', linewidth=2, label='Quadratic trend line')\n",
    "\n",
    "# 标注关键点\n",
    "key_points = [\n",
    "    (0.01, 0.9985, \"Low noise (1%)\\nZ1=99.85%\"),\n",
    "    (0.32, 0.9771, \"Medium noise (32%)\\nZ1=97.71%\"),\n",
    "    (0.95, 0.9035, \"High noise (95%)\\nZ1=90.35%\")\n",
    "]\n",
    "\n",
    "for x, y, text in key_points:\n",
    "    plt.scatter(x, y, s=120, color='red', edgecolor='black', zorder=5)\n",
    "    plt.annotate(text, xy=(x, y),\n",
    "                 xytext=(10, -40 if x < 0.5 else 20),\n",
    "                 textcoords='offset points',\n",
    "                 ha='center', va='bottom',\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                 arrowprops=dict(arrowstyle='->'))\n",
    "\n",
    "# 改进的坐标轴标签\n",
    "plt.xlabel('Noise intensity (proportion of corrupted samples, x)')\n",
    "plt.ylabel('Predicted probability of class Z1')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.ylim(0.88, 1.005)\n",
    "plt.axhline(y=0.9, color='gray', linestyle=':', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"experiment2_2_new.pdf\", bbox_inches='tight', transparent=True)\n",
    "plt.show()\n"
   ],
   "id": "f2c3b0ba05f4f4f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 实验三图片",
   "id": "79307d5aa7dcf1ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# 实验数据\n",
    "interference_x = [-0.10, 0.10, 0.10, -0.10, -0.10, 0.10, -0.10, -0.10, -0.10, -0.10,\n",
    "                 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, -0.10, 0.10, 0.10, 0.10, -0.10,\n",
    "                 -0.10, -0.10, 0.10, 0.10, 0.10, 0.10, -0.10, 0.10, 0.10]\n",
    "\n",
    "probabilities = [{'A': 0.999995200591732, 'B': 1.7360671780575603e-06, 'C': 3.0629405830132987e-06}, {'A': 0.9995001161881935, 'B': 9.211374095243553e-06, 'C': 0.0004906717901036806}, {'A': 0.9998031118618389, 'B': 6.282889697768738e-06, 'C': 0.00019060469662092455}, {'A': 0.9999952815921279, 'B': 1.7296897937837973e-06, 'C': 2.9883179722279753e-06}, {'A': 0.994646889990718, 'B': 0.00010425161266173805, 'C': 0.005248856989726342}, {'A': 0.9965955374198802, 'B': 5.288273272547034e-05, 'C': 0.00335157867732948}, {'A': 0.9877523697223977, 'B': 0.0005115510240114114, 'C': 0.011736077252200001}, {'A': 0.999987553905901, 'B': 2.1933670107985145e-06, 'C': 1.0252301579144135e-05}, {'A': 0.9897464853565955, 'B': 0.0003230711693363251, 'C': 0.009930441603458566}, {'A': 0.9983025602538487, 'B': 2.0517537165546195e-05, 'C': 0.0016769213009968141}, {'A': 0.9942599384354853, 'B': 0.0001164010967237285, 'C': 0.005623659018686438}, {'A': 0.9992928776808968, 'B': 1.0600994164125828e-05, 'C': 0.0006965206237857711}, {'A': 0.999936441379683, 'B': 3.86336325690831e-06, 'C': 5.9694771813156085e-05}, {'A': 0.987869183250263, 'B': 0.0004920407611240911, 'C': 0.011638773991385652}, {'A': 0.9999257791441573, 'B': 4.125421760148719e-06, 'C': 7.009494149480262e-05}, {'A': 0.9999861689727945, 'B': 2.2660656897747494e-06, 'C': 1.156453292246466e-05}, {'A': 0.9940661747389057, 'B': 0.00012274897001386478, 'C': 0.005811074821294998}, {'A': 0.997453906604565, 'B': 3.5040403464732926e-05, 'C': 0.002511051944440823}, {'A': 0.9996350785472964, 'B': 8.1437348566846e-06, 'C': 0.00035677710875551617}, {'A': 0.993441926326688, 'B': 0.00014445186637760863, 'C': 0.006413620272368797}, {'A': 0.9998064274333818, 'B': 6.237314791604617e-06, 'C': 0.00018733470128471372}, {'A': 0.9999929562074714, 'B': 1.8938506898268913e-06, 'C': 5.149531928171621e-06}, {'A': 0.9994835283476878, 'B': 9.330911761805865e-06, 'C': 0.0005071400884243088}, {'A': 0.9964802071021615, 'B': 5.5493232610646896e-05, 'C': 0.0034642984797205287}, {'A': 0.9999853945970226, 'B': 2.3027190625772092e-06, 'C': 1.2302253666995019e-05}, {'A': 0.9999952005137792, 'B': 1.736073271760569e-06, 'C': 3.0630124420598363e-06}, {'A': 0.9999061445151617, 'B': 4.564908090100437e-06, 'C': 8.929007224044227e-05}, {'A': 0.9964259835963866, 'B': 5.6738653740332374e-05, 'C': 0.003517276557179807}, {'A': 0.9974346611573712, 'B': 3.540775595867522e-05, 'C': 0.002529930036184784}, {'A': 0.9904920153150092, 'B': 0.0002774994858765963, 'C': 0.009230483389944439}]\n",
    "\n",
    "# 选取第一个probabilities作为基准分布\n",
    "p_ref = np.array(list(probabilities[0].values()))\n",
    "\n",
    "# 计算KL散度\n",
    "kl_divergence = []\n",
    "for prob in probabilities:\n",
    "    p = np.array(list(prob.values()))\n",
    "    kl_divergence.append(entropy(p, p_ref))\n",
    "\n",
    "# 绘制折线图\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(x=range(len(kl_divergence)), y=kl_divergence, marker='o', linewidth=2.5)\n",
    "plt.xlabel(\"Experiment Index\")\n",
    "plt.ylabel(\"KL Divergence\")\n",
    "# plt.title(\"KL Divergence of Fused Probabilities\")\n",
    "plt.show()"
   ],
   "id": "44a0f3efc57cd1b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 实验三图片",
   "id": "aaf29dc17a837877"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import rel_entr  # 用于计算KL散度\n",
    "\n",
    "# ==================== 1. 定义证据和融合结果 ====================\n",
    "evidence = [\n",
    "    {('A',): 0.9, ('B',): 0.05, ('C',): 0.05},\n",
    "    {('A',): 0.01, ('B',): 0.01, ('C',): 0.98},  # 极端倾向于C\n",
    "    {('A',): 0.9, ('B',): 0.05, ('C',): 0.05}\n",
    "]\n",
    "# fused_result = {'A': 0.9999539729810457, 'C': 3.579169272690302e-05, 'B': 1.0234862701379976e-05}\n",
    "fused_result = {'A': 0.9915903830413398, 'B': 0.0031726089690510334, 'C': 0.005237007787936739}\n",
    "# ==================== 2. 计算每个证据源的权重 ====================\n",
    "def calculate_weights(evidence, fused_result):\n",
    "    # 将融合结果转换为与证据相同的格式（元组键）\n",
    "    r = {('A',): fused_result['A'], ('B',): fused_result['B'], ('C',): fused_result['C']}\n",
    "\n",
    "    # 计算KL散度（r || e_i）\n",
    "    kl_divergences = []\n",
    "    for e in evidence:\n",
    "        # 确保所有证据和结果的键顺序一致\n",
    "        p = np.array([r[key] for key in sorted(r.keys())])\n",
    "        q = np.array([e[key] for key in sorted(e.keys())])\n",
    "        kl = sum(rel_entr(p, q))  # 使用Scipy的KL散度计算\n",
    "        kl_divergences.append(kl)\n",
    "\n",
    "    # 权重为KL散度的倒数，并归一化\n",
    "    weights = 1 / np.array(kl_divergences)\n",
    "    normalized_weights = weights / weights.sum()\n",
    "    return normalized_weights\n",
    "\n",
    "weights = calculate_weights(evidence, fused_result)\n",
    "print(\"证据源权重:\", weights)  # 输出: [0.484 0.005 0.484]\n",
    "\n",
    "# ==================== 3. 可视化（堆叠条形图） ====================\n",
    "def plot_stacked_bar(weights):\n",
    "    labels = ['m1', 'm2', 'm3']\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # 绘制堆叠条形图\n",
    "    bottom = 0\n",
    "    for i, (weight, label, color) in enumerate(zip(weights, labels, colors)):\n",
    "        ax.bar(0, weight, bottom=bottom, label=label, color=color, edgecolor='black', width=0.6)\n",
    "        ax.text(0, bottom + weight/2, f'{weight*100:.1f}%',\n",
    "                ha='center', va='center', fontsize=10, color='white')\n",
    "        bottom += weight\n",
    "\n",
    "    # 美化图表\n",
    "    # ax.set_title('Contribution Weight of Evidence Sources', pad=20)\n",
    "    ax.set_xlim(-0.5, 0.5)\n",
    "    ax.axis('on')  # 隐藏坐标轴\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"experiment3_new.pdf\", bbox_inches='tight', transparent=True)\n",
    "    plt.show()\n",
    "\n",
    "plot_stacked_bar(weights)"
   ],
   "id": "276bbecbeabc7a73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 实验4图片",
   "id": "9448ff75e2b24089"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== 数据准备 ====================\n",
    "evidence = [\n",
    "    {('X1',): 1.0, ('X2',): 0.0, ('X3',): 0.0},  # A 100% 确定\n",
    "    {('X1',): 0.0, ('X2',): 0.0, ('X3',): 1.0},  # C 100% 确定\n",
    "    {('X1',): 0.5, ('X2',): 0.25, ('X3',): 0.25}  # 平衡分布\n",
    "]\n",
    "fused_result = {'X1': 0.886803226132325, 'X2': 0.007547005404806539, 'X3': 0.10564976742660648}\n",
    "\n",
    "# ==================== 可视化融合前后对比 ====================\n",
    "def plot_probability_comparison(evidence, fused_result):\n",
    "    categories = ['X1', 'X2', 'X3']\n",
    "\n",
    "    # 提取融合前的证据分布\n",
    "    evidence_arrays = []\n",
    "    for e in evidence:\n",
    "        evidence_arrays.append(np.array([e[('X1',)], e[('X2',)], e[('X3',)]]))\n",
    "\n",
    "    # 融合结果\n",
    "    fused_array = np.array([fused_result['X1'], fused_result['X2'], fused_result['X3']])\n",
    "\n",
    "    # 创建画布\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # 统一坐标轴标签\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.set_xlabel('target')  # 设置横坐标标签\n",
    "        ax.set_ylabel('BPA')      # 设置纵坐标标签\n",
    "\n",
    "    # -------------------- 融合前的分布（分证据源） --------------------\n",
    "    width = 0.25\n",
    "    x = np.arange(len(categories))\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "    for i, (e, color) in enumerate(zip(evidence_arrays, colors)):\n",
    "        bars = ax1.bar(x + i*width, e, width=width, label=f'm{i+1}', color=color, alpha=0.8)\n",
    "        # 标注每个柱状图的值\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:  # 只标注高度大于0的柱子\n",
    "                ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{height:.2f}',\n",
    "                        ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    ax1.set_title('Probability Distribution (Before Fusion)')\n",
    "    ax1.set_xticks(x + width)\n",
    "    ax1.set_xticklabels(categories)\n",
    "    ax1.set_ylim(0, 1.1)\n",
    "    ax1.legend()\n",
    "\n",
    "    # -------------------- 融合后的分布 --------------------\n",
    "    ax2.bar(x, fused_array, width=0.6, color='#d62728', alpha=0.8)\n",
    "    ax2.set_title('Probability Distribution (After Fusion)')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(categories)\n",
    "    ax2.set_ylim(0, 1.1)\n",
    "\n",
    "    # 标注具体数值\n",
    "    for i, val in enumerate(fused_array):\n",
    "        ax2.text(i, val + 0.02, f'{val:.4f}', ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"experiment4_new.pdf\", bbox_inches='tight', transparent=True)\n",
    "    plt.show()\n",
    "\n",
    "plot_probability_comparison(evidence, fused_result)\n",
    "\n",
    "# ==================== 冲突分析 ====================\n",
    "print(\"\\n冲突分析：\")\n",
    "print(\"1. 完全冲突证据：Evidence 1 (A=100%) 和 Evidence 2 (C=100%) 完全矛盾。\")\n",
    "print(\"2. 融合结果：A占98.68%，C仅1.32%，B几乎被忽略。\")\n",
    "print(\"3. 决策表现：算法倾向于支持多数一致证据（Evidence 1和3均支持A），但极端冲突证据（Evidence 2）仍被部分保留。\")\n",
    "print(\"4. 原因：SLF-CRE通过权重分配抑制了完全冲突证据的影响，但未完全消除。\")"
   ],
   "id": "a69f1a1c1b7e110"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==================== 数据准备 ====================\n",
    "evidence = [\n",
    "    {('Z1',): 1.0, ('Z2',): 0.0, ('Z3',): 0.0},  # 证据1 100% 确定 Z1\n",
    "    {('Z1',): 0.0, ('Z2',): 0.0, ('Z3',): 1.0},  # 证据2 100% 确定 Z3\n",
    "    {('Z1',): 0.5, ('Z2',): 0.25, ('Z3',): 0.25} # 证据3 平衡分布\n",
    "]\n",
    "\n",
    "fused_result = {'Z1': 0.886803226132325, 'Z2': 0.007547005404806539, 'Z3': 0.10564976742660648}\n",
    "\n",
    "# ==================== 可视化融合前后对比 ====================\n",
    "def plot_probability_comparison(evidence, fused_result):\n",
    "    categories = ['Z1', 'Z2', 'Z3']\n",
    "\n",
    "    # 融合前提取数据\n",
    "    evidence_arrays = [np.array([e[('Z1',)], e[('Z2',)], e[('Z3',)]]) for e in evidence]\n",
    "\n",
    "    # 融合结果数据\n",
    "    fused_array = np.array([fused_result['Z1'], fused_result['Z2'], fused_result['Z3']])\n",
    "\n",
    "    # 创建画布和两个子图\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # 统一坐标轴标签\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.set_xlabel('Target Class')\n",
    "        ax.set_ylabel('BBD')\n",
    "\n",
    "    # -------- 融合前 --------\n",
    "    width = 0.25\n",
    "    x = np.arange(len(categories))\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "    for i, (e, color) in enumerate(zip(evidence_arrays, colors)):\n",
    "        bars = ax1.bar(x + i * width, e, width=width, label=f'm{i+1}', color=color, alpha=0.8)\n",
    "        # 标注柱状图数值，带白色背景，避免遮挡\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax1.text(\n",
    "                    bar.get_x() + bar.get_width() / 2, height,\n",
    "                    f'{height:.2f}',\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=14, fontweight='bold', color='black',\n",
    "                    bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1)\n",
    "                )\n",
    "\n",
    "    ax1.set_title('Probability Distribution (Before Fusion)')\n",
    "    ax1.set_xticks(x + width)\n",
    "    ax1.set_xticklabels(categories, fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylim(0, 1.1)\n",
    "    ax1.legend()\n",
    "\n",
    "    # -------- 融合后 --------\n",
    "    bars2 = ax2.bar(x, fused_array, width=0.6, color='#d62728', alpha=0.8)\n",
    "    ax2.set_title('Probability Distribution (After Fusion)')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(categories, fontsize=11, fontweight='bold')\n",
    "    ax2.set_ylim(0, 1.1)\n",
    "\n",
    "    # 标注融合后数值，带白色背景\n",
    "    for i, val in enumerate(fused_array):\n",
    "        ax2.text(\n",
    "            i, val + 0.02, f'{val:.4f}',\n",
    "            ha='center', va='bottom',\n",
    "            fontsize=14, fontweight='bold', color='black',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1)\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"experiment4_BBD.pdf\", bbox_inches='tight', transparent=True)\n",
    "    plt.show()\n",
    "\n",
    "plot_probability_comparison(evidence, fused_result)"
   ],
   "id": "6d17bd3bca4aa3f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data from the LaTeX table\n",
    "methods = [\"Xiao's method [1]\", \"Jiang et al. [2]\", \"Xiao's method [3]\", \"Proposed method\"]\n",
    "data = {\n",
    "    'm(H₁)': [0.0053, 0.00049, 0.0000688, 0.0000818],\n",
    "    'm(H₂)': [0.7390, 0.8798, 0.9163, 0.9241],\n",
    "    'm(H₃)': [0.2407, 0.1130, 0.0790, 0.0757],\n",
    "    'm(H₁,H₂)': [0.000354, 0.000033, 0.00000459, np.nan],\n",
    "    'm(H₁,H₃)': [0.000257, 0.0000022, 0.0000026, np.nan],\n",
    "    'm(H₂,H₃)': [0.0135, 0.0066, 0.0046, np.nan],\n",
    "    'm(H₁,H₂,H₃)': [0.0000171, 0.0000015, 0.000000173, np.nan]\n",
    "}\n",
    "\n",
    "# Prepare the data for plotting\n",
    "categories = ['(H1)', '(H2)', '(H3)', '(H1,H2)', '(H1,H3)', '(H2,H3)', '(H1,H2,H3)']\n",
    "x = np.arange(len(categories))  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot each method's data\n",
    "for i, method in enumerate(methods):\n",
    "    offset = width * i\n",
    "    values = [\n",
    "        data['m(H₁)'][i],\n",
    "        data['m(H₂)'][i],\n",
    "        data['m(H₃)'][i],\n",
    "        data['m(H₁,H₂)'][i],\n",
    "        data['m(H₁,H₃)'][i],\n",
    "        data['m(H₂,H₃)'][i],\n",
    "        data['m(H₁,H₂,H₃)'][i]\n",
    "    ]\n",
    "    # Replace NaN with 0 for plotting\n",
    "    values = [0 if np.isnan(v) else v for v in values]\n",
    "    rects = ax.bar(x + offset, values, width, label=method)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('BPA', fontsize=12)\n",
    "ax.set_title('Comparative Performance on Iris Dataset', fontsize=14)\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(categories, fontsize=11)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Set y-axis to match reference image style\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_yticks([0.00, 0.20, 0.40, 0.60, 0.80, 1.00])\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a3cf2249e0267b84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "plt.subplots_adjust(wspace=0.3)  # 增加子图间距\n",
    "\n",
    "# ----------------------------\n",
    "# 通用设置\n",
    "# ----------------------------\n",
    "methods = [\"Xiao's method [35]\", \"Jiang et al. [19]\", \"Xiao's method [36]\", \"Proposed method\"]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # 统一颜色方案\n",
    "legend_params = {\n",
    "    'fontsize': 12,\n",
    "    'framealpha': 1,\n",
    "    'bbox_to_anchor': (0.5, -0.15),\n",
    "    'loc': 'upper center',\n",
    "    'ncol': 2\n",
    "}  # 统一图例参数\n",
    "\n",
    "# ----------------------------\n",
    "# 第一个子图 (comparative performance)\n",
    "# ----------------------------\n",
    "data = {\n",
    "    'm(H₁)': [0.0053, 0.00049, 0.0000688, 0.0000818],\n",
    "    'm(H₂)': [0.7390, 0.8798, 0.9163, 0.9241],\n",
    "    'm(H₃)': [0.2407, 0.1130, 0.0790, 0.0757],\n",
    "    'm(H₁,H₂)': [0.000354, 0.000033, 0.00000459, np.nan],\n",
    "    'm(H₁,H₃)': [0.000257, 0.0000022, 0.0000026, np.nan],\n",
    "    'm(H₂,H₃)': [0.0135, 0.0066, 0.0046, np.nan],\n",
    "    'm(H₁,H₂,H₃)': [0.0000171, 0.0000015, 0.000000173, np.nan]\n",
    "}\n",
    "\n",
    "categories = ['(K1)', '(K2)', '(K3)', '(K1,K2)', '(K1,K3)', '(K2,K3)', '(K1,K2,K3)']\n",
    "x = np.arange(len(categories))\n",
    "width = 0.18  # 稍微减小柱宽\n",
    "\n",
    "for i, (method, color) in enumerate(zip(methods, colors)):\n",
    "    offset = width * i\n",
    "    values = [\n",
    "        data['m(H₁)'][i],\n",
    "        data['m(H₂)'][i],\n",
    "        data['m(H₃)'][i],\n",
    "        data['m(H₁,H₂)'][i],\n",
    "        data['m(H₁,H₃)'][i],\n",
    "        data['m(H₂,H₃)'][i],\n",
    "        data['m(H₁,H₂,H₃)'][i]\n",
    "    ]\n",
    "    values = [0 if np.isnan(v) else v for v in values]\n",
    "    ax1.bar(x + offset, values, width, color=color, label=method)\n",
    "\n",
    "ax1.set_ylabel('BBD', fontsize=12)\n",
    "ax1.set_xticks(x + width * 1.5)\n",
    "ax1.set_xticklabels(categories, fontsize=10, fontweight='bold')\n",
    "ax1.set_ylim(0, 1.0)\n",
    "ax1.set_yticks([0.00, 0.20, 0.40, 0.60, 0.80, 1.00])\n",
    "ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "ax1.legend(**legend_params)\n",
    "\n",
    "# ----------------------------\n",
    "# 第二个子图 (H₂ comparison)\n",
    "# ----------------------------\n",
    "bba_values = [0.7390, 0.8798, 0.9163, 0.9241]  # m(H₂) values\n",
    "\n",
    "for method, color, value in zip(methods, colors, bba_values):\n",
    "    ax2.bar(method, value, color=color, width=0.6, label=method)  # 注意加 label\n",
    "\n",
    "# 添加数值标签，字体加粗，黑色，白底半透明背景，稍微向上偏移\n",
    "for i, (method, value) in enumerate(zip(methods, bba_values)):\n",
    "    ax2.text(\n",
    "        i, value + 0.01, f'{value:.4f}',\n",
    "        ha='center', va='bottom',\n",
    "        fontsize=14, fontweight='bold', color='black',\n",
    "        bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1)\n",
    "    )\n",
    "\n",
    "ax2.set_ylabel('BBD', fontsize=12)\n",
    "ax2.set_xlabel('K2', fontsize=12)\n",
    "ax2.set_ylim(0.70, 1.00)\n",
    "ax2.set_yticks([0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00])\n",
    "ax2.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "ax2.set_xticklabels([])  # 隐藏x轴标签\n",
    "ax2.legend(**legend_params)\n",
    "\n",
    "# 调整整体布局，底部留图例空间\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
    "\n",
    "plt.savefig(\"comparison_figure1_new.pdf\", bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ],
   "id": "32b298b5638bd00b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 创建包含两个子图的画布\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plt.subplots_adjust(wspace=0.25)  # 调整子图间距\n",
    "\n",
    "# ----------------------------\n",
    "# 通用设置\n",
    "# ----------------------------\n",
    "methods = [\"Dempster[10]\", \"Xiao[35]\", \"Wang et al.[39]\", \"Jiang et al.[40]\", \"Gao et al.[37]\", \"Proposed method\"]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "legend_params = {\n",
    "    'fontsize': 12,\n",
    "    'framealpha': 1,\n",
    "    'bbox_to_anchor': (0.5, -0.2),  # 统一放在横坐标下方\n",
    "    'loc': 'upper center',\n",
    "    'ncol': 3  # 分3列显示更紧凑\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 第一个子图：综合比较\n",
    "# ----------------------------\n",
    "categories = ['A', 'B', 'C', '{A,B,C}']\n",
    "data = {\n",
    "    'A': [0, 0.9946, 0.9933, 0.9914, 0.9957, 0.9980],\n",
    "    'B': [0.3443, 0.0030, 0.0033, 0.0035, 0.0026, 0.0015],\n",
    "    'C': [0.6557, 0.0015, 0.0025, 0.0042, 0.0008, 0.0003],\n",
    "    '(A,B,C)': [0, 0.0009, 0.0009, 0.0009, 0.0009, 0.0000]\n",
    "}\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.12  # 调整宽度以适应6种方法\n",
    "\n",
    "# 绘制每种方法的数据\n",
    "for i, (method, color) in enumerate(zip(methods, colors)):\n",
    "    offset = width * i\n",
    "    values = [data['A'][i], data['B'][i], data['C'][i], data['(A,B,C)'][i]]\n",
    "    ax1.bar(x + offset, values, width, color=color, label=method)\n",
    "\n",
    "ax1.set_ylabel('BPA', fontsize=12)\n",
    "ax1.set_xticks(x + width * 2.5)\n",
    "ax1.set_xticklabels(categories, fontsize=11, fontweight='bold')\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.set_yticks([0.00, 0.20, 0.40, 0.60, 0.80, 1.00])\n",
    "ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "ax1.legend(**legend_params)  # 应用统一图例样式\n",
    "\n",
    "# ----------------------------\n",
    "# 第二个子图：A值比较\n",
    "# ----------------------------\n",
    "bba_values = [0, 0.9946, 0.9933, 0.9914, 0.9957, 0.9980]  # A值数据\n",
    "\n",
    "# 绘制柱状图（保持与第一个子图相同的颜色顺序）\n",
    "for method, color, value in zip(methods, colors, bba_values):\n",
    "    ax2.bar(method, value, color=color, width=0.5, label=method)\n",
    "\n",
    "# 添加数值标签\n",
    "for i, (method, value) in enumerate(zip(methods, bba_values)):\n",
    "    ax2.text(i, value + 0.005, f'{value:.4f}',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 坐标轴和标签设置\n",
    "ax2.set_ylabel('BPA', fontsize=12)\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.set_xticklabels([])\n",
    "ax2.set_xlabel('A', fontsize=12)\n",
    "ax2.set_yticks(np.arange(0, 1.1, 0.2))\n",
    "ax2.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "plt.sca(ax2)\n",
    "plt.xticks(rotation=15, ha='right', fontsize=11)\n",
    "ax2.legend(**legend_params)  # 应用统一图例样式\n",
    "\n",
    "# 调整整体布局（为底部图例留出空间）\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
    "\n",
    "plt.savefig(\"application2_new.pdf\", bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ],
   "id": "4b56d75826793b7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 创建包含两个子图的画布\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plt.subplots_adjust(wspace=0.25)  # 调整子图间距\n",
    "\n",
    "# ----------------------------\n",
    "# 通用设置\n",
    "# ----------------------------\n",
    "methods = [\"Dempster[5]\", \"Xiao[35]\", \"Wang et al.[34]\", \"Jiang et al.[20]\", \"Gao et al.[15]\", \"Proposed method\"]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "legend_params = {\n",
    "    'fontsize': 12,\n",
    "    'framealpha': 1,\n",
    "    'bbox_to_anchor': (0.5, -0.2),  # 统一放在横坐标下方\n",
    "    'loc': 'upper center',\n",
    "    'ncol': 3  # 分3列显示更紧凑\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 第一个子图：综合比较\n",
    "# ----------------------------\n",
    "categories = ['H1', 'H2', 'H3', '{H1,H2,H3}']\n",
    "data = {\n",
    "    'H1': [0, 0.9946, 0.9933, 0.9914, 0.9957, 0.9980],\n",
    "    'H2': [0.3443, 0.0030, 0.0033, 0.0035, 0.0026, 0.0015],\n",
    "    'H3': [0.6557, 0.0015, 0.0025, 0.0042, 0.0008, 0.0003],\n",
    "    '(H1,H2,H3)': [0, 0.0009, 0.0009, 0.0009, 0.0009, 0.0000]\n",
    "}\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.12  # 调整宽度以适应6种方法\n",
    "\n",
    "# 绘制每种方法的数据\n",
    "for i, (method, color) in enumerate(zip(methods, colors)):\n",
    "    offset = width * i\n",
    "    values = [data['H1'][i], data['H2'][i], data['H3'][i], data['(H1,H2,H3)'][i]]\n",
    "    ax1.bar(x + offset, values, width, color=color, label=method)\n",
    "\n",
    "ax1.set_ylabel('BBD', fontsize=12)\n",
    "ax1.set_xticks(x + width * 2.5)\n",
    "ax1.set_xticklabels(categories, fontsize=11, fontweight='bold')\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.set_yticks([0.00, 0.20, 0.40, 0.60, 0.80, 1.00])\n",
    "ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "ax1.legend(**legend_params)  # 应用统一图例样式\n",
    "\n",
    "# ----------------------------\n",
    "# 第二个子图：H1值比较\n",
    "# ----------------------------\n",
    "bba_values = [0, 0.9946, 0.9933, 0.9914, 0.9957, 0.9980]  # H1值数据\n",
    "\n",
    "for method, color, value in zip(methods, colors, bba_values):\n",
    "    ax2.bar(method, value, color=color, width=0.5, label=method)  # 注意加 label\n",
    "\n",
    "# 添加数值标签（同之前建议）\n",
    "for i, (method, value) in enumerate(zip(methods, bba_values)):\n",
    "    ax2.text(\n",
    "        i, value + 0.015, f'{value:.4f}',\n",
    "        ha='center', va='bottom',\n",
    "        fontsize=14, fontweight='bold',\n",
    "        color='black',\n",
    "        bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1)\n",
    "    )\n",
    "\n",
    "# 设置坐标轴等\n",
    "ax2.set_ylabel('BBD', fontsize=12)\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.set_xticklabels([])  # 如果隐藏x轴标签，可以保留，否则注释\n",
    "ax2.set_xlabel('H1', fontsize=12)\n",
    "ax2.set_yticks(np.arange(0, 1.1, 0.2))\n",
    "ax2.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "# 调用图例\n",
    "ax2.legend(**legend_params)\n",
    "\n",
    "# 调整整体布局（为底部图例留出空间）\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
    "\n",
    "plt.savefig(\"application2_BBD.pdf\", bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ],
   "id": "f3dfb7fe5a946674"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 11.1 目标B和C的对比",
   "id": "9c8e86711940c4c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 生成数据缺失场景下的实验数据\n",
    "def generate_missing_data(x_values, missing_rate=0.2):\n",
    "    evidence_list = []\n",
    "    for x in x_values:\n",
    "        evidence = [\n",
    "            {('A',): 0.9, ('B',): 0.05, ('C',): 0.05},  # 传感器1\n",
    "            {('A',): 0.95 - x, ('B',): 0.05, ('C',): x},  # 传感器2（受干扰）\n",
    "            {('A',): 0.9, ('B',): 0.05, ('C',): 0.05}  # 传感器3\n",
    "        ]\n",
    "\n",
    "        # 随机删除部分数据\n",
    "        # for i in range(len(evidence)):\n",
    "        #     if np.random.rand() < missing_rate:\n",
    "        #         evidence[i] = {}  # 删除该传感器的数据\n",
    "\n",
    "        evidence_list.append(evidence)\n",
    "    return evidence_list\n",
    "\n",
    "# 设置干扰强度范围\n",
    "x_values = np.linspace(0, 0.95, 100)  # 从0到0.95，生成11个干扰强度\n",
    "evidence_list = generate_missing_data(x_values, missing_rate=0.2)\n",
    "results = []\n",
    "\n",
    "# 打印生成的实验数据\n",
    "for i, evidence in enumerate(evidence_list):\n",
    "    # print(f\"Interference x = {x_values[i]:.2f}: Evidence = {evidence}\")\n",
    "    normalized_evidence = normalize_evidence(evidence)\n",
    "    S = compute_similarity_matrix(normalized_evidence)\n",
    "    R = new_compute_weight_from_similarity_gaussian(S)\n",
    "    fused_probabilities = fuse_evidence(normalized_evidence, R, alpha=0.1)\n",
    "\n",
    "    results.append((x_values[i], fused_probabilities))\n",
    "\n",
    "# 初始化三个数组来存储 A、B、C 的概率\n",
    "prob_A = []\n",
    "prob_B = []\n",
    "prob_C = []\n",
    "\n",
    "# 遍历结果，提取 A、B、C 的概率\n",
    "for x, fused in results:\n",
    "    prob_A.append(fused.get('A', 0))  # 如果 'A' 不存在，默认值为 0\n",
    "    prob_B.append(fused.get('B', 0))  # 如果 'B' 不存在，默认值为 0\n",
    "    prob_C.append(fused.get('C', 0))  # 如果 'C' 不存在，默认值为 0\n",
    "\n",
    "# 计算均值和方差\n",
    "mean_A = np.mean(prob_A)\n",
    "variance_A = np.var(prob_A)\n",
    "\n",
    "mean_B = np.mean(prob_B)\n",
    "variance_B = np.var(prob_B)\n",
    "\n",
    "mean_C = np.mean(prob_C)\n",
    "variance_C = np.var(prob_C)\n",
    "\n",
    "\n",
    "print(f\"Target H1 (A): Mean = {mean_A:.6f}, Variance = {variance_A:.6f}\")\n",
    "print(f\"Target H2 (B): Mean = {mean_B:.6f}, Variance = {variance_B:.6f}\")\n",
    "print(f\"Target H3 (C): Mean = {mean_C:.6f}, Variance = {variance_C:.6f}\")\n",
    "\n",
    "# 绘制目标 H2 和 H3 的识别概率分布\n",
    "plt.figure(figsize=(10, 6))\n",
    "# plt.hist(prob_A, bins=20, alpha=0.7, label='Target H2 (A)')\n",
    "plt.hist(prob_B, bins=20, alpha=0.7, label='Target x1')\n",
    "plt.hist(prob_C, bins=20, alpha=0.7, label='Target x2')\n",
    "plt.title('Recognition Probability Distribution of Targets x1 and x2', fontsize=16)\n",
    "plt.xlabel('Probability', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ],
   "id": "ee0fef193e6fe81e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10.2 目标A的显示",
   "id": "22b61533c3c971cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 生成数据缺失场景下的实验数据\n",
    "def generate_missing_data(x_values, missing_rate=0.2):\n",
    "    evidence_list = []\n",
    "    for x in x_values:\n",
    "        evidence = [\n",
    "            {('A',): 0.9, ('B',): 0.05, ('C',): 0.05},  # 传感器1\n",
    "            {('A',): 0.95 - x, ('B',): 0.05, ('C',): x},  # 传感器2（受干扰）\n",
    "            {('A',): 0.9, ('B',): 0.05, ('C',): 0.05}  # 传感器3\n",
    "        ]\n",
    "\n",
    "        # 随机删除部分数据\n",
    "        # for i in range(len(evidence)):\n",
    "        #     if np.random.rand() < missing_rate:\n",
    "        #         evidence[i] = {}  # 删除该传感器的数据\n",
    "\n",
    "        evidence_list.append(evidence)\n",
    "    return evidence_list\n",
    "\n",
    "# 设置干扰强度范围\n",
    "x_values = np.linspace(0, 0.95, 100)  # 从0到0.95，生成11个干扰强度\n",
    "evidence_list = generate_missing_data(x_values, missing_rate=0.2)\n",
    "results = []\n",
    "\n",
    "# 打印生成的实验数据\n",
    "for i, evidence in enumerate(evidence_list):\n",
    "    # print(f\"Interference x = {x_values[i]:.2f}: Evidence = {evidence}\")\n",
    "    normalized_evidence = normalize_evidence(evidence)\n",
    "    S = compute_similarity_matrix(normalized_evidence)\n",
    "    R = new_compute_weight_from_similarity_gaussian(S)\n",
    "    fused_probabilities = fuse_evidence(normalized_evidence, R, alpha=0.1)\n",
    "\n",
    "    results.append((x_values[i], fused_probabilities))\n",
    "\n",
    "# 初始化三个数组来存储 A、B、C 的概率\n",
    "prob_A = []\n",
    "prob_B = []\n",
    "prob_C = []\n",
    "\n",
    "# 遍历结果，提取 A、B、C 的概率\n",
    "for x, fused in results:\n",
    "    print(f\"Interference x = {x:.2f}: Fused probabilities = {fused}\")\n",
    "    prob_A.append(fused.get('A', 0))  # 如果 'A' 不存在，默认值为 0\n",
    "    prob_B.append(fused.get('B', 0))  # 如果 'B' 不存在，默认值为 0\n",
    "    prob_C.append(fused.get('C', 0))  # 如果 'C' 不存在，默认值为 0\n",
    "\n",
    "# 计算均值和方差\n",
    "mean_A = np.mean(prob_A)\n",
    "variance_A = np.var(prob_A)\n",
    "\n",
    "mean_B = np.mean(prob_B)\n",
    "variance_B = np.var(prob_B)\n",
    "\n",
    "mean_C = np.mean(prob_C)\n",
    "variance_C = np.var(prob_C)\n",
    "\n",
    "\n",
    "print(f\"Target H1 (A): Mean = {mean_A:.6f}, Variance = {variance_A:.6f}\")\n",
    "print(f\"Target H2 (B): Mean = {mean_B:.6f}, Variance = {variance_B:.6f}\")\n",
    "print(f\"Target H3 (C): Mean = {mean_C:.6f}, Variance = {variance_C:.6f}\")\n",
    "\n",
    "# 绘制目标 A 的识别概率分布\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_values, prob_A, label='Target x1', color='blue', linewidth=2)\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.title('Recognition Probability Distribution of Target x1', fontsize=16)\n",
    "plt.xlabel('Interference Strength (x)', fontsize=14)\n",
    "plt.ylabel('Probability', fontsize=14)\n",
    "\n",
    "# 设置 X 轴范围和刻度\n",
    "plt.xlim(0, 0.95)  # X 轴范围从 0 到 0.95\n",
    "plt.xticks(np.arange(0, 1.0, 0.05))  # X 轴刻度间隔为 0.1\n",
    "\n",
    "# 设置 Y 轴范围和刻度\n",
    "plt.ylim(0.99, 1.001)  # Y 轴范围从 0.99 到 1.001\n",
    "plt.yticks(np.arange(0.99, 1.001, 0.002))  # Y 轴刻度间隔为 0.002\n",
    "\n",
    "# 添加图例\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# 设置网格线\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ],
   "id": "dcbca1de53fcfe84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 12 isir数据转化为BPA",
   "id": "616b4b77d8bad894"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain, combinations\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. 载入鸢尾花数据集\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_mapping = {'Setosa': 'A', 'Versicolor': 'B', 'Virginica': 'C'}\n",
    "target_names = ['A', 'B', 'C']  # 重新定义类别名称\n",
    "\n",
    "# 2. 计算类别中心\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "class_centers = {i: X_train[y_train == i].mean(axis=0) for i in np.unique(y_train)}\n",
    "\n",
    "# 3. 生成完整的幂集（包括组合类别）\n",
    "def get_full_powerset(categories):\n",
    "    \"\"\"生成完整幂集（不包括空集）\"\"\"\n",
    "    return [set(p) for p in chain.from_iterable(combinations(categories, r) for r in range(1, len(categories) + 1))]\n",
    "\n",
    "powerset = get_full_powerset(target_names)  # 计算所有组合命题\n",
    "\n",
    "# 4. 计算 BPA 并分配到完整幂集\n",
    "def compute_bpa(sample, centers, alpha=0.1, beta=0.2):\n",
    "    \"\"\"\n",
    "    计算样本的基本概率指派（BPA）\n",
    "    alpha: 分配给全集的BPA\n",
    "    beta: 组合命题的最大BPA比例\n",
    "    \"\"\"\n",
    "    distances = {i: np.linalg.norm(sample - center) for i, center in centers.items()}\n",
    "\n",
    "    # 计算反距离\n",
    "    inv_distances = {i: 1.0 / d if d > 0 else 1e6 for i, d in distances.items()}  # 避免除零\n",
    "    total_inv_dist = sum(inv_distances.values())\n",
    "\n",
    "    # 计算单类别 BPA\n",
    "    bpa = {frozenset([target_names[i]]): inv_distances[i] / total_inv_dist * (1 - alpha - beta) for i in inv_distances}\n",
    "\n",
    "    # 计算组合类别 BPA\n",
    "    sorted_dist = sorted(distances.items(), key=lambda x: x[1])  # 按距离排序\n",
    "    primary, secondary, tertiary = sorted_dist[0][0], sorted_dist[1][0], sorted_dist[2][0]\n",
    "\n",
    "    # 计算组合命题 BPA\n",
    "    bpa[frozenset([target_names[primary], target_names[secondary]])] = beta * 0.6\n",
    "    bpa[frozenset([target_names[primary], target_names[tertiary]])] = beta * 0.3\n",
    "    bpa[frozenset([target_names[secondary], target_names[tertiary]])] = beta * 0.1\n",
    "\n",
    "    # 计算全集 BPA\n",
    "    bpa[frozenset(target_names)] = alpha\n",
    "\n",
    "    return bpa\n",
    "\n",
    "# 5. 计算所有测试样本的 BPA\n",
    "bpa_list = [compute_bpa(sample, class_centers) for sample in X_test]\n",
    "\n",
    "# 6. 转换为目标格式\n",
    "evidence = []\n",
    "for bpa in bpa_list:\n",
    "    # 将 frozenset 转换为元组，并保留键值对\n",
    "    formatted_bpa = {tuple(k): v for k, v in bpa.items()}\n",
    "    evidence.append(formatted_bpa)\n",
    "\n",
    "# 打印前 2 个证据体\n",
    "# print(evidence[:2])\n",
    "normalized_evidence = normalize_evidence(evidence)\n",
    "S = compute_similarity_matrix_jousselme(normalized_evidence)\n",
    "R = new_compute_weight_from_similarity_nonlinear(S)\n",
    "fused_probabilities = fuse_evidence(normalized_evidence, R, alpha=0.1)\n",
    "\n",
    "print(\"Optimal R:\", R)\n",
    "print(\"Fused probabilities (after SLF):\", fused_probabilities)"
   ],
   "id": "b24c01bda2ef791c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 加载Iris数据集\n",
    "iris = load_iris()\n",
    "X = iris.data  # 特征矩阵\n",
    "y = iris.target  # 类别标签\n",
    "\n",
    "# 将数据集分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 定义类别\n",
    "classes = {'Setosa': 0, 'Versicolor': 1, 'Virginica': 2}\n",
    "\n",
    "# 训练随机森林模型，每个特征单独训练一个模型\n",
    "def train_models(X_train, y_train):\n",
    "    models = {}\n",
    "    for i in range(X_train.shape[1]):  # 遍历每个特征\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(X_train[:, i].reshape(-1, 1), y_train)  # 仅使用当前特征训练模型\n",
    "        models[i] = model\n",
    "    return models\n",
    "\n",
    "# 训练模型\n",
    "models = train_models(X_train, y_train)\n",
    "\n",
    "# 计算每个特征的BBA（证据体）\n",
    "def calculate_bba(models, x):\n",
    "    bba = {}\n",
    "    for i, model in models.items():  # 遍历每个特征对应的模型\n",
    "        probs = model.predict_proba(x[i].reshape(1, -1))[0]  # 预测概率分布\n",
    "        bba[i] = {cls: prob for cls, prob in zip(classes.keys(), probs)}  # 转换为BBA格式\n",
    "    return bba\n",
    "\n",
    "# 示例：计算测试集中第一个样本的BBA\n",
    "sample_index = 0\n",
    "x_sample = X_test[sample_index]\n",
    "bba_sample = calculate_bba(models, x_sample)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"Test sample values: {x_sample}\")\n",
    "print(f\"BBA for sample {sample_index}:\")\n",
    "for feature_idx, bba in bba_sample.items():\n",
    "    print(f\"Feature {feature_idx} (Sepal Length/Sepal Width/Petal Length/Petal Width): {bba}\")\n",
    "\n",
    "# 检查模型训练效果\n",
    "for i, model in models.items():\n",
    "    print(f\"Feature {i} accuracy: {model.score(X_train[:, i].reshape(-1, 1), y_train)}\")"
   ],
   "id": "fe1f724e081d3352"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "def adjust_evidence_bodies(soft_likelihoods):\n",
    "    \"\"\"\n",
    "    调整平均证据体 (Step 3-1)\n",
    "    - 输入: soft_likelihoods (字典), 键为命题，值为软似然函数值。\n",
    "    - 输出: 调整后的证据体 (字典)。\n",
    "    \"\"\"\n",
    "    total = sum(soft_likelihoods.values())\n",
    "    adjusted_evidence = {proposition: likelihood / total for proposition, likelihood in soft_likelihoods.items()}\n",
    "    return adjusted_evidence\n",
    "\n",
    "def dempster_combine(m1, m2):\n",
    "    \"\"\"\n",
    "    Dempster 组合规则 (DCR)\n",
    "    - 输入: m1 和 m2 是两个证据体的字典表示。\n",
    "    - 输出: 融合后的证据体 (字典)。\n",
    "    \"\"\"\n",
    "    hypotheses = set(m1.keys()).union(set(m2.keys()))\n",
    "    m_combined = {h: 0.0 for h in hypotheses}\n",
    "\n",
    "    # 计算冲突因子 K\n",
    "    K = 0.0\n",
    "    for h1, h2 in [(h1, h2) for h1 in m1 for h2 in m2]:\n",
    "        if set(h1).isdisjoint(set(h2)):  # 如果 h1 和 h2 不相交\n",
    "            K += m1[h1] * m2[h2]\n",
    "\n",
    "    # 计算组合后的 BPA\n",
    "    for h1, h2 in [(h1, h2) for h1 in m1 for h2 in m2]:\n",
    "        intersection = set(h1).intersection(set(h2))\n",
    "        if intersection:  # 如果 h1 和 h2 有交集\n",
    "            key = tuple(sorted(intersection))\n",
    "            m_combined[key] += m1[h1] * m2[h2] / (1 - K)\n",
    "\n",
    "    return m_combined\n",
    "\n",
    "def fuse_evidence_multiple_times(adjusted_evidence, n):\n",
    "    \"\"\"\n",
    "    多次融合证据 (Step 3-2)\n",
    "    - 输入: adjusted_evidence (字典), 调整后的证据体。\n",
    "    - 输入: n (整数), 融合次数。\n",
    "    - 输出: 最终融合结果 (字典)。\n",
    "    \"\"\"\n",
    "    result = adjusted_evidence\n",
    "    for _ in range(n - 1):\n",
    "        result = dempster_combine(result, adjusted_evidence)\n",
    "    return result\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义软似然函数值\n",
    "    soft_likelihoods = {\n",
    "        ('A',): 0.2,\n",
    "        ('B',): 0.5,\n",
    "        ('C',): 0.3\n",
    "    }\n",
    "\n",
    "    # Step 3-1: 调整平均证据体\n",
    "    adjusted_evidence = adjust_evidence_bodies(soft_likelihoods)\n",
    "    print(\"Adjusted Evidence Bodies:\", adjusted_evidence)\n",
    "\n",
    "    # Step 3-2: 多次融合证据\n",
    "    n = 3  # 融合次数\n",
    "    fused_result = fuse_evidence_multiple_times(adjusted_evidence, n)\n",
    "    print(\"Fused Result (after\", n, \"fusions):\", fused_result)"
   ],
   "id": "d030bc9f1b7ca817"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 已有论文中的实验总结",
   "id": "e84f19bf347dfac3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 肖老师的论文里：实验部分引用了一个别的论文数据，然后解析实验步骤结果，最后有个对比；在application部分引用了别的论文的数据集，然后有对比",
   "id": "cf1a7f2ffed52703"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 康老师的实验部分设计了一个实验，来说明抗干扰性，说明算法同样可以；但是多了一个案例部分，在这个部分也是自己设计的数据，但是也是有对比的；在application部分引用了别的论文里的案例，他想说明他的融合在第三次就很能识别了。",
   "id": "9fb8d242397a7ffd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "categories = ['A', 'B', 'C']\n",
    "evidence_1 = [1.0, 0.0, 0.0]  # 完全支持 A\n",
    "evidence_2 = [0.0, 0.0, 1.0]  # 完全支持 C\n",
    "evidence_3 = [0.5, 0.5, 0.0]  # 对 A 和 B 各占一半\n",
    "fusion_result = [0.7, 0.2, 0.1]  # 融合后的结果（假设）\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.2  # 柱状图宽度\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x - 1.5*width, evidence_1, width, label='Evidence 1')\n",
    "ax.bar(x - 0.5*width, evidence_2, width, label='Evidence 2')\n",
    "ax.bar(x + 0.5*width, evidence_3, width, label='Evidence 3')\n",
    "ax.bar(x + 1.5*width, fusion_result, width, label='Fusion Result', color='black')\n",
    "\n",
    "ax.set_xlabel('Categories')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title('Comparison of Evidence and Fusion Result')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "id": "91775b22a46dd87d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def product_weight_fusion(w_sim, w_conf):\n",
    "    w_product = w_sim * w_conf\n",
    "    return w_product / np.sum(w_product)\n",
    "\n",
    "# 示例使用\n",
    "w_combined = product_weight_fusion(w_sim, w_conf)\n",
    "print(\"综合权重:\", w_combined)"
   ],
   "id": "d6a8021a75b7ebf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
