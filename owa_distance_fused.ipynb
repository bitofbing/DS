{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import math\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from string import ascii_uppercase\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "import itertools\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 300,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"savefig.format\": \"pdf\",\n",
    "    \"lines.linewidth\": 2,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"axes.titlesize\": 18,\n",
    "    'axes.labelweight': 'bold',\n",
    "    \"xtick.labelsize\": 14,\n",
    "    \"ytick.labelsize\": 14,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"grid.alpha\": 0.5,\n",
    "    \"font.family\": \"DejaVu Sans\",  # 更稳定跨平台字体\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "    \"text.usetex\": False,\n",
    "    \"text.antialiased\": True\n",
    "})"
   ],
   "id": "cb16e7dd94ccd469"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# rps熵计算",
   "id": "e77caac915c79ed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_rps_entropy(evidence):\n",
    "    \"\"\"\n",
    "    严格遵循Example 4.1计算方法的RPS熵\n",
    "\n",
    "    参数:\n",
    "        evidence: 单个RPS证据体（字典格式，键为元组，值为质量）\n",
    "\n",
    "    返回:\n",
    "        entropy: 计算得到的熵值\n",
    "    \"\"\"\n",
    "    total_entropy = 0.0\n",
    "\n",
    "    # 预计算F(i)值（根据Example 4.1的算法）\n",
    "    def compute_F(i):\n",
    "        return sum(math.perm(i, k) for k in range(i + 1))\n",
    "\n",
    "    # 计算每个证据项的贡献\n",
    "    for items, mass in evidence.items():\n",
    "        if mass <= 0:\n",
    "            continue  # 跳过零质量项\n",
    "\n",
    "        i = len(items)  # 当前组合长度\n",
    "        F_i = compute_F(i)\n",
    "\n",
    "        # 特别注意：Example中F(1)-1=2-1=1，但按定义F(1)=1! =1 → 需要确认\n",
    "        # 根据Example 4.1的实际计算，F(1)=2, F(2)=5, F(3)=16\n",
    "        # 这表明原定义可能有调整，这里采用示例中的值\n",
    "\n",
    "        log_arg = mass / (F_i - 1)\n",
    "        term = mass * math.log(log_arg)  # term本身是负的\n",
    "        total_entropy -= term  # 负负得正\n",
    "        # print(f\"组合 {items}: mass={mass}, log参数={log_arg:.4f}, term={term:.4f}, 贡献={-term:.4f}\")\n",
    "    # hypothesis_complexity = 1 - 1 / len(evidence)\n",
    "    #     print(f\"证据 {evidence}的总rps熵: total_entropy={total_entropy}\")\n",
    "    # 标准化到0-1范围（熵值越小质量越高）\n",
    "    max_possible_entropy = math.log(len(evidence) * 10)  # 经验值\n",
    "    normalize_entropy = min(total_entropy / max_possible_entropy, 1.0)\n",
    "    # print(f\"证据 {evidence}的标准化熵: normalize_entropy={normalize_entropy}\")\n",
    "    return normalize_entropy"
   ],
   "id": "3888e3595e8b197c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# owa相似度计算",
   "id": "d1386835d2d11d8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " # 字母到数字的自动映射 (A->1, B->2, ..., Z->26)\n",
    "def to_numeric(x):\n",
    "    if isinstance(x, str) and x.isalpha() and x in ascii_uppercase:\n",
    "        return ascii_uppercase.index(x) + 1\n",
    "    return int(x)  # 如果是数字字符串或数字\n",
    "\n",
    "def compute_singleton_relation(i, j, alpha = 0.8):\n",
    "    \"\"\"\n",
    "    通用单例元素关系计算\n",
    "    支持: 字母(A-Z)、数字、或混合输入\n",
    "    关系公式: 1 - exp(-|a - b|)\n",
    "    \"\"\"\n",
    "    a = to_numeric(i)\n",
    "    b = to_numeric(j)\n",
    "    return alpha * (1 - np.exp(-abs(a - b))) + (1 - alpha) * np.abs(a - b)\n",
    "\n",
    "def compute_singleton_set(A, B, gamma):\n",
    "    \"\"\"计算单例到集合的关系\"\"\"\n",
    "    r_values = []\n",
    "    for b in B:\n",
    "        r_values.append(compute_singleton_relation(A, b))\n",
    "    return owa_aggregation(r_values, gamma, force_binary=False)\n",
    "\n",
    "\n",
    "def build_relation_matrix(focal_elements, gamma=0.8):\n",
    "    \"\"\"构建关系矩阵（适配字母型证据体）\"\"\"\n",
    "    n = len(focal_elements)\n",
    "    R_o = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):  # 仅计算上三角\n",
    "            A, B = focal_elements[i], focal_elements[j]\n",
    "\n",
    "            # Case 1: 单例-单例关系\n",
    "            if len(A) == 1 and len(B) == 1:\n",
    "                a = next(iter(A))  # 获取集合中的唯一元素\n",
    "                b = next(iter(B))\n",
    "                R_o[i, j] = compute_singleton_relation(a, b)\n",
    "\n",
    "            # Case 2: 单例-集合关系\n",
    "            elif len(A) == 1 and len(B) > 1:\n",
    "                a = next(iter(A))\n",
    "                R_o[i, j] = compute_singleton_set(a, B, gamma)\n",
    "\n",
    "            # Case 3: 集合-集合关系\n",
    "            elif len(A) > 1 and len(B) > 1:\n",
    "                r_values = []\n",
    "                for a in A:\n",
    "                    r_values.append(compute_singleton_set(a, B, gamma))\n",
    "                R_o[i, j] = owa_aggregation(r_values, gamma, force_binary=False)\n",
    "\n",
    "    # 对称填充下三角\n",
    "    R_o = np.triu(R_o) + np.triu(R_o, 1).T\n",
    "    return R_o\n",
    "\n",
    "def owa_aggregation(values, gamma, force_binary=False):\n",
    "    \"\"\"OWA聚合函数（保持不变）\"\"\"\n",
    "    if len(values) == 1:\n",
    "        return values[0]\n",
    "\n",
    "    if force_binary:  # 二元情况直接计算\n",
    "        sorted_v = sorted(values, reverse=True)\n",
    "        return gamma * sorted_v[0] + (1 - gamma) * sorted_v[1]\n",
    "\n",
    "    sorted_values = np.array(sorted(values, reverse=True))\n",
    "    n = len(sorted_values)\n",
    "\n",
    "    def entropy(w):\n",
    "        return -np.sum(w * np.log(w + 1e-10))\n",
    "\n",
    "    constraints = [\n",
    "        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},\n",
    "        {'type': 'eq', 'fun': lambda w: np.sum([(n - k - 1) / (n - 1) * w[k] for k in range(n)]) - gamma}\n",
    "    ]\n",
    "    bounds = [(0.001, 1) for _ in range(n)]\n",
    "    res = minimize(entropy, x0=np.ones(n) / n, bounds=bounds,\n",
    "                   constraints=constraints, options={'ftol': 1e-10})\n",
    "\n",
    "    weights = res.x / np.sum(res.x)\n",
    "    return np.dot(sorted_values, weights)\n",
    "\n",
    "def d_OWA(m1, m2, gamma=0.8):\n",
    "    \"\"\"计算两个证据体之间的距离\"\"\"\n",
    "    # 获取所有焦元（保持顺序一致性）\n",
    "    focal_elements = sorted(list(set(m1.keys()).union(set(m2.keys()))), key=lambda x: (len(x), x))\n",
    "\n",
    "    # 构建关系矩阵\n",
    "    R_o = build_relation_matrix(focal_elements, gamma)\n",
    "\n",
    "    # 转换为向量（保持相同顺序）\n",
    "    m1_vec = np.array([m1.get(f, 0) for f in focal_elements])\n",
    "    m2_vec = np.array([m2.get(f, 0) for f in focal_elements])\n",
    "\n",
    "    # 计算距离\n",
    "    diff = m1_vec - m2_vec\n",
    "    M = np.eye(len(R_o)) - R_o\n",
    "    distance = np.sqrt(0.5 * diff.T @ M @ diff)\n",
    "    return distance\n",
    "\n",
    "def owa_similarity_matrix(evidence, gamma=0.8):\n",
    "    \"\"\"计算证据体列表的相似度矩阵\"\"\"\n",
    "    # 转换证据体格式\n",
    "    # evidence_frozenset = [{frozenset(k): v for k, v in ev.items()} for ev in evidence]\n",
    "    evidence_frozenset = evidence.copy()\n",
    "    n = len(evidence_frozenset)\n",
    "    similarity_matrix = np.eye(n)  # 对角线为1\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distance = d_OWA(evidence_frozenset[i], evidence_frozenset[j], gamma)\n",
    "            similarity = 1 - distance\n",
    "            similarity_matrix[i][j] = similarity\n",
    "            similarity_matrix[j][i] = similarity\n",
    "\n",
    "    return similarity_matrix"
   ],
   "id": "fb503cf9f82487ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 置信度",
   "id": "88dcb556035409f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_credibility_from_similarity(S):\n",
    "    \"\"\"\n",
    "    计算归一化的可信度权重 Crd\n",
    "    - 输入: S (相似度矩阵)\n",
    "    - 输出: Crd (可信度权重向量)\n",
    "    \"\"\"\n",
    "    # 计算支持度 Sup(m_i)\n",
    "    Sup = np.sum(S, axis=1) - np.diag(S)  # 去掉自身对自身的支持度\n",
    "\n",
    "    # 计算可信度 Crd 并归一化\n",
    "    Crd = Sup / np.sum(Sup) if np.sum(Sup) != 0 else np.ones_like(Sup) / len(Sup)  # 避免除零\n",
    "\n",
    "    return Crd"
   ],
   "id": "b339606d78101154"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 权重计算",
   "id": "dc28d6eb41d4eb3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_evidence_quality(evidence):\n",
    "    \"\"\"综合质量评估（RPS熵+信息量+确定性）\"\"\"\n",
    "    prob = np.array(list(evidence.values()))\n",
    "    prob = prob / (np.sum(prob) + 1e-10)\n",
    "\n",
    "    # 1. RPS熵指标（取反，因为熵越低质量越高）\n",
    "    rps_entropy = compute_rps_entropy(evidence)\n",
    "    rps_score = 1- rps_entropy  # 已标准化\n",
    "\n",
    "    # 2. 信息量指标（香农熵）\n",
    "    # info_score = 1 - entropy(prob) / math.log(len(prob) + 1e-10)\n",
    "\n",
    "    # 3. 确定性指标\n",
    "    certainty = np.max(prob)\n",
    "\n",
    "    # 三重加权综合（可调参数）\n",
    "    return 0.7 * rps_score + 0.3 * certainty\n",
    "\n",
    "def compute_belief_from_similarity(S, evidence_list, alpha=0.8):\n",
    "    \"\"\"\n",
    "    增强版置信度计算（三级融合）\n",
    "    参数：\n",
    "    - S: 相似度矩阵\n",
    "    - evidence_list: 证据体列表\n",
    "    - alpha: 质量主导系数 (0-1)\n",
    "    返回：\n",
    "    - belief_weights: 最终权重\n",
    "    \"\"\"\n",
    "    n = len(evidence_list)\n",
    "    np.fill_diagonal(S, 0)  # 确保无自相似\n",
    "\n",
    "    # 第一阶段：基础可信度\n",
    "    Crd = compute_credibility_from_similarity(S)\n",
    "\n",
    "    # 第二阶段：质量评估（使用改进的compute_evidence_quality）\n",
    "    Quality = np.array([compute_evidence_quality(ev) for ev in evidence_list])\n",
    "    Q_norm = Quality / (np.sum(Quality) + 1e-10)\n",
    "\n",
    "    # 第三阶段：RPS熵加权一致性\n",
    "    rps_weights = np.array([1 - compute_rps_entropy(ev) for ev in evidence_list])\n",
    "    Consistency = np.sum(S * rps_weights.reshape(-1, 1), axis=1)\n",
    "    C_norm = Consistency / (np.sum(Consistency) + 1e-10)\n",
    "\n",
    "    # 动态融合（带可信度校准）\n",
    "    # combined = (alpha * Q_norm + (1 - alpha) * C_norm) * Crd\n",
    "    combined = (Q_norm + C_norm) * Crd\n",
    "    belief_weights = combined / (np.sum(combined) + 1e-10)\n",
    "\n",
    "    return belief_weights"
   ],
   "id": "148e642c95ab29d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 软似然函数",
   "id": "a8029d8c13fc1e71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def similarity_based_soft_likelihood(omega, evidence_list, R_values, alpha):\n",
    "    \"\"\"\n",
    "    基于相似度矩阵的软似然函数\n",
    "    - 输入:\n",
    "        omega: 待评估假设\n",
    "        evidence_list: 证据列表（每个证据为质量函数字典）\n",
    "        S: 相似度矩阵\n",
    "        alpha: 乐观系数\n",
    "    - 输出:\n",
    "        L: 软似然值\n",
    "    \"\"\"\n",
    "    # 获取各证据对假设的支持度\n",
    "    prob_values = [ev.get(omega, 0) for ev in evidence_list]\n",
    "\n",
    "    # 按加权支持度降序排序\n",
    "    sorted_indices = np.argsort([p * R for p, R in zip(prob_values, R_values)])[::-1]\n",
    "\n",
    "    # 计算权重向量\n",
    "    w = []\n",
    "    for i in range(len(evidence_list)):\n",
    "        if i == 0:\n",
    "            w.append(R_values[sorted_indices[i]] ** ((1 - alpha) / alpha))\n",
    "        else:\n",
    "            sum_k = np.sum([R_values[sorted_indices[k]] for k in range(i + 1)])\n",
    "            sum_k_prev = np.sum([R_values[sorted_indices[k]] for k in range(i)])\n",
    "            w.append(sum_k ** ((1 - alpha) / alpha) - sum_k_prev ** ((1 - alpha) / alpha))\n",
    "\n",
    "    # 计算软似然值\n",
    "    L = 0\n",
    "    for i in range(len(evidence_list)):\n",
    "        prod = np.prod([prob_values[sorted_indices[k]] for k in range(i + 1)])\n",
    "        L += w[i] * prod\n",
    "\n",
    "    return L"
   ],
   "id": "e7496e0832033bf0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 融合函数入口",
   "id": "d719fe54a844f3c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def fuse_evidence(evidence, R, hypotheses = {'A', 'B', 'C','D'}, alpha=0.1):\n",
    "    \"\"\"\n",
    "    计算融合概率：\n",
    "    - 使用软似然函数进行最终的概率估计\n",
    "    - 输入: evidence (列表), 归一化后的证据列表；R (数组), 权重；alpha (浮点数), 乐观系数。\n",
    "    - 输出: 融合后的概率分布。\n",
    "    \"\"\"\n",
    "    # hypotheses = {\n",
    "    # ('A',),\n",
    "    # ('B',),\n",
    "    # ('C',),\n",
    "    # ('B', 'A'),\n",
    "    # ('A', 'B'),\n",
    "    # ('A', 'C'),\n",
    "    # ('C', 'A', 'B'),\n",
    "    # ('B', 'C', 'A'),\n",
    "    # ('B', 'A', 'C'),\n",
    "    # ('A', 'C', 'B'),\n",
    "    # ('A', 'B', 'C')}\n",
    "    # hypotheses = {'A', 'B', 'C','D','E'}\n",
    "    fused = {}\n",
    "\n",
    "    for omega in hypotheses:\n",
    "        L = similarity_based_soft_likelihood(omega, evidence, R, alpha)\n",
    "        fused[omega] = L\n",
    "\n",
    "    # print(\"fused_result_before_nor:\", convert_numpy_types(fused))\n",
    "    # 归一化\n",
    "    total_L = sum(fused.values()) + 1e-10\n",
    "    fused = {k: v / total_L for k, v in fused.items()}\n",
    "    print(\"fused_result_after_nor:\", convert_numpy_types(fused))\n",
    "    return fused"
   ],
   "id": "a1490cf7dda585f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# OPT计算",
   "id": "be8e7692a31dc13b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_OPT(pmf_data, hypotheses={'A', 'B', 'C','D'}):\n",
    "    \"\"\"\n",
    "    直接基于假设集的OPT计算\n",
    "    参数：\n",
    "    - pmf_data: SLF融合后的概率分布字典 {tuple: prob}\n",
    "    - hypotheses: 预设的假设集合\n",
    "    返回：\n",
    "    - 标准化后的OPT概率分布 {class: prob}\n",
    "    \"\"\"\n",
    "    # 初始化概率分布\n",
    "    P_OPT = {h: 0.0 for h in hypotheses}\n",
    "\n",
    "    # 按命题长度排序处理（单元素优先）\n",
    "    sorted_items = sorted(pmf_data.items(),\n",
    "                          key=lambda x: (len(x[0]), x[0]))\n",
    "\n",
    "    for items, mass in sorted_items:\n",
    "        items_set = set(items)  # 转换为集合避免顺序影响\n",
    "        if len(items_set) == 1:\n",
    "            # 单元素直接分配\n",
    "            theta = next(iter(items_set))  # 获取唯一元素\n",
    "            P_OPT[theta] += mass\n",
    "        else:\n",
    "            # 多元素按OPT规则分配\n",
    "            last_element = items[-1]  # 保持原顺序的最后一个元素\n",
    "            for theta in items_set:\n",
    "                if theta != last_element:\n",
    "                    P_OPT[theta] += mass / (len(items_set) - 1)\n",
    "\n",
    "    # 标准化处理\n",
    "    total = sum(P_OPT.values())\n",
    "    return {k: v / total for k, v in P_OPT.items()}\n",
    "\n",
    "def batch_OPT(data_list, hypotheses = {'A', 'B', 'C','D'}):\n",
    "    opt_list = []\n",
    "    for d in data_list:\n",
    "        opt_one  = calculate_OPT(d, hypotheses)\n",
    "        print(\"opt_one:\", convert_numpy_types(opt_one))\n",
    "        opt_list.append(opt_one)\n",
    "    return opt_list"
   ],
   "id": "2f52baa6337071f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 主函数入口",
   "id": "6b8ba6eb777f5bb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main_function(evidence, hypotheses_all, hypotheses = {'A', 'B', 'C','D'}):\n",
    "    # 处理证据体格式\n",
    "    # normalized_evidence = normalize_evidence(evidence)\n",
    "    # OPT_evidence = batch_OPT(evidence, hypotheses)\n",
    "\n",
    "    # 计算相似度矩阵\n",
    "    S = owa_similarity_matrix(evidence)\n",
    "\n",
    "    # 计算权重 R\n",
    "    print(S)\n",
    "    # 计算综合置信度权重\n",
    "    R_values = compute_belief_from_similarity(S, evidence)\n",
    "    # 计算融合概率\n",
    "    alpha = 0.1 # 乐观系数\n",
    "    fused_probabilities = fuse_evidence(evidence, R_values, hypotheses_all, alpha)\n",
    "\n",
    "    print(\"Optimal R:\", R_values)\n",
    "    fused_end = calculate_OPT(fused_probabilities, hypotheses)\n",
    "    print(\"Fused probabilities (after SLF):\", convert_numpy_types(fused_end))"
   ],
   "id": "c54149b65e8d7930"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def convert_numpy_types(obj):\n",
    "    \"\"\"\n",
    "    递归转换数据结构中的 np.int64 和 np.float64 为 Python 原生类型\n",
    "    支持处理: 列表/元组/集合/字典/嵌套结构\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (str, bytes, bytearray)):\n",
    "        return obj\n",
    "    elif isinstance(obj, dict):\n",
    "        return {convert_numpy_types(k): convert_numpy_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, Iterable) and not isinstance(obj, (str, bytes)):\n",
    "        return type(obj)(convert_numpy_types(x) for x in obj)\n",
    "    else:\n",
    "        return obj"
   ],
   "id": "4e5ef85c37c8a344"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# rps生成函数",
   "id": "1ecf995a375438c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 生成所有按顺序选择的排列组合",
   "id": "452a7dac78afde83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 生成所有按顺序选择的排列组合\n",
    "def get_ordered_permutations(num_classes):\n",
    "    result = []\n",
    "    # 逐步增加元素数量\n",
    "    for i in range(1, num_classes + 1):\n",
    "        # 生成i个元素的全排列\n",
    "        result.extend(itertools.permutations(range(i), i))\n",
    "    return result"
   ],
   "id": "a893db76dd4e2056"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 计算 weighted PMF",
   "id": "3a743181b4fa07ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 计算 weighted PMF\n",
    "def calculate_weighted_pmf(weight_matrix, sorted_nmv):\n",
    "    num_combinations, num_attributes = weight_matrix.shape\n",
    "    num_classes = sorted_nmv.shape[0]  # 获取类的数量（classes）\n",
    "\n",
    "    # 获取排列组合\n",
    "    # all_combinations = get_ordered_permutations(num_classes)\n",
    "\n",
    "    # 初始化 weighted_pmf 矩阵\n",
    "    weighted_pmf = np.zeros_like(weight_matrix)\n",
    "\n",
    "    # 记录当前组合数对应的起始位置\n",
    "    current_row = 0\n",
    "\n",
    "    # 遍历组合大小 i（从 1 到 num_classes）\n",
    "    for i in range(1, num_classes + 1):\n",
    "        num_permutations = len(list(itertools.permutations(range(i), i)))  # 当前大小的排列组合数量\n",
    "\n",
    "        # 遍历每个属性 j\n",
    "        for j in range(num_attributes):\n",
    "            # 对于当前大小 i 的排列组合，使用 sorted_nmv[i-1, j]\n",
    "            # 组合有几个类，就乘于多少行\n",
    "            weighted_pmf[current_row:current_row + num_permutations, j] = (\n",
    "                    weight_matrix[current_row:current_row + num_permutations, j] * sorted_nmv[i - 1, j]\n",
    "            )\n",
    "\n",
    "        # 更新起始行\n",
    "        current_row += num_permutations\n",
    "\n",
    "    return weighted_pmf"
   ],
   "id": "293190b71e7734a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## rps生成",
   "id": "367784354979a662"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 获取按顺序选择的排列组合\n",
    "def gen_rps_fun(test_size=0.2):\n",
    "    # 1. 加载Iris数据集并划分为训练集和测试集\n",
    "    iris = load_iris()\n",
    "    X = iris.data  # 四个属性\n",
    "    y = iris.target  # 三个类 (0, 1, 2)\n",
    "    num_classes = len(np.unique(iris.target))\n",
    "    num_attributes = iris.data.shape[1]\n",
    "    # 将数据集划分为训练集和测试集，乱序\n",
    "    print(\"\\n当前 (test_size):\\n\", test_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=True, random_state=42)\n",
    "    # 2. 计算每个类中每个属性的 mean value and standard deviation (无偏估计)\n",
    "    mean_std_by_class = []\n",
    "    for class_label in np.unique(y_train):\n",
    "        X_class = X_train[y_train == class_label]\n",
    "        mean_std = [(np.mean(X_class[:, i]), np.std(X_class[:, i], ddof=1)) for i in range(X_class.shape[1])]\n",
    "        mean_std_by_class.append(mean_std)\n",
    "\n",
    "    mean_std_by_class = np.array(mean_std_by_class)\n",
    "    # print(\"每个类中每个属性的均值和标准差:\\n\", mean_std_by_class)\n",
    "    # print(\"Shape of mean_std_by_class:\\n\", mean_std_by_class.shape)\n",
    "    # 3. 为每个类和每个属性建立高斯分布函数，并对测试集中随机选取的一个样本进行预测\n",
    "\n",
    "    # 保存下(3,4)个Gaussian distribution函数\n",
    "    # 创建一个(3,4)的函数数组，用来存储每个类中每个属性的高斯分布函数\n",
    "    # 对于每个类中的每个属性，计算该样本在该属性下的高斯分布的概率密度值。\n",
    "    gaussian_functions = np.empty((3, 4), dtype=object)\n",
    "\n",
    "    # 初始化并保存高斯分布函数\n",
    "    for class_label in range(num_classes):\n",
    "        for i in range(num_attributes):  # 四个属性\n",
    "            mean, std = mean_std_by_class[class_label, i]\n",
    "            # 保存高斯分布函数\n",
    "            gaussian_functions[class_label, i] = norm(loc=mean, scale=std)\n",
    "\n",
    "    # 随机选择一个测试集中的样本\n",
    "    test_sample = X_test[np.random.randint(0, len(X_test))]\n",
    "\n",
    "    # 计算该测试样本在每个类中每个属性的高斯分布结果\n",
    "    gaussian_results = []\n",
    "    for class_label in range(num_classes):\n",
    "        class_results = []\n",
    "        for i in range(num_attributes):  # 四个属性\n",
    "            # 调用保存的高斯分布函数，计算概率密度值\n",
    "            # pdf是SciPy的norm对象的方法，用于计算给定值在该正态分布下的概率密度值。\n",
    "            pdf_value = gaussian_functions[class_label, i].pdf(test_sample[i])\n",
    "            class_results.append(pdf_value)\n",
    "        gaussian_results.append(class_results)\n",
    "\n",
    "    gaussian_results = np.array(gaussian_results)\n",
    "    # print(\"\\n测试集中选取的样本:\", test_sample)\n",
    "    # print(\"\\n每个类中每个属性的高斯分布函数值:\\n\", gaussian_results)\n",
    "    column_sums = np.sum(gaussian_results, axis=0)\n",
    "    normalized_results = gaussian_results / column_sums\n",
    "    # print(\"\\n每个属性针对所有类的归一化后的MV (归一化后的高斯分布值):\\n\", normalized_results)\n",
    "    # 对归一化后的MV（normalized membership vector）进行降序排序，并保留原始顺序的索引\n",
    "    sorted_indices = np.argsort(-normalized_results, axis=0)  # 降序排序，使用负号实现降序\n",
    "    sorted_nmv = np.take_along_axis(normalized_results, sorted_indices, axis=0)  # 按照索引排序后的值\n",
    "    sorted_gaussian_functions = np.take_along_axis(gaussian_functions, sorted_indices, axis=0)  # 按照索引排序后的GDM\n",
    "\n",
    "    # 打印结果\n",
    "    # print(\"\\n归一化后的MV降序排序的结果:\\n\", sorted_nmv)\n",
    "    # print(\"\\n每个元素排序前的原始类索引:\\n\", sorted_indices)\n",
    "    x_mean_ord = np.empty((3, 4))\n",
    "    std_ord = np.empty((3, 4))\n",
    "\n",
    "    # mean_std_by_class 的 shape 是 (3, 4, 2)，索引 [class, attribute, 0] 获取均值，索引 [class, attribute, 1] 获取标准差\n",
    "    for attr_idx in range(num_attributes):  # 对每个属性进行操作\n",
    "        for class_idx in range(num_classes):  # 对每个类进行操作\n",
    "            sorted_class_idx = sorted_indices[class_idx, attr_idx]  # 获取排序后的类索引\n",
    "            x_mean_ord[class_idx, attr_idx] = mean_std_by_class[sorted_class_idx, attr_idx, 0]  # 获取排序后的均值\n",
    "            std_ord[class_idx, attr_idx] = mean_std_by_class[sorted_class_idx, attr_idx, 1]  # 获取排序后的标准差\n",
    "\n",
    "    # print(\"\\n排序后的 x_mean_ord:\\n\", x_mean_ord)\n",
    "    # print(\"\\n排序后的 std_ord:\\n\", std_ord)\n",
    "    supporting_degree = np.exp(-np.abs(test_sample - x_mean_ord))\n",
    "\n",
    "    # print(\"\\nSupporting degree (支持度):\\n\", supporting_degree)\n",
    "    all_combinations = get_ordered_permutations(num_classes)\n",
    "    print(all_combinations)\n",
    "    # 初始化权重矩阵 weight_matrix\n",
    "    num_combinations = len(all_combinations)  # 所有按顺序排列组合的数量 (应该是9)\n",
    "    weight_matrix = np.zeros((num_combinations, num_attributes))  # (9, 4)\n",
    "\n",
    "    # 对每个属性计算权重,也就是每个属性下的权重\n",
    "    for attr_idx in range(num_attributes):\n",
    "        # 取第一列，第一列对应属性1\n",
    "        s = supporting_degree[:, attr_idx]  # 取出该属性对应的支持度 (3,)\n",
    "\n",
    "        # 遍历每个组合，计算 w(i1...iu...iq)\n",
    "        for comb_idx, combination in enumerate(all_combinations):\n",
    "            q = len(combination)  # 该组合的长度\n",
    "            weight = 1.0  # 初始化权重\n",
    "\n",
    "            # 根据公式 (19) 计算权重\n",
    "            for u in range(q):\n",
    "                i_u = combination[u]  # 当前排列项 i_u\n",
    "                numerator = s[i_u]  # 分子支持度\n",
    "                denominator_sum = np.sum(s[list(combination[u:])])  # 分母，从 u 到 q 的支持度和\n",
    "                weight *= numerator / denominator_sum  # 按公式累乘\n",
    "\n",
    "            # 将计算好的权重保存到 weight_matrix\n",
    "            weight_matrix[comb_idx, attr_idx] = weight\n",
    "\n",
    "    # 输出权重矩阵\n",
    "    # print(\"\\n权重矩阵 (Weight matrix):\\n\", weight_matrix)\n",
    "    weighted_pmf = calculate_weighted_pmf(weight_matrix, sorted_nmv)\n",
    "    RPS_w = []\n",
    "    for j in range(num_attributes):\n",
    "        RPS_w_j = set()\n",
    "\n",
    "        thetas = sorted_indices[:, j]\n",
    "        weighted_pmf_j = weighted_pmf[:, j]\n",
    "\n",
    "        for idx, combination in enumerate(all_combinations):\n",
    "            A = thetas[list(combination)]\n",
    "            M_A = weighted_pmf_j[idx]\n",
    "            A = tuple((A))\n",
    "            RPS_w_j.add((A, M_A))\n",
    "\n",
    "        RPS_w.append(RPS_w_j)\n",
    "    return RPS_w"
   ],
   "id": "3e2477c948fc3836"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义映射关系\n",
    "index_to_label = {0: 'A', 1: 'B', 2: 'C'}\n",
    "\n",
    "# 转换函数\n",
    "def convert_to_labeled_rps(gen_rps):\n",
    "    labeled_evidence = []\n",
    "    for rps in gen_rps:\n",
    "        converted = {}\n",
    "        for items, mass in rps:\n",
    "            # 转换数字索引为字母标签\n",
    "            labeled_items = tuple(index_to_label[i] for i in items)\n",
    "            converted[labeled_items] = mass\n",
    "        labeled_evidence.append(converted)\n",
    "    return labeled_evidence"
   ],
   "id": "3e32d2e7e59cb3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_hypotheses(evidence_rps):\n",
    "    \"\"\"\n",
    "    从RPS证据结构中抽取所有唯一的假设组合\n",
    "\n",
    "    参数:\n",
    "        evidence_rps: 包含多个RPS证据字典的列表\n",
    "\n",
    "    返回:\n",
    "        包含所有唯一假设的集合(set)，每个假设是一个元组\n",
    "    \"\"\"\n",
    "    hypotheses = set()\n",
    "    for evidence in evidence_rps:\n",
    "        hypotheses.update(evidence.keys())\n",
    "    return hypotheses"
   ],
   "id": "e27c0773b5840510"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RPS generation method# 方法调用",
   "id": "2a6844536438a36b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "     # 你的证据体\n",
    "evidenceList = []\n",
    "evidence5 = [\n",
    "    {('A',): 0.7, ('B',): 0.15, ('C',): 0.15},\n",
    "    {('B',): 0.5, ('C',): 0.5},\n",
    "    {('A',): 0.7, ('B',): 0.15, ('C',): 0.15},\n",
    "    {('A',): 0.7, ('B',): 0.15, ('C',): 0.15},\n",
    "    {('A', 'C'): 0.8, ('B',): 0.2}\n",
    "]\n",
    "# evidenceList.append(evidence5)\n",
    "# evidence = [\n",
    "#     {('A',): 0.6, ('B',): 0.3, ('A','B'): 0.1},  # 证据体 1\n",
    "#     {('A',): 0.5, ('B',): 0.4, ('A','B'): 0.1}  # 证据体 2\n",
    "# ]\n",
    "\n",
    "# evidence = [\n",
    "#     {('A',): 0.5, ('B',): 0.2, ('C',): 0.3},\n",
    "#     {('B',): 0.9, ('C',): 0.1},\n",
    "#     {('A',): 0.55, ('B',): 0.1, ('A','C'): 0.35},\n",
    "#     {('A',): 0.55, ('B',): 0.1, ('A','C'): 0.35},\n",
    "#     {('A',): 0.6, ('B',): 0.1, ('A', 'C'): 0.3}\n",
    "# ]\n",
    "\n",
    "evidence1 = [\n",
    "    {('A',): 0.0437, ('B',): 0.3346, ('C',): 0.2916, ('A', 'B'): 0.0437, ('A', 'C'): 0.0239, ('B', 'C'): 0.2385,\n",
    "     ('A', 'B', 'C'): 0.0239},  # 证据体 1 (Sepal Length)\n",
    "    {('A',): 0.0865, ('B',): 0.2879, ('C',): 0.1839, ('A', 'B'): 0.0863, ('A', 'C'): 0.0865, ('B', 'C'): 0.1825,\n",
    "     ('A', 'B', 'C'): 0.0863},  # 证据体 2 (Sepal Width)\n",
    "    {('A',): 1.4e-09, ('B',): 0.6570, ('C',): 0.1726, ('A', 'B'): 1.3e-09, ('A', 'C'): 1.4e-11, ('B', 'C'): 0.1704,\n",
    "     ('A', 'B', 'C'): 1.4e-11},  # 证据体 3 (Petal Length)\n",
    "    {('A',): 8.20e-06, ('B',): 0.6616, ('C',): 0.1692, ('A', 'B'): 8.20e-06, ('A', 'C'): 3.80e-06, ('B', 'C'): 0.1692,\n",
    "     ('A', 'B', 'C'): 3.80e-06}  # 证据体 4 (Petal Width)\n",
    "]\n",
    "# evidenceList.append(evidence1)\n",
    "# 应用一的数据\n",
    "# evidence2 = [\n",
    "#     {('A',): 0.40, ('B',): 0.28, ('C',): 0.30, ('A', 'C'): 0.02},  # S₁\n",
    "#     {('A',): 0.01, ('B',): 0.90, ('C',): 0.08, ('A', 'C'): 0.01},  # S₂\n",
    "#     {('A',): 0.63, ('B',): 0.06, ('C',): 0.01, ('A', 'C'): 0.30},  # S₃\n",
    "#     {('A',): 0.60, ('B',): 0.09, ('C',): 0.01, ('A', 'C'): 0.30},  # S₄\n",
    "#     {('A',): 0.60, ('B',): 0.09, ('C',): 0.01, ('A', 'C'): 0.30}   # S₅\n",
    "# ]\n",
    "evidence2 = [\n",
    "    {('A',): 0.40, ('B',): 0.28, ('C',): 0.30, ('A', 'C'): 0.01, ('C', 'A'): 0.01},  # S₁\n",
    "    {('A',): 0.01, ('B',): 0.90, ('C',): 0.08, ('A', 'C'): 0.005, ('C', 'A'): 0.005},  # S₂\n",
    "    {('A',): 0.63, ('B',): 0.06, ('C',): 0.01, ('A', 'C'): 0.02, ('C', 'A'): 0.01},  # S₃\n",
    "    {('A',): 0.60, ('B',): 0.09, ('C',): 0.01, ('A', 'C'): 0.02, ('C', 'A'): 0.01},  # S₄\n",
    "    {('A',): 0.60, ('B',): 0.09, ('C',): 0.01, ('A', 'C'): 0.02, ('C', 'A'): 0.02}  # S₅\n",
    "]\n",
    "# evidenceList.append(evidence2)\n",
    "#\n",
    "# # 应用二的数据\n",
    "evidence3 = [\n",
    "    {('A',): 0.7, ('B',): 0.1, ('A', 'B', 'C'): 0.2},  # 证据体 m1\n",
    "    {('A',): 0.7, ('A', 'B', 'C'): 0.3},  # 证据体 m2\n",
    "    {('A',): 0.65, ('B',): 0.15, ('A', 'B', 'C'): 0.20},  # 证据体 m3\n",
    "    {('A',): 0.75, ('C',): 0.05, ('A', 'B', 'C'): 0.20},  # 证据体 m4\n",
    "    {('B',): 0.20, ('C',): 0.80}  # 证据体 m5\n",
    "]\n",
    "# evidenceList.append(evidence3)\n",
    "evidence4 = [\n",
    "    {('A',): 0.6, ('B',): 0.15, ('C',): 0.15, ('D',): 0, ('E',): 0.1},  # 证据体 m1\n",
    "    {('A',): 0.001, ('B',): 0.45, ('C',): 0.15, ('D',): 0.24, ('E',): 0.159},  # 证据体 m2\n",
    "    {('A',): 0.55, ('B',): 0.1, ('C',): 0.1, ('D',): 0.15, ('E',): 0.1},  # 证据体 m3\n",
    "    {('A',): 0.8, ('B',): 0.1, ('C',): 0.05, ('D',): 0, ('E',): 0.05},  # 证据体 m3\n",
    "]\n",
    "# evidenceList.append(evidence4)\n",
    "\n",
    "evidence5 = [\n",
    "    {  # 传感器1\n",
    "        ('A',): 0.31,\n",
    "        ('B',): 0.0,\n",
    "        ('C',): 0.29,\n",
    "        ('A', 'C',): 0.0,\n",
    "        ('C', 'A',): 0.0,\n",
    "        ('A', 'B', 'C'): 0.0167,\n",
    "        ('A', 'C', 'B'): 0.0167,\n",
    "        ('B', 'A', 'C'): 0.0167,\n",
    "        ('B', 'C', 'A'): 0.0167,\n",
    "        ('C', 'A', 'B'): 0.3167,\n",
    "        ('C', 'B', 'A'): 0.0167\n",
    "    },\n",
    "    {  # 传感器2\n",
    "        ('A',): 0.0,\n",
    "        ('B',): 0.8,\n",
    "        ('C',): 0.2,\n",
    "        ('A', 'C',): 0.0,\n",
    "        ('C', 'A',): 0.0,\n",
    "        ('A', 'B', 'C'): 0.0,\n",
    "        ('A', 'C', 'B'): 0.0,\n",
    "        ('B', 'A', 'C'): 0.0,\n",
    "        ('B', 'C', 'A'): 0.0,\n",
    "        ('C', 'A', 'B'): 0.0,\n",
    "        ('C', 'B', 'A'): 0.0\n",
    "    },\n",
    "    {  # 传感器3\n",
    "        ('A',): 0.27,\n",
    "        ('B',): 0.07,\n",
    "        ('C',): 0.21,\n",
    "        ('A', 'C',): 0.0,\n",
    "        ('C', 'A',): 0.0,\n",
    "        ('A', 'B', 'C'): 0.025,\n",
    "        ('A', 'C', 'B'): 0.025,\n",
    "        ('B', 'A', 'C'): 0.025,\n",
    "        ('B', 'C', 'A'): 0.025,\n",
    "        ('C', 'A', 'B'): 0.325,\n",
    "        ('C', 'B', 'A'): 0.025\n",
    "    },\n",
    "    {  # 传感器4\n",
    "        ('A',): 0.25,\n",
    "        ('B',): 0.05,\n",
    "        ('C',): 0.3,\n",
    "        ('A', 'C',): 0.09,\n",
    "        ('C', 'A',): 0.31,\n",
    "        ('A', 'B', 'C'): 0.0,\n",
    "        ('A', 'C', 'B'): 0.0,\n",
    "        ('B', 'A', 'C'): 0.0,\n",
    "        ('B', 'C', 'A'): 0.0,\n",
    "        ('C', 'A', 'B'): 0.3,\n",
    "        ('C', 'B', 'A'): 0.0\n",
    "    },\n",
    "    {  # 专家\n",
    "        ('A',): 0.25,\n",
    "        ('B',): 0.0,\n",
    "        ('C',): 0.2,\n",
    "        ('A', 'C'): 0.36,\n",
    "        ('C', 'A'): 0.0,\n",
    "        ('A', 'B', 'C'): 0.0233,\n",
    "        ('A', 'C', 'B'): 0.0733,\n",
    "        ('B', 'A', 'C'): 0.0233,\n",
    "        ('B', 'C', 'A'): 0.0233,\n",
    "        ('C', 'A', 'B'): 0.0233,\n",
    "        ('C', 'B', 'A'): 0.0233\n",
    "    }\n",
    "]\n",
    "# evidenceList.append(evidence5)\n",
    "\n",
    "evidence_rps = [\n",
    "    {  # 第一组RPS证据\n",
    "        ('A',): 0.2,\n",
    "        ('B',): 0.08,\n",
    "        ('C',): 0.0,\n",
    "        ('B', 'A'): 0.05,\n",
    "        ('A', 'B'): 0.12,\n",
    "        ('A', 'C'): 0.03,\n",
    "        ('C', 'A', 'B'): 0.0,\n",
    "        ('B', 'C', 'A'): 0.05,\n",
    "        ('B', 'A', 'C'): 0.1,\n",
    "        ('A', 'C', 'B'): 0.25,\n",
    "        ('A', 'B', 'C'): 0.12\n",
    "    },\n",
    "    {  # 第二组RPS证据\n",
    "        ('A',): 0.07,\n",
    "        ('B',): 0.13,\n",
    "        ('C',): 0.02,\n",
    "        ('B', 'A'): 0.2,\n",
    "        ('A', 'B'): 0.07,\n",
    "        ('A', 'C'): 0.1,\n",
    "        ('C', 'A', 'B'): 0.08,\n",
    "        ('B', 'C', 'A'): 0.0,\n",
    "        ('B', 'A', 'C'): 0.2,\n",
    "        ('A', 'C', 'B'): 0.0,\n",
    "        ('A', 'B', 'C'): 0.13\n",
    "    },\n",
    "    {  # 第三组RPS证据\n",
    "        ('A',): 0.14,\n",
    "        ('B',): 0.09,\n",
    "        ('C',): 0.0,\n",
    "        ('B', 'A'): 0.08,\n",
    "        ('A', 'B'): 0.12,\n",
    "        ('A', 'C'): 0.00,\n",
    "        ('C', 'A', 'B'): 0.05,\n",
    "        ('B', 'C', 'A'): 0.0,\n",
    "        ('B', 'A', 'C'): 0.1,\n",
    "        ('A', 'C', 'B'): 0.3,\n",
    "        ('A', 'B', 'C'): 0.12\n",
    "    }\n",
    "]\n",
    "evidenceList.append(evidence_rps)\n",
    "example5_2 = [\n",
    "    {  # RPS₁\n",
    "        ('A',): 0.0,  # 零质量可保留或移除\n",
    "        ('B', 'C'): 0.05,\n",
    "        ('C', 'B'): 0.05,  # 顺序敏感保留\n",
    "        ('D',): 0.9\n",
    "    },\n",
    "    {  # RPS₂\n",
    "        ('A',): 0.9,\n",
    "        ('B', 'C'): 0.05,\n",
    "        ('C', 'B'): 0.05,\n",
    "        ('D',): 0.0\n",
    "    },\n",
    "    {  # RPS₃\n",
    "        ('A',): 0.8,\n",
    "        ('B', 'C'): 0.05,\n",
    "        ('C', 'B'): 0.05,\n",
    "        ('D',): 0.1\n",
    "    }\n",
    "]\n",
    "# evidenceList.append(example5_2)\n",
    "example5_3 = [\n",
    "    {  # RPS₁\n",
    "        ('A',): 0.9,\n",
    "        ('B', 'C'): 0.01,\n",
    "        ('C', 'B'): 0.0,  # 显式保留零值\n",
    "        ('D',): 0.09\n",
    "    },\n",
    "    {  # RPS₂\n",
    "        ('A',): 0.0,\n",
    "        ('B', 'C'): 0.01,\n",
    "        ('C', 'B'): 0.9,  # 注意与RPS₁的顺序相反\n",
    "        ('D',): 0.09\n",
    "    },\n",
    "    {  # RPS₃\n",
    "        ('A',): 0.5,\n",
    "        ('B', 'C'): 0.01,\n",
    "        ('C', 'B'): 0.4,  # 顺序敏感值\n",
    "        ('D',): 0.09\n",
    "    }\n",
    "]\n",
    "# evidenceList.append(example5_3)\n",
    "example5_4 = [\n",
    "    {  # RPS₁\n",
    "        ('A', 'B', 'C'): 0.9,\n",
    "        ('A', 'C', 'B'): 0.0,  # 显式保留零值\n",
    "        ('B', 'A', 'C'): 0.05,\n",
    "        ('D',): 0.05\n",
    "    },\n",
    "    {  # RPS₂\n",
    "        ('A', 'B', 'C'): 0.0,\n",
    "        ('A', 'C', 'B'): 0.9,  # 注意顺序差异\n",
    "        ('B', 'A', 'C'): 0.05,\n",
    "        ('D',): 0.05\n",
    "    },\n",
    "    {  # RPS₃\n",
    "        ('A', 'B', 'C'): 0.5,\n",
    "        ('A', 'C', 'B'): 0.4,  # 顺序敏感值\n",
    "        ('B', 'A', 'C'): 0.05,\n",
    "        ('D',): 0.05\n",
    "    }\n",
    "]\n",
    "# evidenceList.append(example5_4)\n",
    "# test_size_list = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n",
    "# test_size_list = [0.3]\n",
    "# for test_size in test_size_list:\n",
    "#     gen_rps = gen_rps_fun(test_size)\n",
    "#     print(convert_numpy_types(gen_rps))\n",
    "#     labeled_evidence = convert_to_labeled_rps(gen_rps)\n",
    "#     evidenceList.append(labeled_evidence)\n",
    "hypotheses_all = extract_hypotheses(evidence_rps)\n",
    "hypotheses = {'A', 'B', 'C','D'}\n",
    "for evidence in evidenceList:\n",
    "    main_function(evidence, hypotheses_all, hypotheses)"
   ],
   "id": "8488b06d29a9ca7f"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
