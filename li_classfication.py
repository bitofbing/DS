import copy
import math
import itertools
from collections.abc import Iterable
import numpy as np
from collections import defaultdict
from sklearn.model_selection import RepeatedKFold
from sklearn.datasets import load_iris, load_wine, load_breast_cancer, load_digits
from typing import Dict, List, Tuple, Any
from sklearn.preprocessing import StandardScaler
from scipy.stats import norm
import pickle
import os
from ucimlrepo import fetch_ucirepo
import pandas as pd

import itertools
import math
import numpy as np
from collections import defaultdict
from functools import lru_cache
# ==============================================================
# ğŸ”¹ å·¥å…·å‡½æ•°
# ==============================================================

def get_all_elements_from_dict(rps_dict):
    """ä»å•ä¸ªRPSå­—å…¸ä¸­æå–æ‰€æœ‰å”¯ä¸€å…ƒç´ """
    elements = set()
    for perm in rps_dict.keys():
        elements.update(perm)
    return tuple(sorted(elements))


def flatten_rps_input(rps_input):
    """ç¡®ä¿è¾“å…¥ä¸º [dict, dict, ...] æ ¼å¼"""
    if isinstance(rps_input, dict):
        return [rps_input]
    elif isinstance(rps_input, list) and all(isinstance(d, dict) for d in rps_input):
        return rps_input
    else:
        raise TypeError("è¾“å…¥å¿…é¡»æ˜¯å­—å…¸æˆ–å­—å…¸åˆ—è¡¨ï¼Œä¾‹å¦‚ example5_4ã€‚")


# ==============================================================
# ğŸ”¹ æœ‰åºåº¦è®¡ç®—ï¼ˆå¸¦ç¼“å­˜ï¼‰
# ==============================================================

@lru_cache(maxsize=None)
def ordered_degree_cached(perm_i, perm_j):
    """Definition 2.7ï¼šæœ‰åºåº¦è®¡ç®—ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    inter = set(perm_i) & set(perm_j)
    if not inter:
        return 0.0
    diffs = [abs(perm_i.index(e) - perm_j.index(e)) for e in inter]
    return math.exp(-np.mean(diffs))


# ==============================================================
# ğŸ”¹ RPSè·ç¦»è®¡ç®—ï¼ˆå¸¦ç¼“å­˜ï¼‰
# ==============================================================

def rps_distance(rps1_dict, rps2_dict, cache=None):
    """Definition 2.8ï¼šRPS è·ç¦»è®¡ç®—"""
    all_perms = sorted(set(rps1_dict.keys()) | set(rps2_dict.keys()), key=lambda x: (len(x), x))
    vec1 = np.array([rps1_dict.get(p, 0.0) for p in all_perms])
    vec2 = np.array([rps2_dict.get(p, 0.0) for p in all_perms])
    n = len(all_perms)

    # å¤ç”¨ RD çŸ©é˜µç¼“å­˜
    key = tuple(all_perms)
    if cache is not None and key in cache:
        RD = cache[key]
    else:
        RD = np.zeros((n, n))
        for i in range(n):
            set_i = set(all_perms[i])
            for j in range(i, n):
                set_j = set(all_perms[j])
                inter = len(set_i & set_j)
                union = len(set_i | set_j)
                if union == 0:
                    val = 0
                else:
                    val = (inter / union) * ordered_degree_cached(all_perms[i], all_perms[j])
                RD[i, j] = RD[j, i] = val
        if cache is not None:
            cache[key] = RD

    diff = vec1 - vec2
    return math.sqrt(0.5 * diff @ RD @ diff.T)


# ==============================================================
# ğŸ”¹ ç›¸ä¼¼åº¦çŸ©é˜µ
# ==============================================================

def similarity_matrix(rps_input):
    """Definition 4.1ï¼šæ„å»ºRPSç›¸ä¼¼åº¦çŸ©é˜µ"""
    rps_list = flatten_rps_input(rps_input)
    n = len(rps_list)
    sim = np.eye(n)
    cache = {}

    for i in range(n):
        for j in range(i + 1, n):
            d = rps_distance(rps_list[i], rps_list[j], cache)
            sim[i, j] = sim[j, i] = max(0, 1 - d)
    return sim


# ==============================================================
# ğŸ”¹ æ”¯æŒåº¦ & å¯ä¿¡åº¦
# ==============================================================

def support_degree(sim_matrix):
    """Definition 4.2ï¼šæ”¯æŒåº¦"""
    return np.sum(sim_matrix, axis=1) - 1


def credibility_degree(support):
    """Definition 4.3ï¼šå¯ä¿¡åº¦"""
    total = np.sum(support)
    return np.ones_like(support) / len(support) if total == 0 else support / total


# ==============================================================
# ğŸ”¹ ç†µè®¡ç®—ï¼ˆDefinition 2.9ï¼‰
# ==============================================================

@lru_cache(maxsize=None)
def F_function(i):
    """è®¡ç®— F(i) (Formula 17)"""
    return sum(math.factorial(i) / math.factorial(i - k) for k in range(i + 1))


def rps_entropy(rps_dict):
    """è®¡ç®—RPSç†µ"""
    entropy = 0.0
    for perm, mass in rps_dict.items():
        if mass > 0:
            Fi = F_function(len(perm))
            if Fi > 1:
                entropy -= mass * math.log(mass / (Fi - 1))
    return entropy


# ==============================================================
# ğŸ”¹ åŠ æƒå­é›†èåˆï¼ˆDefinition 4.4ï¼‰
# ==============================================================
from collections import defaultdict
import itertools
from concurrent.futures import ProcessPoolExecutor, as_completed
import math


def weighted_subset_rps(rps_dict_list, credibilities, n_min=2, max_workers=None, chunk_size=1000):
    """è®¡ç®—åŠ æƒå­é›†RPS (å†…å­˜ä¼˜åŒ–+å¹¶è¡Œç‰ˆæœ¬)

    å‚æ•°:
        max_workers: å¹¶è¡Œè¿›ç¨‹æ•° (None=è‡ªåŠ¨)
        chunk_size: æ¯ä¸ªè¿›ç¨‹å¤„ç†çš„ä»»åŠ¡å—å¤§å°
    """
    # é¢„è®¡ç®—å…¨å±€æ•°æ®
    all_perms = sorted(set().union(*[r.keys() for r in rps_dict_list]),
                       key=lambda x: (len(x), x))
    n_rps = len(rps_dict_list)

    # ç”Ÿæˆæ‰€æœ‰å­é›†ç´¢å¼•ï¼ˆæƒ°æ€§ç”Ÿæˆå™¨ï¼‰
    def generate_combinations():
        for subset_size in range(n_min, n_rps + 1):
            for subset_indices in itertools.combinations(range(n_rps), subset_size):
                yield subset_size, subset_indices

    # å¤„ç†å•ä¸ªå­é›†çš„å‡½æ•°
    def process_chunk(chunk):
        local_results = {}
        for subset_size, subset_indices in chunk:
            weighted_mass = defaultdict(float)
            total_cred = sum(credibilities[idx] for idx in subset_indices)

            if total_cred > 0:
                for idx in subset_indices:
                    cred = credibilities[idx]
                    for perm, mass in rps_dict_list[idx].items():
                        weighted_mass[perm] += cred * mass / total_cred

            result_dict = {perm: weighted_mass.get(perm, 0.0) for perm in all_perms}
            local_results[f"WS{subset_size}_{len(local_results) + 1}"] = result_dict
        return local_results

    # å¹¶è¡Œå¤„ç†
    weighted_rps_results = {}
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # å°†ä»»åŠ¡åˆ†å—å¤„ç†
        futures = []
        current_chunk = []

        for task in generate_combinations():
            current_chunk.append(task)
            if len(current_chunk) >= chunk_size:
                futures.append(executor.submit(process_chunk, current_chunk))
                current_chunk = []

        if current_chunk:  # å¤„ç†å‰©ä½™ä»»åŠ¡
            futures.append(executor.submit(process_chunk, current_chunk))

        # åˆå¹¶ç»“æœ
        for future in as_completed(futures):
            weighted_rps_results.update(future.result())

    return weighted_rps_results

def weighted_subset_rps_optimized(rps_dict_list, credibilities, n_min=2, subset_size=None):
    """ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ ¹æ®Definition 4.4è®¡ç®—åŠ æƒå­é›†RPS

    å‚æ•°:
        subset_size: æŒ‡å®šå­é›†å¤§å°ï¼ˆNone=ä½¿ç”¨n_minï¼Œé¿å…ç»„åˆçˆ†ç‚¸ï¼‰
    """
    # é¢„è®¡ç®—æ‰€æœ‰æ’åˆ—
    all_perms = sorted(set().union(*[r.keys() for r in rps_dict_list]),
                       key=lambda x: (len(x), x))

    n_rps = len(rps_dict_list)
    weighted_rps_results = {}

    # å…³é”®ä¼˜åŒ–ï¼šåªè®¡ç®—ç‰¹å®šå¤§å°çš„å­é›†ï¼Œé¿å…ç»„åˆçˆ†ç‚¸
    if subset_size is None:
        subset_size = n_min  # é»˜è®¤ä½¿ç”¨æœ€å°å­é›†å¤§å°

    # åªè®¡ç®—æŒ‡å®šå¤§å°çš„å­é›†ï¼ˆè€Œä¸æ˜¯æ‰€æœ‰å¤§å°ï¼‰
    valid_sizes = [subset_size] if subset_size >= n_min else [n_min]

    counter = 1
    for size in valid_sizes:
        # é™åˆ¶æœ€å¤§å­é›†æ•°é‡ï¼Œé˜²æ­¢ç»„åˆçˆ†ç‚¸
        max_combinations = min(100, math.comb(n_rps, size))  # å®‰å…¨é™åˆ¶

        for i, subset_indices in enumerate(itertools.combinations(range(n_rps), size)):
            if i >= max_combinations:  # é˜²æ­¢ç»„åˆçˆ†ç‚¸
                break

            # è®¡ç®—åŠ æƒè´¨é‡å‡½æ•°
            weighted_mass = defaultdict(float)
            total_cred = sum(credibilities[idx] for idx in subset_indices)

            if total_cred > 0:
                for idx in subset_indices:
                    cred = credibilities[idx]
                    for perm, mass in rps_dict_list[idx].items():
                        weighted_mass[perm] += cred * mass / total_cred

            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            result_dict = {perm: weighted_mass.get(perm, 0.0) for perm in all_perms}
            weighted_rps_results[f"WS{size}_{counter}"] = result_dict
            counter += 1

    return weighted_rps_results
def process_example(example_data):
    """å¤„ç†ç¤ºä¾‹æ•°æ® - ä¿®æ”¹ä¸ºä½¿ç”¨å­—å…¸åˆ—è¡¨"""
    # æ­¥éª¤1: æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ
    sim_matrix = similarity_matrix(example_data)
    # print("ç›¸ä¼¼åº¦çŸ©é˜µ:")
    # print(sim_matrix)
    # print()

    # æ­¥éª¤2: è®¡ç®—æ”¯æŒåº¦
    support = support_degree(sim_matrix)
    # # print("æ”¯æŒåº¦:")
    # for i, sup in enumerate(support):
    #     print(f"RPS{i + 1}: {sup:.4f}")
    # print()

    # æ­¥éª¤3: è®¡ç®—å¯ä¿¡åº¦
    credibility = credibility_degree(support)
    # print("å¯ä¿¡åº¦:")
    # for i, cred in enumerate(credibility):
    #     print(f"RPS{i + 1}: {cred:.4f}")
    # print()

    # æ­¥éª¤4: è®¡ç®—åŠ æƒå­é›†RPS
    weighted_rps = weighted_subset_rps_optimized(example_data, credibility, n_min=2)
    # print("åŠ æƒå­é›†RPSæ•°é‡:", len(weighted_rps))
    # print()

    # æ­¥éª¤5: è®¡ç®—æ¯ä¸ªåŠ æƒå­é›†çš„ç†µå¹¶æ‰¾åˆ°æœ€å°ç†µ
    min_entropy = float('inf')
    min_entropy_key = None
    min_entropy_rps = None

    # print("åŠ æƒå­é›†ç†µå€¼:")
    for key, rps_data in weighted_rps.items():
        entropy_val = rps_entropy(rps_data)
        # print(f"{key}: {entropy_val:.4f}")

        if entropy_val < min_entropy:
            min_entropy = entropy_val
            min_entropy_key = key
            min_entropy_rps = rps_data

    # print(f"\næœ€å°ç†µå­é›†: {min_entropy_key}, ç†µå€¼: {min_entropy:.4f}")

    return min_entropy_rps


# -------------------------------
# åŸºç¡€äº¤é›†å‡½æ•°
# -------------------------------
def right_intersection(A, B):
    """
    å³æ­£äº¤ (RI)ï¼Œå³ B ä¸­å»é™¤ä¸åœ¨ A ä¸­çš„å…ƒç´ 
    """
    return tuple(item for item in B if item in A)

def left_intersection(A, B):
    """
    å·¦æ­£äº¤ (LI)ï¼Œå³ A ä¸­å»é™¤ä¸åœ¨ B ä¸­çš„å…ƒç´ 
    """
    return tuple(item for item in A if item in B)

# -------------------------------
# K å€¼è®¡ç®—
# -------------------------------
def calculate_KR(M1, M2):
    """
    è®¡ç®—å³æ­£äº¤å’Œçš„ K^R (K_R)
    è¾“å…¥: [(é›†åˆ, æƒé‡), ...]
    """
    K_R = 0
    for B, w1 in M1:
        for C, w2 in M2:
            if right_intersection(B, C) == ():
                K_R += w1 * w2
    return K_R

def calculate_KL(M1, M2):
    """
    è®¡ç®—å·¦æ­£äº¤å’Œçš„ K^L (K_L)
    """
    K_L = 0
    for B, w1 in M1:
        for C, w2 in M2:
            if left_intersection(B, C) == ():
                K_L += w1 * w2
    return K_L

# -------------------------------
# ROS ä¸ LOS
# -------------------------------
def ROS(M1, M2):
    """
    å³æ­£äº¤å’Œ (ROS)
    è¾“å…¥: [(é›†åˆ, æƒé‡), ...]
    è¾“å‡º: {é›†åˆ: æƒé‡, ...}ï¼Œä¿ç•™ 0 å€¼
    """
    K_R = calculate_KR(M1, M2)
    result = defaultdict(float)

    if K_R != 1:  # é˜²æ­¢é™¤ä»¥ 0
        # éå†æ‰€æœ‰å¯èƒ½äº¤é›†
        all_keys = set(A for A, _ in M1) | set(C for C, _ in M2)
        for A in all_keys:
            weight_sum = 0
            for B, w1 in M1:
                for C, w2 in M2:
                    if right_intersection(B, C) == A:
                        weight_sum += w1 * w2
            result[A] = (1 / (1 - K_R)) * weight_sum  # å³ä½¿ weight_sum=0 ä¹Ÿä¿ç•™

    return dict(result)

def LOS(M1, M2):
    """
    å·¦æ­£äº¤å’Œ (LOS)
    """
    K_L = calculate_KL(M1, M2)
    result = defaultdict(float)

    if K_L != 1:  # é˜²æ­¢é™¤ä»¥ 0
        all_keys = set(A for A, _ in M1) | set(C for C, _ in M2)
        for A in all_keys:
            weight_sum = 0
            for B, w1 in M1:
                for C, w2 in M2:
                    if left_intersection(B, C) == A:
                        weight_sum += w1 * w2
            result[A] = (1 / (1 - K_L)) * weight_sum

    return dict(result)

# -------------------------------
# è¿ç»­æ­£äº¤å’Œ
# -------------------------------
def dict_to_pmf(rps_dict):
    """
    å°†å­—å…¸æ ¼å¼ {('A',): 0.99, ...} è½¬æ¢ä¸º [(é›†åˆ, æƒé‡), ...]
    """
    return [(k, v) for k, v in rps_dict.items()]

def continuous_right_orthogonal_sum(PMFs):
    """
    è¿ç»­æ‰§è¡Œå³æ­£äº¤å’Œæ“ä½œ
    è¾“å…¥: [ {('A',):0.9,...}, {...}, ... ]
    è¾“å‡º: {é›†åˆ: æƒé‡, ...}
    """
    result = dict_to_pmf(PMFs[0])
    for i in range(1, len(PMFs)):
        result = ROS(result, dict_to_pmf(PMFs[i])).items()
    return dict(result)

def continuous_left_orthogonal_sum(PMFs):
    """
    è¿ç»­æ‰§è¡Œå·¦æ­£äº¤å’Œæ“ä½œ
    """
    result = dict_to_pmf(PMFs[0])
    for i in range(1, len(PMFs)):
        result = LOS(result, dict_to_pmf(PMFs[i])).items()
    return dict(result)


# ==================== æ­¥éª¤3: æœ€ç»ˆåˆ†ç±» ====================
def final_classification(rps_me, rps_std_list, class_labels=None):
    """
    æ‰§è¡Œæœ€ç»ˆåˆ†ç±»
    :param rps_me: å¾…åˆ†ç±»çš„RPS (RandomPermutationSetå¯¹è±¡)
    :param rps_std_list: æ ‡å‡†RPSåˆ—è¡¨ (RandomPermutationSetå¯¹è±¡åˆ—è¡¨)
    :param class_labels: ç±»åˆ«æ ‡ç­¾åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç´¢å¼•ä½œä¸ºæ ‡ç­¾
    :return: åˆ†ç±»ç»“æœï¼ŒåŒ…å«è·ç¦»å€¼å’Œæœ€ç»ˆç±»åˆ«
    """
    # æ­¥éª¤3(i): è®¡ç®—è·ç¦»
    distances = []
    for i, rps_std in enumerate(rps_std_list):
        dist = rps_distance(rps_me, rps_std)
        distances.append(dist)
        # print(f"è·ç¦» RPS_me å’Œ RPS_std_{i + 1}: {dist:.6f}")

    # æ­¥éª¤3(ii): æ¯”è¾ƒè·ç¦»å€¼ï¼Œæ‰¾åˆ°æœ€å°è·ç¦»
    min_distance = min(distances)
    min_index = distances.index(min_distance)
    rps_final = rps_std_list[min_index]
    pred = max(rps_final, key=rps_final.get)
    return pred
    # # ç¡®å®šç±»åˆ«æ ‡ç­¾
    # if class_labels is None:
    #     class_label = f"Class_{min_index + 1}"
    # else:
    #     class_label = class_labels[min_index]
    #
    # print(f"\næœ€å°è·ç¦»: {min_distance:.6f}")
    # print(f"æœ€ç»ˆåˆ†ç±»ç»“æœ: {class_label}")
    #
    # return {
    #     'distances': distances,
    #     'min_distance': min_distance,
    #     'min_index': min_index,
    #     'class_label': class_label
    # }

# è¾…åŠ©å‡½æ•°å®šä¹‰
def get_ordered_permutations(n):
    """è·å–æ‰€æœ‰æŒ‰é¡ºåºçš„æ’åˆ—ç»„åˆ"""
    all_combinations = []
    for r in range(1, n + 1):
        for comb in itertools.combinations(range(n), r):
            all_combinations.append(comb)
    return all_combinations

def calculate_weighted_pmf(weight_matrix, sorted_nmv):
    """è®¡ç®—åŠ æƒæ¦‚ç‡è´¨é‡å‡½æ•° - ä¿®å¤ç‰ˆæœ¬"""
    num_combinations, num_attributes = weight_matrix.shape
    num_classes = sorted_nmv.shape[0]

    weighted_pmf = np.zeros((num_combinations, num_attributes))

    for attr_idx in range(num_attributes):
        weights = weight_matrix[:, attr_idx]
        # å¯¹äºæ¯ä¸ªç»„åˆï¼Œè®¡ç®—åŠ æƒæ¦‚ç‡
        for comb_idx, combination in enumerate(get_ordered_permutations(num_classes)):
            if len(combination) == 1:
                # å•å…ƒç´ ç»„åˆ
                class_idx = combination[0]
                weighted_pmf[comb_idx, attr_idx] = weights[comb_idx] * sorted_nmv[class_idx, attr_idx]
            else:
                # å¤šå…ƒç´ ç»„åˆ - ç®€åŒ–å¤„ç†
                weighted_pmf[comb_idx, attr_idx] = weights[comb_idx] * np.mean([sorted_nmv[i, attr_idx] for i in combination])

    return weighted_pmf

def gen_rps_fun_for_sample(X_train, y_train, test_sample):
    """ä¸ºå•ä¸ªæµ‹è¯•æ ·æœ¬ç”ŸæˆRPS - ä¿®å¤ç‰ˆæœ¬"""
    num_classes = len(np.unique(y_train))
    num_attributes = X_train.shape[1]

    # è®¡ç®—æ¯ä¸ªç±»ä¸­æ¯ä¸ªå±æ€§çš„ mean value and standard deviation
    mean_std_by_class = []
    for class_label in np.unique(y_train):
        X_class = X_train[y_train == class_label]
        mean_std = [(np.mean(X_class[:, i]), np.std(X_class[:, i], ddof=1)) for i in range(X_class.shape[1])]
        mean_std_by_class.append(mean_std)

    mean_std_by_class = np.array(mean_std_by_class)

    # ä¸ºæ¯ä¸ªç±»å’Œæ¯ä¸ªå±æ€§å»ºç«‹é«˜æ–¯åˆ†å¸ƒå‡½æ•°
    gaussian_functions = np.empty((num_classes, num_attributes), dtype=object)
    for class_label in range(num_classes):
        for i in range(num_attributes):
            mean, std = mean_std_by_class[class_label, i]
            gaussian_functions[class_label, i] = norm(loc=mean, scale=std)

    # è®¡ç®—è¯¥æµ‹è¯•æ ·æœ¬åœ¨æ¯ä¸ªç±»ä¸­æ¯ä¸ªå±æ€§çš„é«˜æ–¯åˆ†å¸ƒç»“æœ
    gaussian_results = []
    for class_label in range(num_classes):
        class_results = []
        for i in range(num_attributes):
            pdf_value = gaussian_functions[class_label, i].pdf(test_sample[i])
            class_results.append(pdf_value)
        gaussian_results.append(class_results)

    gaussian_results = np.array(gaussian_results)
    column_sums = np.sum(gaussian_results, axis=0)
    normalized_results = gaussian_results / column_sums

    # å¯¹å½’ä¸€åŒ–åçš„MVè¿›è¡Œé™åºæ’åº
    sorted_indices = np.argsort(-normalized_results, axis=0)
    sorted_nmv = np.take_along_axis(normalized_results, sorted_indices, axis=0)

    # è®¡ç®—æ’åºåçš„å‡å€¼
    x_mean_ord = np.empty((num_classes, num_attributes))
    for attr_idx in range(num_attributes):
        for class_idx in range(num_classes):
            sorted_class_idx = sorted_indices[class_idx, attr_idx]
            x_mean_ord[class_idx, attr_idx] = mean_std_by_class[sorted_class_idx, attr_idx, 0]

    # è®¡ç®—æ”¯æŒåº¦
    supporting_degree = np.exp(-np.abs(test_sample - x_mean_ord))

    # è·å–æ‰€æœ‰æ’åˆ—ç»„åˆ
    all_combinations = get_ordered_permutations(num_classes)
    num_combinations = len(all_combinations)

    # åˆå§‹åŒ–æƒé‡çŸ©é˜µ
    weight_matrix = np.zeros((num_combinations, num_attributes))

    # å¯¹æ¯ä¸ªå±æ€§è®¡ç®—æƒé‡
    for attr_idx in range(num_attributes):
        s = supporting_degree[:, attr_idx]
        for comb_idx, combination in enumerate(all_combinations):
            q = len(combination)
            weight = 1.0
            for u in range(q):
                i_u = combination[u]
                numerator = s[i_u]
                denominator_sum = np.sum(s[list(combination[u:])])
                weight *= numerator / denominator_sum
            weight_matrix[comb_idx, attr_idx] = weight

    # è®¡ç®—åŠ æƒæ¦‚ç‡è´¨é‡å‡½æ•°
    weighted_pmf = calculate_weighted_pmf(weight_matrix, sorted_nmv)

    # æ„å»ºRPS
    RPS_w = []
    for j in range(num_attributes):
        RPS_w_j = set()
        thetas = sorted_indices[:, j]
        weighted_pmf_j = weighted_pmf[:, j]

        for idx, combination in enumerate(all_combinations):
            A = thetas[list(combination)]
            M_A = weighted_pmf_j[idx]
            RPS_w_j.add((tuple(A), M_A))

        RPS_w.append(RPS_w_j)

    return RPS_w

def convert_numpy_types(obj):
    """
    é€’å½’è½¬æ¢æ•°æ®ç»“æ„ä¸­çš„ np.int64 å’Œ np.float64 ä¸º Python åŸç”Ÿç±»å‹
    æ”¯æŒå¤„ç†: åˆ—è¡¨/å…ƒç»„/é›†åˆ/å­—å…¸/åµŒå¥—ç»“æ„
    """
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, (str, bytes, bytearray)):
        return obj
    elif isinstance(obj, dict):
        return {convert_numpy_types(k): convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, Iterable) and not isinstance(obj, (str, bytes)):
        return type(obj)(convert_numpy_types(x) for x in obj)
    else:
        return obj

# åŸæœ‰çš„æ‰€æœ‰ç±»å®šä¹‰å’Œå‡½æ•°ä¿æŒä¸å˜...
# [è¿™é‡ŒåŒ…å«æ‚¨æä¾›çš„æ‰€æœ‰ç±»å®šä¹‰å’Œå‡½æ•°ï¼Œä»RandomPermutationSetåˆ°final_classification]
# å®šä¹‰æ˜ å°„å…³ç³»
# index_to_label = {0: 'S', 1: 'E', 2: 'V'}
index_to_label= {
0: "A",
1: "B",
2: "C",
3: "D",
4: "E",
5: "F",
6: "G",
7: "H",
8: "I",
9: "J"
}
# è½¬æ¢å‡½æ•°
def convert_to_labeled_rps(gen_rps):
    labeled_evidence = []
    for rps in gen_rps:
        converted = {}
        for items, mass in rps:
            # è½¬æ¢æ•°å­—ç´¢å¼•ä¸ºå­—æ¯æ ‡ç­¾
            labeled_items = tuple(index_to_label[i] for i in items)
            converted[labeled_items] = mass
        labeled_evidence.append(converted)
    return labeled_evidence
# ==================== æ–°å¢çš„äº¤å‰éªŒè¯æ¡†æ¶ ====================

def cross_validation_with_rps(n_splits=5, n_repeats=100):
    """100æ¬¡äº”æŠ˜äº¤å‰éªŒè¯ä¸»å‡½æ•°"""

    # åŠ è½½Irisæ•°æ®é›†
    iris = load_iris()
    # åŠ è½½ wine æ•°æ®é›†
    wine_data = load_wine()
    X = wine_data.data
    y = wine_data.target

    # åˆå§‹åŒ–é‡å¤çš„äº”æŠ˜äº¤å‰éªŒè¯
    rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)

    all_accuracies = []  # å­˜å‚¨æ‰€æœ‰é‡å¤å®éªŒçš„å‡†ç¡®ç‡
    repeat_count = 1  # é‡å¤è®¡æ•°å™¨

    for train_index, test_index in rkf.split(X):
        print(f"æ­£åœ¨å¤„ç†ç¬¬ {repeat_count} æ¬¡é‡å¤çš„ç¬¬ {((repeat_count - 1) % n_splits) + 1} æŠ˜...")

        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # æ•°æ®æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # ä¸ºå½“å‰æŠ˜ç”Ÿæˆæ‰€æœ‰æµ‹è¯•æ ·æœ¬çš„RPSå¹¶è¿›è¡Œåˆ†ç±»
        correct_predictions = 0
        total_predictions = 0

        for test_idx in range(len(X_test_scaled)):
            try:
                # 1. ç”Ÿæˆæµ‹è¯•æ ·æœ¬çš„RPS
                gen_rps = gen_rps_fun_for_sample(X_train_scaled, y_train, X_test_scaled[test_idx])

                # 2. å°†RPSè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                convert_numpy_types(gen_rps)
                labeled_evidence = convert_to_labeled_rps(gen_rps)

                # 3. å¤„ç†RPSè¯æ®ï¼ˆæ‚¨çš„æ ¸å¿ƒç®—æ³•ï¼‰
                rps_me = process_example(labeled_evidence)

                # 4. åˆ›å»ºæ ‡å‡†RPSåˆ—è¡¨ï¼ˆåŸºäºè®­ç»ƒé›†ï¼‰
                # rps_std_list = create_standard_rps_from_training(X_train_scaled, y_train)
                rps_list_r = [copy.deepcopy(rps_me) for _ in range(len(gen_rps))]
                orthogonal_sum = continuous_right_orthogonal_sum(rps_list_r)

                # 5. æ‰§è¡Œæœ€ç»ˆåˆ†ç±»
                classification_result = final_classification(
                    orthogonal_sum,
                    rps_list_r
                )

                # 6. æ£€æŸ¥åˆ†ç±»æ˜¯å¦æ­£ç¡®
                true_label = y_test[test_idx]
                # predicted_label = classification_result['min_index']  # å‡è®¾min_indexå¯¹åº”ç±»åˆ«
                predicted_label = classification_result
                true_label_str  = index_to_label[true_label]
                if predicted_label[0] == true_label_str:
                    correct_predictions += 1
                total_predictions += 1

            except Exception as e:
                print(f"å¤„ç†æ ·æœ¬ {test_idx} æ—¶å‡ºé”™: {e}")
                continue

        # è®¡ç®—å½“å‰æŠ˜çš„å‡†ç¡®ç‡
        if total_predictions > 0:
            acc = correct_predictions / total_predictions
            all_accuracies.append(acc)
            print(f"ç¬¬ {repeat_count} æ¬¡é‡å¤çš„ç¬¬ {((repeat_count - 1) % n_splits) + 1} æŠ˜å‡†ç¡®ç‡: {acc:.4f}")
        else:
            print(f"ç¬¬ {repeat_count} æ¬¡é‡å¤çš„ç¬¬ {((repeat_count - 1) % n_splits) + 1} æŠ˜æ— æœ‰æ•ˆæ ·æœ¬")
            all_accuracies.append(0)

        # æ¯å®Œæˆä¸€æ¬¡å®Œæ•´çš„5æŠ˜äº¤å‰éªŒè¯ï¼Œè¾“å‡ºä¸­é—´ç»“æœ
        if repeat_count % n_splits == 0:
            current_repeat = repeat_count // n_splits
            current_accuracies = all_accuracies[-n_splits:]
            mean_acc = np.mean(current_accuracies)
            print(f"\n=== ç¬¬ {current_repeat} æ¬¡5æŠ˜äº¤å‰éªŒè¯å®Œæˆ ===")
            print(f"æœ¬æ¬¡å¹³å‡å‡†ç¡®ç‡: {mean_acc:.4f}")
            print(f"å„æŠ˜å‡†ç¡®ç‡: {[f'{acc:.4f}' for acc in current_accuracies]}\n")

        repeat_count += 1

    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡ç»“æœ
    return calculate_final_statistics(all_accuracies, n_splits, n_repeats)

# def calculate_final_statistics(all_accuracies, n_splits, n_repeats):
#     """è®¡ç®—æœ€ç»ˆç»Ÿè®¡ç»“æœ"""
#     # è®¡ç®—æ¯æ¬¡5æŠ˜äº¤å‰éªŒè¯çš„å¹³å‡å€¼
#     repeat_means = []
#     for i in range(n_repeats):
#         start_idx = i * n_splits
#         end_idx = start_idx + n_splits
#         repeat_mean = np.mean(all_accuracies[start_idx:end_idx])
#         repeat_means.append(repeat_mean)
#
#     # è¾“å‡ºæœ€ç»ˆç»“æœ
#     print("=" * 60)
#     print(f"{n_repeats}æ¬¡{n_splits}æŠ˜äº¤å‰éªŒè¯æœ€ç»ˆç»“æœ:")
#     print(f"æ€»æŠ˜æ¬¡æ•°: {len(all_accuracies)}")
#     print(f"æ€»ä½“å¹³å‡å‡†ç¡®ç‡: {np.mean(all_accuracies):.4f} (Â±{np.std(all_accuracies):.4f})")
#     print(f"æ¯æ¬¡{n_splits}æŠ˜äº¤å‰éªŒè¯çš„å¹³å‡å‡†ç¡®ç‡: {np.mean(repeat_means):.4f} (Â±{np.std(repeat_means):.4f})")
#     print(f"æœ€é«˜å‡†ç¡®ç‡: {np.max(all_accuracies):.4f}")
#     print(f"æœ€ä½å‡†ç¡®ç‡: {np.min(all_accuracies):.4f}")
#
#     return {
#         'all_accuracies': all_accuracies,
#         'repeat_means': repeat_means,
#         'overall_mean': np.mean(all_accuracies),
#         'overall_std': np.std(all_accuracies),
#         'repeat_mean': np.mean(repeat_means),
#         'repeat_std': np.std(repeat_means)
#     }
def load_sonar_dataset():
    """åŠ è½½Sonaræ•°æ®é›†"""
    sonar = fetch_ucirepo(id=151)
    X = sonar.data.features
    y = sonar.data.targets

    # å°†ç›®æ ‡å˜é‡è½¬æ¢ä¸ºæ•°å€¼ï¼šRock=0, Mine=1
    y = (y == 'M').astype(int).values.ravel()

    return {'data': X.values, 'target': y}
def generalized_cross_validation_with_rps(
        dataset_name: str = 'iris',
        n_splits: int = 5,
        n_repeats: int = 100,
        results_dir: str = 'cv_results\li_classfication',
        save_final: bool = True
) -> Dict[str, Any]:
    """
    é€šç”¨çš„äº”æŠ˜äº¤å‰éªŒè¯å‡½æ•°ï¼Œæ”¯æŒå¤šç§æ•°æ®é›†

    å‚æ•°:
        dataset_name: æ•°æ®é›†åç§° ('iris', 'wine', 'breast_cancer', 'digits', 'heart')
        n_splits: æŠ˜æ•° (é»˜è®¤ä¸º5)
        n_repeats: é‡å¤æ¬¡æ•° (é»˜è®¤ä¸º100)
        results_dir: ç»“æœä¿å­˜ç›®å½•
        save_final: æ˜¯å¦ä¿å­˜æœ€ç»ˆç»“æœ

    è¿”å›:
        åŒ…å«å®Œæ•´ç»“æœçš„å­—å…¸
    """
    # 1. åŠ è½½æ•°æ®é›†
    global elements
    global class_mapping
    data_loader = {
        'iris': load_iris,
        'wine': load_wine,
        'breast_cancer': load_breast_cancer,
        'digits': load_digits,
        'heart': load_heart_dataset,
        'sonar': load_sonar_dataset  # æ–°å¢Sonaræ•°æ®é›†
    }

    if dataset_name not in data_loader:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}. å¯é€‰: {list(data_loader.keys())}")

    dataset = data_loader[dataset_name]()
    X = dataset['data']
    y = dataset['target']

    # 2. æ ¹æ®æ•°æ®é›†è®¾ç½®ç±»åˆ«æ ‡ç­¾
    # 2. æ ¹æ®æ•°æ®é›†è®¾ç½®ç±»åˆ«æ ‡ç­¾å’Œæ˜ å°„å…³ç³»
    if dataset_name == 'iris':
        # 3ç±»: A, B, C
        class_labels = ['setosa', 'versicolor', 'virginica']
        elements = ['A', 'B', 'C']
        class_mapping = {'A': 0, 'B': 1, 'C': 2}
        original_to_letter = {0: 'A', 1: 'B', 2: 'C'}
    elif dataset_name == 'wine':
        # 3ç±»: A, B, C
        class_labels = ['Class_1', 'Class_2', 'Class_3']
        elements = ['A', 'B', 'C']
        class_mapping = {'A': 0, 'B': 1, 'C': 2}
        original_to_letter = {0: 'A', 1: 'B', 2: 'C'}
    elif dataset_name == 'breast_cancer':
        # 2ç±»: A, B
        class_labels = ['Benign', 'Malignant']
        elements = ['A', 'B']
        class_mapping = {'A': 0, 'B': 1}
        original_to_letter = {0: 'A', 1: 'B'}
    elif dataset_name == 'digits':
        # 10ç±»: A-J
        class_labels = [str(i) for i in range(10)]
        elements = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']
        class_mapping = {letter: i for i, letter in enumerate(elements)}
        original_to_letter = {i: letter for i, letter in enumerate(elements)}
    elif dataset_name == 'heart':
        # 2ç±»: A, B
        class_labels = ['No_Disease', 'Disease']
        elements = ['A', 'B']
        class_mapping = {'A': 0, 'B': 1}
        original_to_letter = {0: 'A', 1: 'B'}
    elif dataset_name == 'sonar':
        # 2ç±»: A, B
        class_labels = ['Rock', 'Mine']
        elements = ['A', 'B']
        class_mapping = {'A': 0, 'B': 1}

    # 3. åˆ›å»ºç»“æœç›®å½•
    os.makedirs(results_dir, exist_ok=True)

    # 4. åˆå§‹åŒ–äº¤å‰éªŒè¯
    rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)

    file_results = {
        'dataset': dataset_name,
        'mean_accuracy': 0.0,  # å­˜å‚¨æ¯æ¬¡é‡å¤çš„ç»Ÿè®¡æ‘˜è¦
        'std_accuracy': 0.0
    }
    # 5. å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {
        'dataset': dataset_name,
        'n_splits': n_splits,
        'n_repeats': n_repeats,
        'class_labels': class_labels,
        'class_mapping': class_mapping,
        'fold_results': [],  # å­˜å‚¨æ¯ä¸€æŠ˜çš„è¯¦ç»†ç»“æœ
        'repeat_summaries': [],  # å­˜å‚¨æ¯æ¬¡é‡å¤çš„ç»Ÿè®¡æ‘˜è¦
        'all_accuracies': []  # å­˜å‚¨æ‰€æœ‰å‡†ç¡®ç‡
    }

    repeat_count = 1
    current_repeat_results = []

    for train_index, test_index in rkf.split(X):
        fold_result = {
            'repeat': (repeat_count - 1) // n_splits + 1,
            'fold': ((repeat_count - 1) % n_splits) + 1,
            'train_indices': train_index.tolist(),
            'test_indices': test_index.tolist(),
            'sample_results': [],
            'accuracy': 0.0
        }

        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # æ•°æ®æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        correct_predictions = 0
        total_predictions = 0

        for test_idx in range(len(X_test_scaled)):
            # sample_result = {
            #     'true_label': int(y_test[test_idx]),
            #     'predicted_label': None,
            #     'prob_dist': None,
            #     'evidence': None,
            #     'correct': False
            # }

            try:
                # 1. ç”Ÿæˆæµ‹è¯•æ ·æœ¬çš„RPS
                gen_rps = gen_rps_fun_for_sample(X_train_scaled, y_train, X_test_scaled[test_idx])

                # 2. å°†RPSè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                convert_numpy_types(gen_rps)
                labeled_evidence = convert_to_labeled_rps(gen_rps)

                # 3. å¤„ç†RPSè¯æ®ï¼ˆæ‚¨çš„æ ¸å¿ƒç®—æ³•ï¼‰
                rps_me = process_example(labeled_evidence)

                # 4. åˆ›å»ºæ ‡å‡†RPSåˆ—è¡¨ï¼ˆåŸºäºè®­ç»ƒé›†ï¼‰
                # rps_std_list = create_standard_rps_from_training(X_train_scaled, y_train)
                rps_list_r = [copy.deepcopy(rps_me) for _ in range(len(gen_rps))]
                orthogonal_sum = continuous_right_orthogonal_sum(rps_list_r)

                # 5. æ‰§è¡Œæœ€ç»ˆåˆ†ç±»
                classification_result = final_classification(
                    orthogonal_sum,
                    rps_list_r
                )

                # 6. æ£€æŸ¥åˆ†ç±»æ˜¯å¦æ­£ç¡®
                true_label = y_test[test_idx]
                # predicted_label = classification_result['min_index']  # å‡è®¾min_indexå¯¹åº”ç±»åˆ«
                predicted_label = classification_result
                true_label_str  = index_to_label[true_label]
                if predicted_label[0] == true_label_str:
                    correct_predictions += 1
                total_predictions += 1

            except Exception as e:
                print(f"å¤„ç†æ ·æœ¬ {test_idx} æ—¶å‡ºé”™: {e}")
                # sample_result['error'] = str(e)

            # fold_result['sample_results'].append(sample_result)

        # è®¡ç®—å½“å‰æŠ˜çš„å‡†ç¡®ç‡
        if total_predictions > 0:
            acc = correct_predictions / total_predictions
            fold_result['accuracy'] = acc
            all_results['all_accuracies'].append(acc)
        else:
            fold_result['accuracy'] = 0.0
            all_results['all_accuracies'].append(0.0)

        current_repeat_results.append(fold_result)
        all_results['fold_results'].append(fold_result)

        print(f"Repeat {(repeat_count - 1) // n_splits + 1}, Fold {((repeat_count - 1) % n_splits) + 1}: "
              f"Accuracy = {fold_result['accuracy']:.4f}")

        # æ¯å®Œæˆä¸€æ¬¡å®Œæ•´çš„5æŠ˜äº¤å‰éªŒè¯ï¼Œç”Ÿæˆç»Ÿè®¡æ‘˜è¦ä½†ä¸ä¿å­˜æ–‡ä»¶
        if repeat_count % n_splits == 0:
            current_repeat = repeat_count // n_splits
            current_accuracies = [r['accuracy'] for r in current_repeat_results[-n_splits:]]
            mean_acc = np.mean(current_accuracies)

            repeat_summary = {
                'repeat': current_repeat,
                # 'fold_accuracies': current_accuracies,
                'mean_accuracy': mean_acc,
                # 'std_accuracy': np.std(current_accuracies),
                # 'min_accuracy': min(current_accuracies),
                # 'max_accuracy': max(current_accuracies)
            }

            # file_results['repeat_summaries'].append(repeat_summary)

            print(f"\n=== Repeat {current_repeat} å®Œæˆ ===")
            print(f"å¹³å‡å‡†ç¡®ç‡: {mean_acc:.4f} Â± {np.std(current_accuracies):.4f}")
            print(f"å„æŠ˜å‡†ç¡®ç‡: {[f'{acc:.4f}' for acc in current_accuracies]}")
            print(f"æœ€å°å‡†ç¡®ç‡: {min(current_accuracies):.4f}")
            print(f"æœ€å¤§å‡†ç¡®ç‡: {max(current_accuracies):.4f}\n")

            # é‡ç½®å½“å‰é‡å¤ç»“æœï¼ˆä»…æ¸…ç©ºä¸´æ—¶åˆ—è¡¨ï¼‰
            current_repeat_results = []

        repeat_count += 1

    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡ç»“æœ
    final_stats = calculate_final_statistics(all_results['all_accuracies'], n_splits, n_repeats)
    all_results.update(final_stats)
    file_results['mean_accuracy'] = final_stats['mean_accuracy']
    file_results['std_accuracy'] = final_stats['std_accuracy']

    # ä¿å­˜å®Œæ•´ç»“æœ
    if save_final:
        final_path = os.path.join(results_dir, f"{dataset_name}_final_results.pkl")
        with open(final_path, 'wb') as f:
            pickle.dump(file_results, f)
        print(f"\nå®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {final_path}")

    return all_results


def load_heart_dataset():
    """
    åŠ è½½Heartç–¾ç—…æ•°æ®é›†
    ä½¿ç”¨UCIæœºå™¨å­¦ä¹ ä»“åº“çš„APIè·å–æ•°æ®
    """
    heart_disease = fetch_ucirepo(id=45)
    X = heart_disease.data.features
    y = heart_disease.data.targets

    # å¤„ç†ç›®æ ‡å˜é‡ï¼šå°†>0çš„å€¼è§†ä¸ºæœ‰ç–¾ç—…(1)ï¼Œ0ä¸ºæ— ç–¾ç—…(0)
    y = (y > 0).astype(int).values.ravel()

    # å¤„ç†ç¼ºå¤±å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
    if isinstance(X, pd.DataFrame):
        X = X.fillna(X.mean()).values

    return {'data': X, 'target': y}


def calculate_final_statistics(all_accuracies: List[float], n_splits: int, n_repeats: int) -> Dict[str, Any]:
    """è®¡ç®—æœ€ç»ˆçš„ç»Ÿè®¡ç»“æœ"""
    accuracies = np.array(all_accuracies)
    repeat_accuracies = accuracies.reshape(n_repeats, n_splits).mean(axis=1)

    return {
        'mean_accuracy': np.mean(accuracies),
        'std_accuracy': np.std(accuracies),
        'mean_repeat_accuracy': np.mean(repeat_accuracies),
        'std_repeat_accuracy': np.std(repeat_accuracies),
        'min_accuracy': np.min(accuracies),
        'max_accuracy': np.max(accuracies),
        'median_accuracy': np.median(accuracies),
        'repeat_accuracies': repeat_accuracies.tolist(),
        'all_accuracies': all_accuracies
    }
# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    # æ‰§è¡Œ100æ¬¡äº”æŠ˜äº¤å‰éªŒè¯
    # results = cross_validation_with_rps(n_splits=5, n_repeats=100)
    # heartæ•°æ®é›†æœ€ç»ˆç»“æœ:
    # å¹³å‡å‡†ç¡®ç‡: 0.7613 Â± 0.0495
    # breast_canceræ•°æ®é›†æœ€ç»ˆç»“æœ:
    # å¹³å‡å‡†ç¡®ç‡: 0.8928 Â± 0.0271
    # sonaræ•°æ®é›†æœ€ç»ˆç»“æœ:
    # å¹³å‡å‡†ç¡®ç‡: 0.5622 Â± 0.0691
    results = generalized_cross_validation_with_rps('sonar')
    print("\nIrisæ•°æ®é›†æœ€ç»ˆç»“æœ:")
    print(f"å¹³å‡å‡†ç¡®ç‡: {results['mean_accuracy']:.4f} Â± {results['std_accuracy']:.4f}")
    # ä¿å­˜ç»“æœï¼ˆå¯é€‰ï¼‰
    # databases = ['breast_cancer','digits','heart']
    # for database in databases:
    #     iris_results = generalized_cross_validation_with_rps(database)


    # # æ‰§è¡Œå¤„ç†æµç¨‹
    # results = process_example(example5_4)
    # # åˆ›å»ºæ ‡å‡†RPSåˆ—è¡¨ (è¿™é‡Œä½¿ç”¨åŸå§‹ev_rps2ä½œä¸ºæ ‡å‡†RPS)
    # rps_std_list = [RandomPermutationSet(rps_data) for rps_data in ev_rps2]
    # rps_list_r = [copy.deepcopy(results) for _ in range(len(ev_rps2))]
    # orthogonal_sum = continuous_right_orthogonal_sum(rps_list_r)
    # classification_result = final_classification(RandomPermutationSet(orthogonal_sum), rps_std_list)
    # print(classification_result)


# å¤„ç†æ‚¨çš„ç¤ºä¾‹æ•°æ®
example5_4 = [
    {  # RPSâ‚
        ('A', 'B', 'C'): 0.9,
        ('A', 'C', 'B'): 0.0,
        ('B', 'A', 'C'): 0.05,
        ('D',): 0.05
    },
    {  # RPSâ‚‚
        ('A', 'B', 'C'): 0.0,
        ('A', 'C', 'B'): 0.9,
        ('B', 'A', 'C'): 0.05,
        ('D',): 0.05
    },
    {  # RPSâ‚ƒ
        ('A', 'B', 'C'): 0.5,
        ('A', 'C', 'B'): 0.4,
        ('B', 'A', 'C'): 0.05,
        ('D',): 0.05
    }
]

evidence_rps = [
    {  # ç¬¬ä¸€ç»„RPSè¯æ®
        ('A',): 0.2,
        ('B',): 0.08,
        ('C',): 0.0,
        ('B', 'A'): 0.05,
        ('A', 'B'): 0.12,
        ('A', 'C'): 0.03,
        ('C', 'A'): 0.0,
        ('A', 'B', 'C'): 0.12,
        ('B', 'A', 'C'): 0.1,
        ('B', 'C', 'A'): 0.05,
        ('A', 'C', 'B'): 0.25,
        ('C', 'A', 'B'): 0.0,
    },
    {  # ç¬¬äºŒç»„RPSè¯æ®
        ('A',): 0.07,
        ('B',): 0.13,
        ('C',): 0.02,
        ('B', 'A'): 0.2,
        ('A', 'B'): 0.07,
        ('A', 'C'): 0.1,
        ('C', 'A'): 0.0,
        ('C', 'A', 'B'): 0.08,
        ('B', 'C', 'A'): 0.0,
        ('B', 'A', 'C'): 0.2,
        ('A', 'C', 'B'): 0.0,
        ('A', 'B', 'C'): 0.13
    },
    {  # ç¬¬ä¸‰ç»„RPSè¯æ®
        ('A',): 0.14,
        ('B',): 0.09,
        ('C',): 0.0,
        ('B', 'A'): 0.08,
        ('A', 'B'): 0.12,
        ('A', 'C'): 0.00,
        ('C', 'A'): 0.0,
        ('C', 'A', 'B'): 0.05,
        ('B', 'C', 'A'): 0.0,
        ('B', 'A', 'C'): 0.1,
        ('A', 'C', 'B'): 0.3,
        ('A', 'B', 'C'): 0.12
    }
]

ev_rps = [
    {('S', 'V', 'E'): 0.006954555165045033, ('E', 'V', 'S'): 0.025813146220725232, ('E', 'V'): 0.21158161078461896,
     ('V', 'E'): 0.12630526307958156, ('E', 'S', 'V'): 0.018255297906819686, ('V', 'E', 'S'): 0.018497732862412533,
     ('S', 'E', 'V'): 0.011649997381214387, ('E',): 0.5731331342160642, ('V', 'S', 'E'): 0.007809262383518508},
    {('S', 'V', 'E'): 0.012854692513762032, ('E', 'S', 'V'): 0.021139232112424976, ('V', 'E'): 0.1440308601589531,
     ('V', 'E', 'S'): 0.028299447822906273, ('E', 'V'): 0.17645358835615865, ('E',): 0.5541393697255563,
     ('S', 'E', 'V'): 0.015748407103624188, ('V', 'S', 'E'): 0.015191930076137743,
     ('E', 'V', 'S'): 0.03214247213047656},
    {('E', 'V', 'S'): 2.162089895557878e-114, ('E', 'S', 'V'): 8.69849694732277e-116,
     ('S', 'E', 'V'): 5.408959847014939e-116, ('E', 'V'): 0.07626595079639409, ('V',): 0.810388772553717,
     ('V', 'E'): 0.11334527664988885, ('V', 'S', 'E'): 1.8858155485975245e-115, ('S', 'V', 'E'): 8.038712477141707e-116,
     ('V', 'E', 'S'): 3.153958728468296e-114},
    {('V', 'S', 'E'): 4.380326273109402e-46, ('E', 'V'): 0.08573845743356745, ('V', 'E'): 0.08236412407318644,
     ('V', 'E', 'S'): 1.2558151371858575e-45, ('V',): 0.831897418493246, ('S', 'V', 'E'): 3.0133973711356856e-46,
     ('S', 'E', 'V'): 3.1368516953566586e-46, ('E', 'S', 'V'): 4.6968282252751586e-46,
     ('E', 'V', 'S'): 1.2935593182643496e-45}
]

# è¿è¡Œç®—æ³•
ev_rps2 = [
    {('S',): 0.8597874519936183, ('V', 'S', 'E'): 0.004826656846152488, ('E', 'V', 'S'): 0.00232304354480607,
     ('E', 'S', 'V'): 0.009386821806016948, ('E', 'S'): 0.02787470476262233, ('S', 'E'): 0.06602657421343949,
     ('S', 'V', 'E'): 0.010250597315175868, ('S', 'E', 'V'): 0.01748646035770921,
     ('V', 'E', 'S'): 0.002037689160459349},
    {('V', 'E'): 0.1657907051681264, ('V', 'S', 'E'): 0.030296772131616363, ('S', 'E', 'V'): 0.020686104084820585,
     ('S', 'V', 'E'): 0.025342744539935916, ('V', 'E', 'S'): 0.035176977695437046, ('E', 'V', 'S'): 0.03138155796666076,
     ('E', 'V'): 0.1353272443716286, ('E', 'S', 'V'): 0.02206161988693075, ('V',): 0.5339362741548436},
    {('E', 'S', 'V'): 4.422459965473793e-17, ('S',): 0.9999999999439214, ('S', 'E', 'V'): 5.922085196355288e-16,
     ('E', 'V', 'S'): 7.114282447383555e-19, ('V', 'E', 'S'): 6.820892889872117e-19,
     ('S', 'V', 'E'): 1.5933039689132633e-16, ('V', 'S', 'E'): 1.1407698214153826e-17,
     ('S', 'E'): 5.291409850222717e-11, ('E', 'S'): 3.163840693120981e-12},
    {('E', 'V', 'S'): 1.7593224080148952e-15, ('S',): 0.9999999945065885, ('S', 'V', 'E'): 8.677908717381138e-15,
     ('V', 'S', 'E'): 3.755678798402833e-15, ('V', 'S'): 9.328831146080873e-10, ('S', 'V'): 4.56048691226305e-09,
     ('V', 'E', 'S'): 1.5111628146043908e-15, ('E', 'S', 'V'): 8.600613185686967e-15,
     ('S', 'E', 'V'): 1.706954224293838e-14}
]

